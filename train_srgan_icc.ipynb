{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, splitext\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, ToPILImage\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SRGAN Model Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srgan.model import Generator, Discriminator\n",
    "from srgan.loss import GeneratorLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative Crowd Counting Model Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icc.data_loaderB import ImageDataLoader\n",
    "from icc.model_ic_CNN import modelicCNN, retrain_icCNN\n",
    "from icc.evaluate_icCNN import evaluate_model\n",
    "from icc import network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU to run on\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random seed\n",
    "rand_seed = 26700\n",
    "\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/train/images'\n",
    "train_gt_path = 'data/train/ground_truth_csv'\n",
    "\n",
    "val_path = 'data/val/images'\n",
    "val_gt_path = 'data/val/ground_truth_csv'\n",
    "\n",
    "output_dir = 'logs/model_icCNN/'\n",
    "\n",
    "method = 'mcnn'\n",
    "dataset_name = 'shtechA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = ImageDataLoader(train_path, \n",
    "                                    train_gt_path,\n",
    "                                    shuffle=True,\n",
    "                                    gt_downsample=False,\n",
    "                                    pre_load=False,\n",
    "                                    sr_mode=True)\n",
    "\n",
    "val_data_loader = ImageDataLoader(val_path, \n",
    "                                  val_gt_path,\n",
    "                                  shuffle=True,\n",
    "                                  gt_downsample=False,\n",
    "                                  pre_load=False,\n",
    "                                  sr_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "alpha = 2\n",
    "MAX_EPOCH = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils to read and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomCrop(Input, Density, h, w, th, tw):\n",
    "    x1 = random.randint(0, h - th)\n",
    "    y1 = random.randint(0, w - tw)\n",
    "\n",
    "    Input = Input[x1:x1 + th, y1:y1 + tw]\n",
    "    Density = Density.reshape((h, w))[x1:x1 + th, y1:y1 + tw]\n",
    "\n",
    "    return Input, Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LowerResolution(img):\n",
    "    y, x = img.shape[0], img.shape[1]\n",
    "    fx, fy = int(x // alpha), int(y // alpha)\n",
    "    \n",
    "    img_small = cv2.resize(img, (fx, fy), interpolation=cv2.INTER_CUBIC)\n",
    "    lr_img = cv2.resize(img_small, (x, y), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "    return lr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing SRGAN parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initializing optimizer\n",
    "optimizerG = optim.Adam(netG.parameters())\n",
    "optimizerD = optim.Adam(netD.parameters())\n",
    "\n",
    "# Initializing loss\n",
    "gen_criterion = GeneratorLoss()\n",
    "\n",
    "netG.cuda()\n",
    "netD.cuda()\n",
    "gen_criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing ICC parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "net = modelicCNN()\n",
    "\n",
    "# Initializing optimizer\n",
    "optimizerN = torch.optim.SGD(net.parameters(), lr=0.00001, weight_decay=0.00001, momentum = 0.9)\n",
    "\n",
    "# Initializing loss\n",
    "net_loss_fn = nn.MSELoss()\n",
    "\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch(blob):\n",
    "    img = blob['data']\n",
    "    gt_density = blob['gt_density']\n",
    "\n",
    "    h = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    \n",
    "    th = int(h/3.0 - ((h/3.0) % 4))\n",
    "    tw = int(w/3.0 - ((w/3.0) % 4))\n",
    "\n",
    "    th_small = th//4\n",
    "    tw_small = tw//4\n",
    "\n",
    "    Input_HR = torch.zeros(BATCH_SIZE, 3, th, tw)\n",
    "    Input_LR = torch.zeros(BATCH_SIZE, 3, th, tw)\n",
    "    GT_Density = torch.zeros(BATCH_SIZE, 1, th, tw)\n",
    "\n",
    "    for cur_step in range(0, BATCH_SIZE):\n",
    "        img_crop, gt_density_crop = RandomCrop(img, gt_density, h, w, th, tw)\n",
    "\n",
    "        lr_img = LowerResolution(img_crop)\n",
    "\n",
    "        Input_HR[cur_step] = ToTensor()(img_crop)\n",
    "        Input_LR[cur_step] = ToTensor()(lr_img)\n",
    "        GT_Density[cur_step] = torch.from_numpy(gt_density_crop)\n",
    "        \n",
    "    return Input_HR, Input_LR, GT_Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_path = os.path.join(output_dir, 'netG.pth')\n",
    "icc_path = os.path.join(output_dir, '{}_{}_icCNN.h5'.format(method, dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_maeHR = float('inf') #sys.maxint\n",
    "best_epochHR = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_Loss = []\n",
    "D_Loss = []\n",
    "N_Loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, MAX_EPOCH+1):\n",
    "    \n",
    "    net.train()\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "\n",
    "    count = 1\n",
    "    \n",
    "    g_epoch_loss = 0\n",
    "    d_epoch_loss = 0\n",
    "    n_epoch_loss = 0\n",
    "    \n",
    "    for blob in train_data_loader:\n",
    "        \n",
    "        Input_HR, Input_LR, GT_Density = get_training_batch(blob)\n",
    "        \n",
    "        Input_HR = Input_HR.cuda()\n",
    "        Input_LR = Input_LR.cuda()\n",
    "        GT_Density = GT_Density.cuda()\n",
    "        \n",
    "        Input_SR = netG(Input_LR)\n",
    "\n",
    "        Input_SR_Gray = torch.zeros(Input_SR.size()[0], 1, Input_SR.size()[2], Input_SR.size()[3])\n",
    "        Input_SR_Gray[:,0,:,:] = (0.2126 * Input_SR[:,0,:,:] + 0.7152 * Input_SR[:,1,:,:] + 0.0722 * Input_SR[:,2,:,:])\n",
    "\n",
    "        Input_SR_Gray = Input_SR_Gray.cuda()\n",
    "            \n",
    "        Density = net(Input_SR_Gray)\n",
    "        \n",
    "        # Optimizing loss for icc net\n",
    "        optimizerN.zero_grad()\n",
    "        n_loss = 1000.0 * net_loss_fn(GT_Density, Density)\n",
    "        n_loss.backward(retain_graph=True)\n",
    "        optimizerN.step()\n",
    "        \n",
    "        # Optimizing loss for Discriminator\n",
    "        optimizerD.zero_grad()\n",
    "        real_out = netD(Input_HR).mean()\n",
    "        fake_out = netD(Input_SR).mean()\n",
    "        d_loss = 1 - real_out + fake_out\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Optimizing loss for Generator\n",
    "        optimizerG.zero_grad()\n",
    "        g_loss = 0.01 * gen_criterion(fake_out, Input_SR, Input_HR) + n_loss\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        print(\"Training epoch {}, Batch_Num {}/{}, G_Loss {}, D_Loss {}, N_Loss {}\".format(\n",
    "            epoch, count, train_data_loader.get_num_samples(), g_loss, d_loss, n_loss))\n",
    "        count += 1\n",
    "        \n",
    "        g_epoch_loss += g_loss.item()\n",
    "        d_epoch_loss += d_loss.item()\n",
    "        n_epoch_loss += n_loss.item()\n",
    "\n",
    "        \n",
    "    G_Loss.append(g_epoch_loss/train_data_loader.get_num_samples())\n",
    "    D_Loss.append(d_epoch_loss/train_data_loader.get_num_samples())\n",
    "    N_Loss.append(n_epoch_loss/train_data_loader.get_num_samples())\n",
    "        \n",
    "    if (epoch % 5 == 0):\n",
    "        maeHR, mseHR = evaluate_model(net, netG, val_data_loader)\n",
    "        \n",
    "        if maeHR < best_maeHR:\n",
    "            best_maeHR = maeHR\n",
    "            best_mseHR = mseHR\n",
    "            \n",
    "            best_epochHR = epoch\n",
    "            \n",
    "            torch.save(netG, generator_path)\n",
    "            network.save_net(icc_path, net)\n",
    "\n",
    "        print(\"EPOCH: %d, MAE_HR: %.1f, MSE_HR: %0.1f\" % (epoch, maeHR, mseHR))\n",
    "        print(\"BEST MAE_HR: %0.1f, BEST MSE_HR: %0.1f, BEST Epoch: %4.2f\" % (best_maeHR, best_mseHR, best_epochHR))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(N_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to output the Super Resolved images.\n",
    "\n",
    "# val_data_loader = ImageDataLoader(val_path, \n",
    "#                                   val_gt_path,\n",
    "#                                   shuffle=False,\n",
    "#                                   gt_downsample=False,\n",
    "#                                   pre_load=False,\n",
    "#                                   sr_mode=True)\n",
    "\n",
    "# model = torch.load(os.path.join(output_dir, 'netG.pth'))\n",
    "# model.cuda()\n",
    "\n",
    "# out_path = \"logs/SRGAN/val/\"\n",
    "\n",
    "# for blob in val_data_loader:\n",
    "#     img = blob['data']\n",
    "#     fname = blob['fname']\n",
    "    \n",
    "#     img = Variable(ToTensor()(img), requires_grad=False).unsqueeze(0)\n",
    "#     img = img.cuda()\n",
    "    \n",
    "#     out = model(img)\n",
    "#     out = ToPILImage()(out[0].data.cpu())\n",
    "    \n",
    "#     out.save(join(out_path, 'out_' + fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
