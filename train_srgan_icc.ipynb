{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, splitext\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, ToPILImage\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SRGAN Model Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srgan.model import Generator, Discriminator\n",
    "from srgan.loss import GeneratorLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative Crowd Counting Model Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icc.data_loaderB import ImageDataLoader\n",
    "from icc.model_ic_CNN import modelicCNN, retrain_icCNN\n",
    "from icc.evaluate_icCNN import evaluate_model\n",
    "from icc import network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU to run on\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random seed\n",
    "rand_seed = 26700\n",
    "\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/train/images'\n",
    "train_gt_path = 'data/train/ground_truth_csv'\n",
    "\n",
    "val_path = 'data/val/images'\n",
    "val_gt_path = 'data/val/ground_truth_csv'\n",
    "\n",
    "output_dir = './logs/model_icCNN/'\n",
    "\n",
    "method = 'mcnn'\n",
    "dataset_name = 'shtechA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading the data. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udbhav/Documents/Thesis Research/Crowd_SRGAN/icc/data_loaderB.py:44: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).as_matrix()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  100 / 300 files\n",
      "Loaded  200 / 300 files\n",
      "Loaded  300 / 300 files\n",
      "Completed Loading  300 files\n",
      "Pre-loading the data. This may take a while...\n",
      "Loaded  100 / 182 files\n",
      "Completed Loading  182 files\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = ImageDataLoader(train_path, \n",
    "                                    train_gt_path,\n",
    "                                    shuffle=True,\n",
    "                                    gt_downsample=False,\n",
    "                                    pre_load=True,\n",
    "                                    sr_mode=True)\n",
    "\n",
    "val_data_loader = ImageDataLoader(val_path, \n",
    "                                  val_gt_path,\n",
    "                                  shuffle=True,\n",
    "                                  gt_downsample=False,\n",
    "                                  pre_load=True,\n",
    "                                  sr_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "alpha = 2\n",
    "MAX_EPOCH = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils to read and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomCrop(Input, Density, h, w, th, tw):\n",
    "    x1 = random.randint(0, h - th)\n",
    "    y1 = random.randint(0, w - tw)\n",
    "\n",
    "    Input = Input[x1:x1 + th, y1:y1 + tw]\n",
    "    Density = Density[x1:x1 + th, y1:y1 + tw]\n",
    "\n",
    "    return Input, Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LowerResolution(img):\n",
    "    y, x = img.shape[0], img.shape[1]\n",
    "    fx, fy = int(x // alpha), int(y // alpha)\n",
    "    \n",
    "    img_small = cv2.resize(img, (fx, fy), interpolation=cv2.INTER_CUBIC)\n",
    "    lr_img = cv2.resize(img_small, (x, y), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "    return lr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing SRGAN parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2)\n",
       "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): LeakyReLU(negative_slope=0.2)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): LeakyReLU(negative_slope=0.2)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): LeakyReLU(negative_slope=0.2)\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (21): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): LeakyReLU(negative_slope=0.2)\n",
       "    (23): AdaptiveAvgPool2d(output_size=1)\n",
       "    (24): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (25): LeakyReLU(negative_slope=0.2)\n",
       "    (26): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing model\n",
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initializing optimizer\n",
    "optimizerG = optim.Adam(netG.parameters())\n",
    "optimizerD = optim.Adam(netD.parameters())\n",
    "\n",
    "# Initializing loss\n",
    "gen_criterion = GeneratorLoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    netG.cuda()\n",
    "    netD.cuda()\n",
    "    gen_criterion.cuda()\n",
    "    \n",
    "netG.train()\n",
    "netD.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing ICC parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retrain_icCNN(\n",
       "  (netDME): DME2(\n",
       "    (branch3): Sequential(\n",
       "      (0): Conv2d(\n",
       "        (conv): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): Conv2d(\n",
       "        (conv): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(\n",
       "        (conv): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (5): Conv2d(\n",
       "        (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (6): Conv2d(\n",
       "        (conv): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (netFCNN): FCNN304(\n",
       "    (branch1): Sequential(\n",
       "      (0): Conv2d(\n",
       "        (conv): Conv2d(281, 196, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (1): Conv2d(\n",
       "        (conv): Conv2d(196, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): UpsamplingBilinear2d(scale_factor=2, mode=bilinear)\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(\n",
       "        (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (5): UpsamplingBilinear2d(scale_factor=2, mode=bilinear)\n",
       "      (6): ReLU()\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): Conv2d(\n",
       "        (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (netGCE): GCEVGG16(\n",
       "    (vgg16): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace)\n",
       "    )\n",
       "    (branch1): Sequential(\n",
       "      (0): Conv2d(\n",
       "        (conv): Conv2d(256, 196, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (1): Conv2d(\n",
       "        (conv): Conv2d(196, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Conv2d(\n",
       "        (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Conv2d(\n",
       "        (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing model\n",
    "net = retrain_icCNN()\n",
    "\n",
    "# Initializing optimizer\n",
    "optimizerN = torch.optim.SGD(net.parameters(), lr=0.00001, weight_decay=0.00001, momentum = 0.9)\n",
    "\n",
    "# Initializing loss\n",
    "net_loss_fn = nn.MSELoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch(blob):\n",
    "    img = blob['data']\n",
    "    gt_density = blob['gt_density']\n",
    "\n",
    "    h = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    \n",
    "    th = int(h/3.0 - ((h/3.0) % 4))\n",
    "    tw = int(w/3.0 - ((w/3.0) % 4))\n",
    "\n",
    "    th_small = th//4\n",
    "    tw_small = tw//4\n",
    "\n",
    "    Input_HR = torch.zeros(BATCH_SIZE, 3, th, tw)\n",
    "    Input_LR = torch.zeros(BATCH_SIZE, 3, th, tw)\n",
    "    GT_Density = torch.zeros(BATCH_SIZE, 1, th, tw)\n",
    "    GT_Density_Small = torch.zeros(BATCH_SIZE, 1, th_small, tw_small)\n",
    "\n",
    "    for cur_step in range(0, BATCH_SIZE):\n",
    "        img_crop, gt_density_crop = RandomCrop(img, gt_density, h, w, th, tw)\n",
    "\n",
    "        lr_img = LowerResolution(img_crop)\n",
    "\n",
    "        gt_density_crop_small = cv2.resize(gt_density_crop, (tw_small, th_small))\n",
    "        gt_density_crop_small = gt_density_crop_small * ((tw*th)/(tw_small*th_small))\n",
    "\n",
    "        Input_HR[cur_step] = ToTensor()(img_crop)\n",
    "        Input_LR[cur_step] = ToTensor()(lr_img)\n",
    "        GT_Density[cur_step] = torch.from_numpy(gt_density_crop)\n",
    "        GT_Density_Small[cur_step] = torch.from_numpy(gt_density_crop_small)\n",
    "        \n",
    "    return Input_HR, Input_LR, GT_Density, GT_Density_Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_maeHR = float('inf') #sys.maxint\n",
    "best_epochHR = 1\n",
    "best_maeLR = float('inf') #sys.maxint\n",
    "best_epochLR = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:225: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1, Batch_Num 1/300, G_Loss 1423.5343017578125, D_Loss 1.0001497268676758, N_Loss 985.6886596679688\n",
      "Training epoch 1, Batch_Num 2/300, G_Loss 1582.117431640625, D_Loss 1.004669427871704, N_Loss 611.9422607421875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-cac85f089400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mfake_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput_SR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreal_out\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfake_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizerD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, MAX_EPOCH+1):\n",
    "    \n",
    "    count = 1\n",
    "    for blob in train_data_loader:\n",
    "        \n",
    "        Input_HR, Input_LR, GT_Density, GT_Density_Small = get_training_batch(blob)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            Input_HR = Input_HR.cuda()\n",
    "            Input_LR = Input_LR.cuda()\n",
    "            GT_Density = GT_Density.cuda()\n",
    "            GT_Density_Small = GT_Density_Small.cuda()\n",
    "        \n",
    "        Input_SR = netG(Input_LR)\n",
    "\n",
    "        Input_SR_Gray = torch.zeros(Input_SR.size()[0], 1, Input_SR.size()[2], Input_SR.size()[3])\n",
    "        Input_SR_Gray[:,0,:,:] = (0.2126 * Input_SR[:,0,:,:] + 0.7152 * Input_SR[:,1,:,:] + 0.0722 * Input_SR[:,2,:,:])\n",
    "\n",
    "        Density_Small, Density = net(Input_SR_Gray)\n",
    "        \n",
    "        # Optimizing loss for icc net\n",
    "        n_loss = 1000.0 * net_loss_fn(GT_Density, Density) + 0.01 * net_loss_fn(GT_Density_Small, Density_Small)\n",
    "        n_loss.backward(retain_graph=True)\n",
    "        optimizerN.step()\n",
    "        \n",
    "        # Optimizing loss for Discriminator\n",
    "        netD.zero_grad()\n",
    "        real_out = netD(Input_HR).mean()\n",
    "        fake_out = netD(Input_SR).mean()\n",
    "        d_loss = 1 - real_out + fake_out\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Optimizing loss for Generator\n",
    "        netG.zero_grad()\n",
    "        g_loss = gen_criterion(fake_out, Input_SR, Input_HR, GT_Density, Density)\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        print(\"Training epoch {}, Batch_Num {}/{}, G_Loss {}, D_Loss {}, N_Loss {}\".format(\n",
    "            epoch, count, train_data_loader.get_num_samples(), g_loss, d_loss, n_loss.item()))\n",
    "        count += 1\n",
    "    \n",
    "    if (epoch % 5 == 0  ):\n",
    "        save_name = os.path.join(output_dir, '{}_{}_icCNN.h5'.format(method, dataset_name))\n",
    "        network.save_net(save_name, net)\n",
    "        maeLR, mseLR, maeHR, mseHR = evaluate_model(save_name, val_data_loader)\n",
    "        if maeHR < best_maeHR:\n",
    "            best_maeHR = maeHR\n",
    "            best_mseHR = mseHR\n",
    "            best_maeLR = maeLR\n",
    "            best_mseLR = mseLR\n",
    "            best_epochHR = epoch\n",
    "        print(\"EPOCH: %d, MAE_LR: %.1f, MSE_LR: %0.1f, MAE_HR: %.1f, MSE_HR: %0.1f\" % (epoch, maeLR, mseLR, maeHR, mseHR))\n",
    "        print(\"BEST MAE_LR: %0.1f, BEST MSE_LR: %0.1f\" % (best_maeLR, best_mseLR))\n",
    "        print(\"BEST MAE_HR: %0.1f, BEST MSE_HR: %0.1f, BEST Epoch: %4.2f\" % (best_maeHR, best_mseHR, best_epochHR))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions?\n",
    "\n",
    "requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
