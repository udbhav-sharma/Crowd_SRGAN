{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, splitext\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, ToPILImage\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative Crowd Counting Model Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icc.data_loaderB import ImageDataLoader\n",
    "from icc.model_ic_CNN import modelicCNN, retrain_icCNN\n",
    "from icc.evaluate_icCNN import evaluate_model\n",
    "from icc import network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU to run on\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random seed\n",
    "rand_seed = 26700\n",
    "\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "MAX_EPOCH = 50\n",
    "\n",
    "LR = 0.00001\n",
    "WEIGHT_DECAY = 0.00001\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/train/images'\n",
    "train_gt_path = 'data/train/ground_truth_csv'\n",
    "\n",
    "val_path = 'data/val/images'\n",
    "val_gt_path = 'data/val/ground_truth_csv'\n",
    "\n",
    "output_dir = 'logs/model_icCNN/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = ImageDataLoader(train_path, \n",
    "                                    train_gt_path,\n",
    "                                    shuffle=True,\n",
    "                                    gt_downsample=False,\n",
    "                                    pre_load=False,\n",
    "                                    sr_mode=False)\n",
    "\n",
    "val_data_loader = ImageDataLoader(val_path, \n",
    "                                  val_gt_path,\n",
    "                                  shuffle=True,\n",
    "                                  gt_downsample=False,\n",
    "                                  pre_load=False,\n",
    "                                  sr_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils to read and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomCrop(Input, Density, h, w, th, tw):\n",
    "    x1 = random.randint(0, h - th)\n",
    "    y1 = random.randint(0, w - tw)\n",
    "\n",
    "    Input = Input[x1:x1 + th, y1:y1 + tw]\n",
    "    Density = Density.reshape((h, w))[x1:x1 + th, y1:y1 + tw]\n",
    "\n",
    "    return Input, Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch(blob):\n",
    "    img = blob['data']\n",
    "    gt_density = blob['gt_density']\n",
    "\n",
    "    h = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    \n",
    "    th = int(h/3.0 - ((h/3.0) % 4))\n",
    "    tw = int(w/3.0 - ((w/3.0) % 4))\n",
    "    \n",
    "    Input_HR = torch.zeros(BATCH_SIZE, 1, th, tw)\n",
    "    GT_Density = torch.zeros(BATCH_SIZE, 1, th, tw)\n",
    "\n",
    "    for cur_step in range(0, BATCH_SIZE):\n",
    "        img_crop, gt_density_crop = RandomCrop(img, gt_density, h, w, th, tw)\n",
    "\n",
    "        Input_HR[cur_step] = torch.from_numpy(img_crop)\n",
    "        GT_Density[cur_step] = torch.from_numpy(gt_density_crop)\n",
    "        \n",
    "    return Input_HR, GT_Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing ICC parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "net = modelicCNN()\n",
    "net.cuda()\n",
    "\n",
    "# Initializing optimizer\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=LR)\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
    "\n",
    "# Initializing loss\n",
    "n_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_maeHR = float('inf') #sys.maxint\n",
    "best_epochHR = 1\n",
    "\n",
    "N_Loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udbhav/thesis/brain-storming/Crowd_SRGAN_2/Crowd_SRGAN/icc/data_loaderB.py:103: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).as_matrix()\n",
      "/home/udbhav/anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:183: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.Upsample instead.\n",
      "  warnings.warn(\"nn.UpsamplingBilinear2d is deprecated. Use nn.Upsample instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, Batch_Num: 1/300, N_Loss: 42.84021759033203\n",
      "Training epoch: 1, Batch_Num: 2/300, N_Loss: 2.514537811279297\n",
      "Training epoch: 1, Batch_Num: 3/300, N_Loss: 0.10253629833459854\n",
      "Training epoch: 1, Batch_Num: 4/300, N_Loss: 0.036158062517642975\n",
      "Training epoch: 1, Batch_Num: 5/300, N_Loss: 0.009580442681908607\n",
      "Training epoch: 1, Batch_Num: 6/300, N_Loss: 0.0035459815990179777\n",
      "Training epoch: 1, Batch_Num: 7/300, N_Loss: 0.003995529375970364\n",
      "Training epoch: 1, Batch_Num: 8/300, N_Loss: 0.004674268886446953\n",
      "Training epoch: 1, Batch_Num: 9/300, N_Loss: 0.003101392649114132\n",
      "Training epoch: 1, Batch_Num: 10/300, N_Loss: 0.0024698420893400908\n",
      "Training epoch: 1, Batch_Num: 11/300, N_Loss: 0.006208175793290138\n",
      "Training epoch: 1, Batch_Num: 12/300, N_Loss: 0.0039164344780147076\n",
      "Training epoch: 1, Batch_Num: 13/300, N_Loss: 0.016327088698744774\n",
      "Training epoch: 1, Batch_Num: 14/300, N_Loss: 0.006103903986513615\n",
      "Training epoch: 1, Batch_Num: 15/300, N_Loss: 0.008371256291866302\n",
      "Training epoch: 1, Batch_Num: 16/300, N_Loss: 0.0034975847229361534\n",
      "Training epoch: 1, Batch_Num: 17/300, N_Loss: 0.010284952819347382\n",
      "Training epoch: 1, Batch_Num: 18/300, N_Loss: 0.003858333919197321\n",
      "Training epoch: 1, Batch_Num: 19/300, N_Loss: 0.0026689586229622364\n",
      "Training epoch: 1, Batch_Num: 20/300, N_Loss: 0.005901141092181206\n",
      "Training epoch: 1, Batch_Num: 21/300, N_Loss: 0.0229432824999094\n",
      "Training epoch: 1, Batch_Num: 22/300, N_Loss: 0.0034272216726094484\n",
      "Training epoch: 1, Batch_Num: 23/300, N_Loss: 0.0010546781122684479\n",
      "Training epoch: 1, Batch_Num: 24/300, N_Loss: 0.012328761629760265\n",
      "Training epoch: 1, Batch_Num: 25/300, N_Loss: 0.028178274631500244\n",
      "Training epoch: 1, Batch_Num: 26/300, N_Loss: 0.004024714697152376\n",
      "Training epoch: 1, Batch_Num: 27/300, N_Loss: 0.14208322763442993\n",
      "Training epoch: 1, Batch_Num: 28/300, N_Loss: 0.0015638822223991156\n",
      "Training epoch: 1, Batch_Num: 29/300, N_Loss: 0.07191731035709381\n",
      "Training epoch: 1, Batch_Num: 30/300, N_Loss: 0.006730201188474894\n",
      "Training epoch: 1, Batch_Num: 31/300, N_Loss: 0.06556694954633713\n",
      "Training epoch: 1, Batch_Num: 32/300, N_Loss: 0.0057603707537055016\n",
      "Training epoch: 1, Batch_Num: 33/300, N_Loss: 0.011393425054848194\n",
      "Training epoch: 1, Batch_Num: 34/300, N_Loss: 0.003965080715715885\n",
      "Training epoch: 1, Batch_Num: 35/300, N_Loss: 0.0016029315302148461\n",
      "Training epoch: 1, Batch_Num: 36/300, N_Loss: 0.01134370919317007\n",
      "Training epoch: 1, Batch_Num: 37/300, N_Loss: 0.004191869404166937\n",
      "Training epoch: 1, Batch_Num: 38/300, N_Loss: 0.003697105450555682\n",
      "Training epoch: 1, Batch_Num: 39/300, N_Loss: 0.0017236127750948071\n",
      "Training epoch: 1, Batch_Num: 40/300, N_Loss: 0.015000982210040092\n",
      "Training epoch: 1, Batch_Num: 41/300, N_Loss: 0.0035863504745066166\n",
      "Training epoch: 1, Batch_Num: 42/300, N_Loss: 0.020131738856434822\n",
      "Training epoch: 1, Batch_Num: 43/300, N_Loss: 0.015010016970336437\n",
      "Training epoch: 1, Batch_Num: 44/300, N_Loss: 0.04528876766562462\n",
      "Training epoch: 1, Batch_Num: 45/300, N_Loss: 0.017012640833854675\n",
      "Training epoch: 1, Batch_Num: 46/300, N_Loss: 0.001447436516173184\n",
      "Training epoch: 1, Batch_Num: 47/300, N_Loss: 0.006316400598734617\n",
      "Training epoch: 1, Batch_Num: 48/300, N_Loss: 0.013763494789600372\n",
      "Training epoch: 1, Batch_Num: 49/300, N_Loss: 0.0015216257888823748\n",
      "Training epoch: 1, Batch_Num: 50/300, N_Loss: 0.0037497468292713165\n",
      "Training epoch: 1, Batch_Num: 51/300, N_Loss: 0.0026823694352060556\n",
      "Training epoch: 1, Batch_Num: 52/300, N_Loss: 0.024168113246560097\n",
      "Training epoch: 1, Batch_Num: 53/300, N_Loss: 0.0312686450779438\n",
      "Training epoch: 1, Batch_Num: 54/300, N_Loss: 0.012324548326432705\n",
      "Training epoch: 1, Batch_Num: 55/300, N_Loss: 0.0010747172636911273\n",
      "Training epoch: 1, Batch_Num: 56/300, N_Loss: 0.0008216499700210989\n",
      "Training epoch: 1, Batch_Num: 57/300, N_Loss: 0.03581982105970383\n",
      "Training epoch: 1, Batch_Num: 58/300, N_Loss: 0.011408179998397827\n",
      "Training epoch: 1, Batch_Num: 59/300, N_Loss: 0.0074799261055886745\n",
      "Training epoch: 1, Batch_Num: 60/300, N_Loss: 0.002259189262986183\n",
      "Training epoch: 1, Batch_Num: 61/300, N_Loss: 0.005213831551373005\n",
      "Training epoch: 1, Batch_Num: 62/300, N_Loss: 0.00468816002830863\n",
      "Training epoch: 1, Batch_Num: 63/300, N_Loss: 0.011285175569355488\n",
      "Training epoch: 1, Batch_Num: 64/300, N_Loss: 0.040389060974121094\n",
      "Training epoch: 1, Batch_Num: 65/300, N_Loss: 0.0013643389102071524\n",
      "Training epoch: 1, Batch_Num: 66/300, N_Loss: 0.003984478767961264\n",
      "Training epoch: 1, Batch_Num: 67/300, N_Loss: 0.007885645143687725\n",
      "Training epoch: 1, Batch_Num: 68/300, N_Loss: 0.006384982727468014\n",
      "Training epoch: 1, Batch_Num: 69/300, N_Loss: 0.019688567146658897\n",
      "Training epoch: 1, Batch_Num: 70/300, N_Loss: 0.021762404590845108\n",
      "Training epoch: 1, Batch_Num: 71/300, N_Loss: 0.005044829100370407\n",
      "Training epoch: 1, Batch_Num: 72/300, N_Loss: 0.016085244715213776\n",
      "Training epoch: 1, Batch_Num: 73/300, N_Loss: 0.006060487125068903\n",
      "Training epoch: 1, Batch_Num: 74/300, N_Loss: 0.007189595606178045\n",
      "Training epoch: 1, Batch_Num: 75/300, N_Loss: 0.0062401373870670795\n",
      "Training epoch: 1, Batch_Num: 76/300, N_Loss: 0.002227957360446453\n",
      "Training epoch: 1, Batch_Num: 77/300, N_Loss: 0.013885855674743652\n",
      "Training epoch: 1, Batch_Num: 78/300, N_Loss: 0.0066055976785719395\n",
      "Training epoch: 1, Batch_Num: 79/300, N_Loss: 0.0023431305307894945\n",
      "Training epoch: 1, Batch_Num: 80/300, N_Loss: 0.0010061177890747786\n",
      "Training epoch: 1, Batch_Num: 81/300, N_Loss: 0.03717760369181633\n",
      "Training epoch: 1, Batch_Num: 82/300, N_Loss: 0.0065019200555980206\n",
      "Training epoch: 1, Batch_Num: 83/300, N_Loss: 0.07000972330570221\n",
      "Training epoch: 1, Batch_Num: 84/300, N_Loss: 0.04318615421652794\n",
      "Training epoch: 1, Batch_Num: 85/300, N_Loss: 0.00711682066321373\n",
      "Training epoch: 1, Batch_Num: 86/300, N_Loss: 0.002816432621330023\n",
      "Training epoch: 1, Batch_Num: 87/300, N_Loss: 0.0014551813947036862\n",
      "Training epoch: 1, Batch_Num: 88/300, N_Loss: 0.0029770389664918184\n",
      "Training epoch: 1, Batch_Num: 89/300, N_Loss: 0.0024902482982724905\n",
      "Training epoch: 1, Batch_Num: 90/300, N_Loss: 0.0045196362771093845\n",
      "Training epoch: 1, Batch_Num: 91/300, N_Loss: 0.020097697153687477\n",
      "Training epoch: 1, Batch_Num: 92/300, N_Loss: 0.054790377616882324\n",
      "Training epoch: 1, Batch_Num: 93/300, N_Loss: 0.07959423214197159\n",
      "Training epoch: 1, Batch_Num: 94/300, N_Loss: 0.003574625588953495\n",
      "Training epoch: 1, Batch_Num: 95/300, N_Loss: 0.00950318481773138\n",
      "Training epoch: 1, Batch_Num: 96/300, N_Loss: 0.03452593460679054\n",
      "Training epoch: 1, Batch_Num: 97/300, N_Loss: 0.0025583014357835054\n",
      "Training epoch: 1, Batch_Num: 98/300, N_Loss: 0.014256962575018406\n",
      "Training epoch: 1, Batch_Num: 99/300, N_Loss: 0.02662733383476734\n",
      "Training epoch: 1, Batch_Num: 100/300, N_Loss: 0.0032465748954564333\n",
      "Training epoch: 1, Batch_Num: 101/300, N_Loss: 0.003360372968018055\n",
      "Training epoch: 1, Batch_Num: 102/300, N_Loss: 0.0023521946277469397\n",
      "Training epoch: 1, Batch_Num: 103/300, N_Loss: 0.0021924187894910574\n",
      "Training epoch: 1, Batch_Num: 104/300, N_Loss: 0.002204546704888344\n",
      "Training epoch: 1, Batch_Num: 105/300, N_Loss: 0.021001797169446945\n",
      "Training epoch: 1, Batch_Num: 106/300, N_Loss: 0.006329002790153027\n",
      "Training epoch: 1, Batch_Num: 107/300, N_Loss: 0.014765139669179916\n",
      "Training epoch: 1, Batch_Num: 108/300, N_Loss: 0.0014236884890124202\n",
      "Training epoch: 1, Batch_Num: 109/300, N_Loss: 0.007846340537071228\n",
      "Training epoch: 1, Batch_Num: 110/300, N_Loss: 0.0026157665997743607\n",
      "Training epoch: 1, Batch_Num: 111/300, N_Loss: 0.00356330000795424\n",
      "Training epoch: 1, Batch_Num: 112/300, N_Loss: 0.004518991336226463\n",
      "Training epoch: 1, Batch_Num: 113/300, N_Loss: 0.05544821172952652\n",
      "Training epoch: 1, Batch_Num: 114/300, N_Loss: 0.013223635032773018\n",
      "Training epoch: 1, Batch_Num: 115/300, N_Loss: 0.013746291399002075\n",
      "Training epoch: 1, Batch_Num: 116/300, N_Loss: 0.028553057461977005\n",
      "Training epoch: 1, Batch_Num: 117/300, N_Loss: 0.0026406412944197655\n",
      "Training epoch: 1, Batch_Num: 118/300, N_Loss: 0.009759852662682533\n",
      "Training epoch: 1, Batch_Num: 119/300, N_Loss: 0.003759560640901327\n",
      "Training epoch: 1, Batch_Num: 120/300, N_Loss: 0.026506537571549416\n",
      "Training epoch: 1, Batch_Num: 121/300, N_Loss: 0.0018193849828094244\n",
      "Training epoch: 1, Batch_Num: 122/300, N_Loss: 0.004215857945382595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, Batch_Num: 123/300, N_Loss: 0.015422440133988857\n",
      "Training epoch: 1, Batch_Num: 124/300, N_Loss: 0.0019008954986929893\n",
      "Training epoch: 1, Batch_Num: 125/300, N_Loss: 0.002296952996402979\n",
      "Training epoch: 1, Batch_Num: 126/300, N_Loss: 0.0025151006411761045\n",
      "Training epoch: 1, Batch_Num: 127/300, N_Loss: 0.022140365093946457\n",
      "Training epoch: 1, Batch_Num: 128/300, N_Loss: 0.004444336984306574\n",
      "Training epoch: 1, Batch_Num: 129/300, N_Loss: 0.0008258402231149375\n",
      "Training epoch: 1, Batch_Num: 130/300, N_Loss: 0.0069687627255916595\n",
      "Training epoch: 1, Batch_Num: 131/300, N_Loss: 0.000903665495570749\n",
      "Training epoch: 1, Batch_Num: 132/300, N_Loss: 0.004090901464223862\n",
      "Training epoch: 1, Batch_Num: 133/300, N_Loss: 0.014943767338991165\n",
      "Training epoch: 1, Batch_Num: 134/300, N_Loss: 0.012375679798424244\n",
      "Training epoch: 1, Batch_Num: 135/300, N_Loss: 0.008856630884110928\n",
      "Training epoch: 1, Batch_Num: 136/300, N_Loss: 0.07888299226760864\n",
      "Training epoch: 1, Batch_Num: 137/300, N_Loss: 0.009410805068910122\n",
      "Training epoch: 1, Batch_Num: 138/300, N_Loss: 0.0011840799124911427\n",
      "Training epoch: 1, Batch_Num: 139/300, N_Loss: 0.033691588789224625\n",
      "Training epoch: 1, Batch_Num: 140/300, N_Loss: 0.0027277430053800344\n",
      "Training epoch: 1, Batch_Num: 141/300, N_Loss: 0.0054942891001701355\n",
      "Training epoch: 1, Batch_Num: 142/300, N_Loss: 0.004795265384018421\n",
      "Training epoch: 1, Batch_Num: 143/300, N_Loss: 0.027895690873265266\n",
      "Training epoch: 1, Batch_Num: 144/300, N_Loss: 0.008094407618045807\n",
      "Training epoch: 1, Batch_Num: 145/300, N_Loss: 0.006708447355777025\n",
      "Training epoch: 1, Batch_Num: 146/300, N_Loss: 0.004554309416562319\n",
      "Training epoch: 1, Batch_Num: 147/300, N_Loss: 0.0028739692643284798\n",
      "Training epoch: 1, Batch_Num: 148/300, N_Loss: 0.023481156677007675\n",
      "Training epoch: 1, Batch_Num: 149/300, N_Loss: 0.00348080787807703\n",
      "Training epoch: 1, Batch_Num: 150/300, N_Loss: 0.008346284739673138\n",
      "Training epoch: 1, Batch_Num: 151/300, N_Loss: 0.0018055549589917064\n",
      "Training epoch: 1, Batch_Num: 152/300, N_Loss: 0.025193864479660988\n",
      "Training epoch: 1, Batch_Num: 153/300, N_Loss: 0.004227949306368828\n",
      "Training epoch: 1, Batch_Num: 154/300, N_Loss: 0.0018434363882988691\n",
      "Training epoch: 1, Batch_Num: 155/300, N_Loss: 0.007263964973390102\n",
      "Training epoch: 1, Batch_Num: 156/300, N_Loss: 0.016492554917931557\n",
      "Training epoch: 1, Batch_Num: 157/300, N_Loss: 0.011884450912475586\n",
      "Training epoch: 1, Batch_Num: 158/300, N_Loss: 0.011283371597528458\n",
      "Training epoch: 1, Batch_Num: 159/300, N_Loss: 0.05979103222489357\n",
      "Training epoch: 1, Batch_Num: 160/300, N_Loss: 0.026577193289995193\n",
      "Training epoch: 1, Batch_Num: 161/300, N_Loss: 0.021573767066001892\n",
      "Training epoch: 1, Batch_Num: 162/300, N_Loss: 0.007849491201341152\n",
      "Training epoch: 1, Batch_Num: 163/300, N_Loss: 0.024759788066148758\n",
      "Training epoch: 1, Batch_Num: 164/300, N_Loss: 0.013992488384246826\n",
      "Training epoch: 1, Batch_Num: 165/300, N_Loss: 0.07630477845668793\n",
      "Training epoch: 1, Batch_Num: 166/300, N_Loss: 0.0018601007759571075\n",
      "Training epoch: 1, Batch_Num: 167/300, N_Loss: 0.01131074782460928\n",
      "Training epoch: 1, Batch_Num: 168/300, N_Loss: 0.0032226014882326126\n",
      "Training epoch: 1, Batch_Num: 169/300, N_Loss: 0.02870159223675728\n",
      "Training epoch: 1, Batch_Num: 170/300, N_Loss: 0.009668994694948196\n",
      "Training epoch: 1, Batch_Num: 171/300, N_Loss: 0.011258467100560665\n",
      "Training epoch: 1, Batch_Num: 172/300, N_Loss: 0.00753304548561573\n",
      "Training epoch: 1, Batch_Num: 173/300, N_Loss: 0.0068738567642867565\n",
      "Training epoch: 1, Batch_Num: 174/300, N_Loss: 0.006968229543417692\n",
      "Training epoch: 1, Batch_Num: 175/300, N_Loss: 0.007578740362077951\n",
      "Training epoch: 1, Batch_Num: 176/300, N_Loss: 0.002971374662593007\n",
      "Training epoch: 1, Batch_Num: 177/300, N_Loss: 0.0587274432182312\n",
      "Training epoch: 1, Batch_Num: 178/300, N_Loss: 0.006188889034092426\n",
      "Training epoch: 1, Batch_Num: 179/300, N_Loss: 0.004363426938652992\n",
      "Training epoch: 1, Batch_Num: 180/300, N_Loss: 0.05573973059654236\n",
      "Training epoch: 1, Batch_Num: 181/300, N_Loss: 0.01396340224891901\n",
      "Training epoch: 1, Batch_Num: 182/300, N_Loss: 0.004100282210856676\n",
      "Training epoch: 1, Batch_Num: 183/300, N_Loss: 0.01114436611533165\n",
      "Training epoch: 1, Batch_Num: 184/300, N_Loss: 0.0008307798998430371\n",
      "Training epoch: 1, Batch_Num: 185/300, N_Loss: 0.009557755663990974\n",
      "Training epoch: 1, Batch_Num: 186/300, N_Loss: 0.005715367384254932\n",
      "Training epoch: 1, Batch_Num: 187/300, N_Loss: 0.0011555376695469022\n",
      "Training epoch: 1, Batch_Num: 188/300, N_Loss: 0.058382585644721985\n",
      "Training epoch: 1, Batch_Num: 189/300, N_Loss: 0.013652849942445755\n",
      "Training epoch: 1, Batch_Num: 190/300, N_Loss: 0.014548036269843578\n",
      "Training epoch: 1, Batch_Num: 191/300, N_Loss: 0.008966238237917423\n",
      "Training epoch: 1, Batch_Num: 192/300, N_Loss: 0.003457038663327694\n",
      "Training epoch: 1, Batch_Num: 193/300, N_Loss: 0.012286616489291191\n",
      "Training epoch: 1, Batch_Num: 194/300, N_Loss: 0.01208651252090931\n",
      "Training epoch: 1, Batch_Num: 195/300, N_Loss: 0.0891992449760437\n",
      "Training epoch: 1, Batch_Num: 196/300, N_Loss: 0.030441658571362495\n",
      "Training epoch: 1, Batch_Num: 197/300, N_Loss: 0.005956343375146389\n",
      "Training epoch: 1, Batch_Num: 198/300, N_Loss: 0.002166047692298889\n",
      "Training epoch: 1, Batch_Num: 199/300, N_Loss: 0.006229006219655275\n",
      "Training epoch: 1, Batch_Num: 200/300, N_Loss: 0.004207669757306576\n",
      "Training epoch: 1, Batch_Num: 201/300, N_Loss: 0.11251607537269592\n",
      "Training epoch: 1, Batch_Num: 202/300, N_Loss: 0.0056442818604409695\n",
      "Training epoch: 1, Batch_Num: 203/300, N_Loss: 0.004388480447232723\n",
      "Training epoch: 1, Batch_Num: 204/300, N_Loss: 0.025620151311159134\n",
      "Training epoch: 1, Batch_Num: 205/300, N_Loss: 0.002544394228607416\n",
      "Training epoch: 1, Batch_Num: 206/300, N_Loss: 0.005068744998425245\n",
      "Training epoch: 1, Batch_Num: 207/300, N_Loss: 0.004428376909345388\n",
      "Training epoch: 1, Batch_Num: 208/300, N_Loss: 0.07920108735561371\n",
      "Training epoch: 1, Batch_Num: 209/300, N_Loss: 0.025268418714404106\n",
      "Training epoch: 1, Batch_Num: 210/300, N_Loss: 0.0018797502852976322\n",
      "Training epoch: 1, Batch_Num: 211/300, N_Loss: 0.02874075621366501\n",
      "Training epoch: 1, Batch_Num: 212/300, N_Loss: 0.007100074551999569\n",
      "Training epoch: 1, Batch_Num: 213/300, N_Loss: 0.017496507614850998\n",
      "Training epoch: 1, Batch_Num: 214/300, N_Loss: 0.005021783523261547\n",
      "Training epoch: 1, Batch_Num: 215/300, N_Loss: 0.0028287465684115887\n",
      "Training epoch: 1, Batch_Num: 216/300, N_Loss: 0.0048723542131483555\n",
      "Training epoch: 1, Batch_Num: 217/300, N_Loss: 0.0029205719474703074\n",
      "Training epoch: 1, Batch_Num: 218/300, N_Loss: 0.0044097560457885265\n",
      "Training epoch: 1, Batch_Num: 219/300, N_Loss: 0.005629171617329121\n",
      "Training epoch: 1, Batch_Num: 220/300, N_Loss: 0.003842629725113511\n",
      "Training epoch: 1, Batch_Num: 221/300, N_Loss: 0.006648835726082325\n",
      "Training epoch: 1, Batch_Num: 222/300, N_Loss: 0.016823722049593925\n",
      "Training epoch: 1, Batch_Num: 223/300, N_Loss: 0.0021080123260617256\n",
      "Training epoch: 1, Batch_Num: 224/300, N_Loss: 0.009780827909708023\n",
      "Training epoch: 1, Batch_Num: 225/300, N_Loss: 0.0020509010646492243\n",
      "Training epoch: 1, Batch_Num: 226/300, N_Loss: 0.009998750872910023\n",
      "Training epoch: 1, Batch_Num: 227/300, N_Loss: 0.021223099902272224\n",
      "Training epoch: 1, Batch_Num: 228/300, N_Loss: 0.00888973381370306\n",
      "Training epoch: 1, Batch_Num: 229/300, N_Loss: 0.005504853092133999\n",
      "Training epoch: 1, Batch_Num: 230/300, N_Loss: 0.0030059346463531256\n",
      "Training epoch: 1, Batch_Num: 231/300, N_Loss: 0.017899321392178535\n",
      "Training epoch: 1, Batch_Num: 232/300, N_Loss: 0.002413512207567692\n",
      "Training epoch: 1, Batch_Num: 233/300, N_Loss: 0.009455209597945213\n",
      "Training epoch: 1, Batch_Num: 234/300, N_Loss: 0.014769126661121845\n",
      "Training epoch: 1, Batch_Num: 235/300, N_Loss: 0.021078627556562424\n",
      "Training epoch: 1, Batch_Num: 236/300, N_Loss: 0.008490683510899544\n",
      "Training epoch: 1, Batch_Num: 237/300, N_Loss: 0.0029006432741880417\n",
      "Training epoch: 1, Batch_Num: 238/300, N_Loss: 0.006345502100884914\n",
      "Training epoch: 1, Batch_Num: 239/300, N_Loss: 0.010891271755099297\n",
      "Training epoch: 1, Batch_Num: 240/300, N_Loss: 0.011825820431113243\n",
      "Training epoch: 1, Batch_Num: 241/300, N_Loss: 0.007317251991480589\n",
      "Training epoch: 1, Batch_Num: 242/300, N_Loss: 0.0016184611013159156\n",
      "Training epoch: 1, Batch_Num: 243/300, N_Loss: 0.0020086716394871473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, Batch_Num: 244/300, N_Loss: 0.011933395639061928\n",
      "Training epoch: 1, Batch_Num: 245/300, N_Loss: 0.00560403848066926\n",
      "Training epoch: 1, Batch_Num: 246/300, N_Loss: 0.1339763104915619\n",
      "Training epoch: 1, Batch_Num: 247/300, N_Loss: 0.006048221606761217\n",
      "Training epoch: 1, Batch_Num: 248/300, N_Loss: 0.003178781596943736\n",
      "Training epoch: 1, Batch_Num: 249/300, N_Loss: 0.007805480156093836\n",
      "Training epoch: 1, Batch_Num: 250/300, N_Loss: 0.01098338607698679\n",
      "Training epoch: 1, Batch_Num: 251/300, N_Loss: 0.012501158751547337\n",
      "Training epoch: 1, Batch_Num: 252/300, N_Loss: 0.04049311950802803\n",
      "Training epoch: 1, Batch_Num: 253/300, N_Loss: 0.001796227297745645\n",
      "Training epoch: 1, Batch_Num: 254/300, N_Loss: 0.01105897780507803\n",
      "Training epoch: 1, Batch_Num: 255/300, N_Loss: 0.0030630105175077915\n",
      "Training epoch: 1, Batch_Num: 256/300, N_Loss: 0.044356703758239746\n",
      "Training epoch: 1, Batch_Num: 257/300, N_Loss: 0.006553046405315399\n",
      "Training epoch: 1, Batch_Num: 258/300, N_Loss: 0.014565913006663322\n",
      "Training epoch: 1, Batch_Num: 259/300, N_Loss: 0.01834159530699253\n",
      "Training epoch: 1, Batch_Num: 260/300, N_Loss: 0.011305499821901321\n",
      "Training epoch: 1, Batch_Num: 261/300, N_Loss: 0.0016835089772939682\n",
      "Training epoch: 1, Batch_Num: 262/300, N_Loss: 0.0010978739010170102\n",
      "Training epoch: 1, Batch_Num: 263/300, N_Loss: 0.0086771659553051\n",
      "Training epoch: 1, Batch_Num: 264/300, N_Loss: 0.0015478128334507346\n",
      "Training epoch: 1, Batch_Num: 265/300, N_Loss: 0.007450340315699577\n",
      "Training epoch: 1, Batch_Num: 266/300, N_Loss: 0.0019947809632867575\n",
      "Training epoch: 1, Batch_Num: 267/300, N_Loss: 0.012103503569960594\n",
      "Training epoch: 1, Batch_Num: 268/300, N_Loss: 0.004504852928221226\n",
      "Training epoch: 1, Batch_Num: 269/300, N_Loss: 0.00455453060567379\n",
      "Training epoch: 1, Batch_Num: 270/300, N_Loss: 0.005894218105822802\n",
      "Training epoch: 1, Batch_Num: 271/300, N_Loss: 0.0031204114202409983\n",
      "Training epoch: 1, Batch_Num: 272/300, N_Loss: 0.006984424777328968\n",
      "Training epoch: 1, Batch_Num: 273/300, N_Loss: 0.008625608868896961\n",
      "Training epoch: 1, Batch_Num: 274/300, N_Loss: 0.002515146741643548\n",
      "Training epoch: 1, Batch_Num: 275/300, N_Loss: 0.05727463960647583\n",
      "Training epoch: 1, Batch_Num: 276/300, N_Loss: 0.013869473710656166\n",
      "Training epoch: 1, Batch_Num: 277/300, N_Loss: 0.003425473114475608\n",
      "Training epoch: 1, Batch_Num: 278/300, N_Loss: 0.002330888994038105\n",
      "Training epoch: 1, Batch_Num: 279/300, N_Loss: 0.007928791455924511\n",
      "Training epoch: 1, Batch_Num: 280/300, N_Loss: 0.009157247841358185\n",
      "Training epoch: 1, Batch_Num: 281/300, N_Loss: 0.004178471863269806\n",
      "Training epoch: 1, Batch_Num: 282/300, N_Loss: 0.0008328676922246814\n",
      "Training epoch: 1, Batch_Num: 283/300, N_Loss: 0.010554177686572075\n",
      "Training epoch: 1, Batch_Num: 284/300, N_Loss: 0.006243214942514896\n",
      "Training epoch: 1, Batch_Num: 285/300, N_Loss: 0.004485039040446281\n",
      "Training epoch: 1, Batch_Num: 286/300, N_Loss: 0.0185892041772604\n",
      "Training epoch: 1, Batch_Num: 287/300, N_Loss: 0.005639673676341772\n",
      "Training epoch: 1, Batch_Num: 288/300, N_Loss: 0.005875997245311737\n",
      "Training epoch: 1, Batch_Num: 289/300, N_Loss: 0.03642578050494194\n",
      "Training epoch: 1, Batch_Num: 290/300, N_Loss: 0.00594022311270237\n",
      "Training epoch: 1, Batch_Num: 291/300, N_Loss: 0.003884584177285433\n",
      "Training epoch: 1, Batch_Num: 292/300, N_Loss: 0.002675740048289299\n",
      "Training epoch: 1, Batch_Num: 293/300, N_Loss: 0.004724772647023201\n",
      "Training epoch: 1, Batch_Num: 294/300, N_Loss: 0.015637654811143875\n",
      "Training epoch: 1, Batch_Num: 295/300, N_Loss: 0.0019926901441067457\n",
      "Training epoch: 1, Batch_Num: 296/300, N_Loss: 0.005489534232765436\n",
      "Training epoch: 1, Batch_Num: 297/300, N_Loss: 0.0037627622950822115\n",
      "Training epoch: 1, Batch_Num: 298/300, N_Loss: 0.009733269922435284\n",
      "Training epoch: 1, Batch_Num: 299/300, N_Loss: 0.01867593266069889\n",
      "Training epoch: 1, Batch_Num: 300/300, N_Loss: 0.0027883530128747225\n",
      "Training epoch: 2, Batch_Num: 1/300, N_Loss: 0.01676231063902378\n",
      "Training epoch: 2, Batch_Num: 2/300, N_Loss: 0.0006272726459428668\n",
      "Training epoch: 2, Batch_Num: 3/300, N_Loss: 0.022702841088175774\n",
      "Training epoch: 2, Batch_Num: 4/300, N_Loss: 0.009843338280916214\n",
      "Training epoch: 2, Batch_Num: 5/300, N_Loss: 0.03093266487121582\n",
      "Training epoch: 2, Batch_Num: 6/300, N_Loss: 0.005866314750164747\n",
      "Training epoch: 2, Batch_Num: 7/300, N_Loss: 0.007491494528949261\n",
      "Training epoch: 2, Batch_Num: 8/300, N_Loss: 0.0033536467235535383\n",
      "Training epoch: 2, Batch_Num: 9/300, N_Loss: 0.023672273382544518\n",
      "Training epoch: 2, Batch_Num: 10/300, N_Loss: 0.0026061488315463066\n",
      "Training epoch: 2, Batch_Num: 11/300, N_Loss: 0.0017650863155722618\n",
      "Training epoch: 2, Batch_Num: 12/300, N_Loss: 0.002618268597871065\n",
      "Training epoch: 2, Batch_Num: 13/300, N_Loss: 0.002016461454331875\n",
      "Training epoch: 2, Batch_Num: 14/300, N_Loss: 0.001379248104058206\n",
      "Training epoch: 2, Batch_Num: 15/300, N_Loss: 0.0017748126992955804\n",
      "Training epoch: 2, Batch_Num: 16/300, N_Loss: 0.008971183560788631\n",
      "Training epoch: 2, Batch_Num: 17/300, N_Loss: 0.005690938327461481\n",
      "Training epoch: 2, Batch_Num: 18/300, N_Loss: 0.002518187742680311\n",
      "Training epoch: 2, Batch_Num: 19/300, N_Loss: 0.005500889848917723\n",
      "Training epoch: 2, Batch_Num: 20/300, N_Loss: 0.006210680119693279\n",
      "Training epoch: 2, Batch_Num: 21/300, N_Loss: 0.010004407726228237\n",
      "Training epoch: 2, Batch_Num: 22/300, N_Loss: 0.007432477083057165\n",
      "Training epoch: 2, Batch_Num: 23/300, N_Loss: 0.004003008361905813\n",
      "Training epoch: 2, Batch_Num: 24/300, N_Loss: 0.004090722184628248\n",
      "Training epoch: 2, Batch_Num: 25/300, N_Loss: 0.0012064409675076604\n",
      "Training epoch: 2, Batch_Num: 26/300, N_Loss: 0.005618744995445013\n",
      "Training epoch: 2, Batch_Num: 27/300, N_Loss: 0.003860620316118002\n",
      "Training epoch: 2, Batch_Num: 28/300, N_Loss: 0.008846261538565159\n",
      "Training epoch: 2, Batch_Num: 29/300, N_Loss: 0.00521987397223711\n",
      "Training epoch: 2, Batch_Num: 30/300, N_Loss: 0.0018335614586248994\n",
      "Training epoch: 2, Batch_Num: 31/300, N_Loss: 0.019076162949204445\n",
      "Training epoch: 2, Batch_Num: 32/300, N_Loss: 0.008692140690982342\n",
      "Training epoch: 2, Batch_Num: 33/300, N_Loss: 0.01926671527326107\n",
      "Training epoch: 2, Batch_Num: 34/300, N_Loss: 0.0022262954153120518\n",
      "Training epoch: 2, Batch_Num: 35/300, N_Loss: 0.0055091967806220055\n",
      "Training epoch: 2, Batch_Num: 36/300, N_Loss: 0.017751391977071762\n",
      "Training epoch: 2, Batch_Num: 37/300, N_Loss: 0.005198492668569088\n",
      "Training epoch: 2, Batch_Num: 38/300, N_Loss: 0.0073873866349458694\n",
      "Training epoch: 2, Batch_Num: 39/300, N_Loss: 0.0068924883380532265\n",
      "Training epoch: 2, Batch_Num: 40/300, N_Loss: 0.059464093297719955\n",
      "Training epoch: 2, Batch_Num: 41/300, N_Loss: 0.050864726305007935\n",
      "Training epoch: 2, Batch_Num: 42/300, N_Loss: 0.115263931453228\n",
      "Training epoch: 2, Batch_Num: 43/300, N_Loss: 0.02053164131939411\n",
      "Training epoch: 2, Batch_Num: 44/300, N_Loss: 0.007383293472230434\n",
      "Training epoch: 2, Batch_Num: 45/300, N_Loss: 0.01470334641635418\n",
      "Training epoch: 2, Batch_Num: 46/300, N_Loss: 0.007379945367574692\n",
      "Training epoch: 2, Batch_Num: 47/300, N_Loss: 0.01885417103767395\n",
      "Training epoch: 2, Batch_Num: 48/300, N_Loss: 0.005736930295825005\n",
      "Training epoch: 2, Batch_Num: 49/300, N_Loss: 0.08192303031682968\n",
      "Training epoch: 2, Batch_Num: 50/300, N_Loss: 0.00180701888166368\n",
      "Training epoch: 2, Batch_Num: 51/300, N_Loss: 0.003885605139657855\n",
      "Training epoch: 2, Batch_Num: 52/300, N_Loss: 0.00323117570951581\n",
      "Training epoch: 2, Batch_Num: 53/300, N_Loss: 0.0243338905274868\n",
      "Training epoch: 2, Batch_Num: 54/300, N_Loss: 0.016360528767108917\n",
      "Training epoch: 2, Batch_Num: 55/300, N_Loss: 0.007569551933556795\n",
      "Training epoch: 2, Batch_Num: 56/300, N_Loss: 0.0008400672231800854\n",
      "Training epoch: 2, Batch_Num: 57/300, N_Loss: 0.011925255879759789\n",
      "Training epoch: 2, Batch_Num: 58/300, N_Loss: 0.011716328561306\n",
      "Training epoch: 2, Batch_Num: 59/300, N_Loss: 0.003182146232575178\n",
      "Training epoch: 2, Batch_Num: 60/300, N_Loss: 0.0015417536487802863\n",
      "Training epoch: 2, Batch_Num: 61/300, N_Loss: 0.001736838836222887\n",
      "Training epoch: 2, Batch_Num: 62/300, N_Loss: 0.04902535676956177\n",
      "Training epoch: 2, Batch_Num: 63/300, N_Loss: 0.010514688678085804\n",
      "Training epoch: 2, Batch_Num: 64/300, N_Loss: 0.0009598693577572703\n",
      "Training epoch: 2, Batch_Num: 65/300, N_Loss: 0.05587299168109894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2, Batch_Num: 66/300, N_Loss: 0.001087216311134398\n",
      "Training epoch: 2, Batch_Num: 67/300, N_Loss: 0.0022157570347189903\n",
      "Training epoch: 2, Batch_Num: 68/300, N_Loss: 0.006237523164600134\n",
      "Training epoch: 2, Batch_Num: 69/300, N_Loss: 0.030269749462604523\n",
      "Training epoch: 2, Batch_Num: 70/300, N_Loss: 0.023345094174146652\n",
      "Training epoch: 2, Batch_Num: 71/300, N_Loss: 0.010570627637207508\n",
      "Training epoch: 2, Batch_Num: 72/300, N_Loss: 0.004335569683462381\n",
      "Training epoch: 2, Batch_Num: 73/300, N_Loss: 0.011113767512142658\n",
      "Training epoch: 2, Batch_Num: 74/300, N_Loss: 0.008389235474169254\n",
      "Training epoch: 2, Batch_Num: 75/300, N_Loss: 0.013818135485053062\n",
      "Training epoch: 2, Batch_Num: 76/300, N_Loss: 0.019567513838410378\n",
      "Training epoch: 2, Batch_Num: 77/300, N_Loss: 0.0068444060161709785\n",
      "Training epoch: 2, Batch_Num: 78/300, N_Loss: 0.002373325638473034\n",
      "Training epoch: 2, Batch_Num: 79/300, N_Loss: 0.009935220703482628\n",
      "Training epoch: 2, Batch_Num: 80/300, N_Loss: 0.07856884598731995\n",
      "Training epoch: 2, Batch_Num: 81/300, N_Loss: 0.01932658813893795\n",
      "Training epoch: 2, Batch_Num: 82/300, N_Loss: 0.0030110806692391634\n",
      "Training epoch: 2, Batch_Num: 83/300, N_Loss: 0.003319019917398691\n",
      "Training epoch: 2, Batch_Num: 84/300, N_Loss: 0.08722583204507828\n",
      "Training epoch: 2, Batch_Num: 85/300, N_Loss: 0.0035111443139612675\n",
      "Training epoch: 2, Batch_Num: 86/300, N_Loss: 0.09823548048734665\n",
      "Training epoch: 2, Batch_Num: 87/300, N_Loss: 0.0029962765984237194\n",
      "Training epoch: 2, Batch_Num: 88/300, N_Loss: 0.0025584581308066845\n",
      "Training epoch: 2, Batch_Num: 89/300, N_Loss: 0.0029374954756349325\n",
      "Training epoch: 2, Batch_Num: 90/300, N_Loss: 0.00079390435712412\n",
      "Training epoch: 2, Batch_Num: 91/300, N_Loss: 0.004483554977923632\n",
      "Training epoch: 2, Batch_Num: 92/300, N_Loss: 0.00242698285728693\n",
      "Training epoch: 2, Batch_Num: 93/300, N_Loss: 0.130547896027565\n",
      "Training epoch: 2, Batch_Num: 94/300, N_Loss: 0.004291329067200422\n",
      "Training epoch: 2, Batch_Num: 95/300, N_Loss: 0.007940499112010002\n",
      "Training epoch: 2, Batch_Num: 96/300, N_Loss: 0.007858699187636375\n",
      "Training epoch: 2, Batch_Num: 97/300, N_Loss: 0.05551876872777939\n",
      "Training epoch: 2, Batch_Num: 98/300, N_Loss: 0.07895088940858841\n",
      "Training epoch: 2, Batch_Num: 99/300, N_Loss: 0.00977789144963026\n",
      "Training epoch: 2, Batch_Num: 100/300, N_Loss: 0.03501717746257782\n",
      "Training epoch: 2, Batch_Num: 101/300, N_Loss: 0.005149134900420904\n",
      "Training epoch: 2, Batch_Num: 102/300, N_Loss: 0.004191792570054531\n",
      "Training epoch: 2, Batch_Num: 103/300, N_Loss: 0.013549448922276497\n",
      "Training epoch: 2, Batch_Num: 104/300, N_Loss: 0.006302671507000923\n",
      "Training epoch: 2, Batch_Num: 105/300, N_Loss: 0.016062719747424126\n",
      "Training epoch: 2, Batch_Num: 106/300, N_Loss: 0.09710170328617096\n",
      "Training epoch: 2, Batch_Num: 107/300, N_Loss: 0.001809593173675239\n",
      "Training epoch: 2, Batch_Num: 108/300, N_Loss: 0.0075308894738554955\n",
      "Training epoch: 2, Batch_Num: 109/300, N_Loss: 0.012326680123806\n",
      "Training epoch: 2, Batch_Num: 110/300, N_Loss: 0.0015278416685760021\n",
      "Training epoch: 2, Batch_Num: 111/300, N_Loss: 0.07730241119861603\n",
      "Training epoch: 2, Batch_Num: 112/300, N_Loss: 0.006850983016192913\n",
      "Training epoch: 2, Batch_Num: 113/300, N_Loss: 0.001352679100818932\n",
      "Training epoch: 2, Batch_Num: 114/300, N_Loss: 0.028132211416959763\n",
      "Training epoch: 2, Batch_Num: 115/300, N_Loss: 0.003085126169025898\n",
      "Training epoch: 2, Batch_Num: 116/300, N_Loss: 0.09881650656461716\n",
      "Training epoch: 2, Batch_Num: 117/300, N_Loss: 0.031099971383810043\n",
      "Training epoch: 2, Batch_Num: 118/300, N_Loss: 0.0014664476038888097\n",
      "Training epoch: 2, Batch_Num: 119/300, N_Loss: 0.006354741286486387\n",
      "Training epoch: 2, Batch_Num: 120/300, N_Loss: 0.00721316272392869\n",
      "Training epoch: 2, Batch_Num: 121/300, N_Loss: 0.009677662514150143\n",
      "Training epoch: 2, Batch_Num: 122/300, N_Loss: 0.009538225829601288\n",
      "Training epoch: 2, Batch_Num: 123/300, N_Loss: 0.003647022880613804\n",
      "Training epoch: 2, Batch_Num: 124/300, N_Loss: 0.013698020949959755\n",
      "Training epoch: 2, Batch_Num: 125/300, N_Loss: 0.008104161359369755\n",
      "Training epoch: 2, Batch_Num: 126/300, N_Loss: 0.004945393186062574\n",
      "Training epoch: 2, Batch_Num: 127/300, N_Loss: 0.002403391059488058\n",
      "Training epoch: 2, Batch_Num: 128/300, N_Loss: 0.07224702090024948\n",
      "Training epoch: 2, Batch_Num: 129/300, N_Loss: 0.018144389614462852\n",
      "Training epoch: 2, Batch_Num: 130/300, N_Loss: 0.00552590424194932\n",
      "Training epoch: 2, Batch_Num: 131/300, N_Loss: 0.036573376506567\n",
      "Training epoch: 2, Batch_Num: 132/300, N_Loss: 0.002939097350463271\n",
      "Training epoch: 2, Batch_Num: 133/300, N_Loss: 0.016917165368795395\n",
      "Training epoch: 2, Batch_Num: 134/300, N_Loss: 0.006720955949276686\n",
      "Training epoch: 2, Batch_Num: 135/300, N_Loss: 0.013264322653412819\n",
      "Training epoch: 2, Batch_Num: 136/300, N_Loss: 0.007775464095175266\n",
      "Training epoch: 2, Batch_Num: 137/300, N_Loss: 0.006007354240864515\n",
      "Training epoch: 2, Batch_Num: 138/300, N_Loss: 0.000983033562079072\n",
      "Training epoch: 2, Batch_Num: 139/300, N_Loss: 0.014289803802967072\n",
      "Training epoch: 2, Batch_Num: 140/300, N_Loss: 0.006353542674332857\n",
      "Training epoch: 2, Batch_Num: 141/300, N_Loss: 0.005104081705212593\n",
      "Training epoch: 2, Batch_Num: 142/300, N_Loss: 0.005822984501719475\n",
      "Training epoch: 2, Batch_Num: 143/300, N_Loss: 0.015600888058543205\n",
      "Training epoch: 2, Batch_Num: 144/300, N_Loss: 0.07280018925666809\n",
      "Training epoch: 2, Batch_Num: 145/300, N_Loss: 0.00480198347941041\n",
      "Training epoch: 2, Batch_Num: 146/300, N_Loss: 0.008152743801474571\n",
      "Training epoch: 2, Batch_Num: 147/300, N_Loss: 0.012707512825727463\n",
      "Training epoch: 2, Batch_Num: 148/300, N_Loss: 0.006203668192028999\n",
      "Training epoch: 2, Batch_Num: 149/300, N_Loss: 0.0022542192600667477\n",
      "Training epoch: 2, Batch_Num: 150/300, N_Loss: 0.0148848257958889\n",
      "Training epoch: 2, Batch_Num: 151/300, N_Loss: 0.0305850300937891\n",
      "Training epoch: 2, Batch_Num: 152/300, N_Loss: 0.0023084047716110945\n",
      "Training epoch: 2, Batch_Num: 153/300, N_Loss: 0.0147499218583107\n",
      "Training epoch: 2, Batch_Num: 154/300, N_Loss: 0.0032966879662126303\n",
      "Training epoch: 2, Batch_Num: 155/300, N_Loss: 0.006395981181412935\n",
      "Training epoch: 2, Batch_Num: 156/300, N_Loss: 0.027699487283825874\n",
      "Training epoch: 2, Batch_Num: 157/300, N_Loss: 0.059230878949165344\n",
      "Training epoch: 2, Batch_Num: 158/300, N_Loss: 0.007306595798581839\n",
      "Training epoch: 2, Batch_Num: 159/300, N_Loss: 0.015164886601269245\n",
      "Training epoch: 2, Batch_Num: 160/300, N_Loss: 0.003022922435775399\n",
      "Training epoch: 2, Batch_Num: 161/300, N_Loss: 0.0023122273851186037\n",
      "Training epoch: 2, Batch_Num: 162/300, N_Loss: 0.0010059516644105315\n",
      "Training epoch: 2, Batch_Num: 163/300, N_Loss: 0.002847476629540324\n",
      "Training epoch: 2, Batch_Num: 164/300, N_Loss: 0.002449105493724346\n",
      "Training epoch: 2, Batch_Num: 165/300, N_Loss: 0.011838802136480808\n",
      "Training epoch: 2, Batch_Num: 166/300, N_Loss: 0.0056517827324569225\n",
      "Training epoch: 2, Batch_Num: 167/300, N_Loss: 0.016081862151622772\n",
      "Training epoch: 2, Batch_Num: 168/300, N_Loss: 0.002640901133418083\n",
      "Training epoch: 2, Batch_Num: 169/300, N_Loss: 0.005608938168734312\n",
      "Training epoch: 2, Batch_Num: 170/300, N_Loss: 0.005250127054750919\n",
      "Training epoch: 2, Batch_Num: 171/300, N_Loss: 0.0048270574770867825\n",
      "Training epoch: 2, Batch_Num: 172/300, N_Loss: 0.002165411366149783\n",
      "Training epoch: 2, Batch_Num: 173/300, N_Loss: 0.0019123561214655638\n",
      "Training epoch: 2, Batch_Num: 174/300, N_Loss: 0.010214397683739662\n",
      "Training epoch: 2, Batch_Num: 175/300, N_Loss: 0.0057801175862550735\n",
      "Training epoch: 2, Batch_Num: 176/300, N_Loss: 0.0026608461048454046\n",
      "Training epoch: 2, Batch_Num: 177/300, N_Loss: 0.01228105928748846\n",
      "Training epoch: 2, Batch_Num: 178/300, N_Loss: 0.0014134926022961736\n",
      "Training epoch: 2, Batch_Num: 179/300, N_Loss: 0.005714781582355499\n",
      "Training epoch: 2, Batch_Num: 180/300, N_Loss: 0.01125198695808649\n",
      "Training epoch: 2, Batch_Num: 181/300, N_Loss: 0.008900556713342667\n",
      "Training epoch: 2, Batch_Num: 182/300, N_Loss: 0.016319580376148224\n",
      "Training epoch: 2, Batch_Num: 183/300, N_Loss: 0.0027752139139920473\n",
      "Training epoch: 2, Batch_Num: 184/300, N_Loss: 0.022236159071326256\n",
      "Training epoch: 2, Batch_Num: 185/300, N_Loss: 0.0018529584631323814\n",
      "Training epoch: 2, Batch_Num: 186/300, N_Loss: 0.007493092678487301\n",
      "Training epoch: 2, Batch_Num: 187/300, N_Loss: 0.012456617318093777\n",
      "Training epoch: 2, Batch_Num: 188/300, N_Loss: 0.01758439466357231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2, Batch_Num: 189/300, N_Loss: 0.008126577362418175\n",
      "Training epoch: 2, Batch_Num: 190/300, N_Loss: 0.09186608344316483\n",
      "Training epoch: 2, Batch_Num: 191/300, N_Loss: 0.055993180721998215\n",
      "Training epoch: 2, Batch_Num: 192/300, N_Loss: 0.0015692566521465778\n",
      "Training epoch: 2, Batch_Num: 193/300, N_Loss: 0.03951726481318474\n",
      "Training epoch: 2, Batch_Num: 194/300, N_Loss: 0.004888103809207678\n",
      "Training epoch: 2, Batch_Num: 195/300, N_Loss: 0.0012825188459828496\n",
      "Training epoch: 2, Batch_Num: 196/300, N_Loss: 0.0014919238165020943\n",
      "Training epoch: 2, Batch_Num: 197/300, N_Loss: 0.01925252564251423\n",
      "Training epoch: 2, Batch_Num: 198/300, N_Loss: 0.01498046051710844\n",
      "Training epoch: 2, Batch_Num: 199/300, N_Loss: 0.003260992933064699\n",
      "Training epoch: 2, Batch_Num: 200/300, N_Loss: 0.004859634209424257\n",
      "Training epoch: 2, Batch_Num: 201/300, N_Loss: 0.05417853966355324\n",
      "Training epoch: 2, Batch_Num: 202/300, N_Loss: 0.0019948475528508425\n",
      "Training epoch: 2, Batch_Num: 203/300, N_Loss: 0.0032970281317830086\n",
      "Training epoch: 2, Batch_Num: 204/300, N_Loss: 0.0008248828817158937\n",
      "Training epoch: 2, Batch_Num: 205/300, N_Loss: 0.036902185529470444\n",
      "Training epoch: 2, Batch_Num: 206/300, N_Loss: 0.014040221460163593\n",
      "Training epoch: 2, Batch_Num: 207/300, N_Loss: 0.003316288348287344\n",
      "Training epoch: 2, Batch_Num: 208/300, N_Loss: 0.0174347423017025\n",
      "Training epoch: 2, Batch_Num: 209/300, N_Loss: 0.02677294611930847\n",
      "Training epoch: 2, Batch_Num: 210/300, N_Loss: 0.003028653096407652\n",
      "Training epoch: 2, Batch_Num: 211/300, N_Loss: 0.0016602919204160571\n",
      "Training epoch: 2, Batch_Num: 212/300, N_Loss: 0.022306956350803375\n",
      "Training epoch: 2, Batch_Num: 213/300, N_Loss: 0.015160013921558857\n",
      "Training epoch: 2, Batch_Num: 214/300, N_Loss: 0.006258922629058361\n",
      "Training epoch: 2, Batch_Num: 215/300, N_Loss: 0.0030385281424969435\n",
      "Training epoch: 2, Batch_Num: 216/300, N_Loss: 0.0022019303869456053\n",
      "Training epoch: 2, Batch_Num: 217/300, N_Loss: 0.009903807193040848\n",
      "Training epoch: 2, Batch_Num: 218/300, N_Loss: 0.0015123247867450118\n",
      "Training epoch: 2, Batch_Num: 219/300, N_Loss: 0.010805689729750156\n",
      "Training epoch: 2, Batch_Num: 220/300, N_Loss: 0.004432800225913525\n",
      "Training epoch: 2, Batch_Num: 221/300, N_Loss: 0.0066903987899422646\n",
      "Training epoch: 2, Batch_Num: 222/300, N_Loss: 0.0044778515584766865\n",
      "Training epoch: 2, Batch_Num: 223/300, N_Loss: 0.0022570013534277678\n",
      "Training epoch: 2, Batch_Num: 224/300, N_Loss: 0.006203745026141405\n",
      "Training epoch: 2, Batch_Num: 225/300, N_Loss: 0.004585609305649996\n",
      "Training epoch: 2, Batch_Num: 226/300, N_Loss: 0.0010948918061330914\n",
      "Training epoch: 2, Batch_Num: 227/300, N_Loss: 0.014171692542731762\n",
      "Training epoch: 2, Batch_Num: 228/300, N_Loss: 0.0017659703735262156\n",
      "Training epoch: 2, Batch_Num: 229/300, N_Loss: 0.011081153526902199\n",
      "Training epoch: 2, Batch_Num: 230/300, N_Loss: 0.03602466359734535\n",
      "Training epoch: 2, Batch_Num: 231/300, N_Loss: 0.004182657226920128\n",
      "Training epoch: 2, Batch_Num: 232/300, N_Loss: 0.013302408158779144\n",
      "Training epoch: 2, Batch_Num: 233/300, N_Loss: 0.002025933936238289\n",
      "Training epoch: 2, Batch_Num: 234/300, N_Loss: 0.00493065919727087\n",
      "Training epoch: 2, Batch_Num: 235/300, N_Loss: 0.0008440862875431776\n",
      "Training epoch: 2, Batch_Num: 236/300, N_Loss: 0.0020487471483647823\n",
      "Training epoch: 2, Batch_Num: 237/300, N_Loss: 0.004193730652332306\n",
      "Training epoch: 2, Batch_Num: 238/300, N_Loss: 0.0016410273965448141\n",
      "Training epoch: 2, Batch_Num: 239/300, N_Loss: 0.002973367692902684\n",
      "Training epoch: 2, Batch_Num: 240/300, N_Loss: 0.011857901699841022\n",
      "Training epoch: 2, Batch_Num: 241/300, N_Loss: 0.034503381699323654\n",
      "Training epoch: 2, Batch_Num: 242/300, N_Loss: 0.011110588908195496\n",
      "Training epoch: 2, Batch_Num: 243/300, N_Loss: 0.0016739364946261048\n",
      "Training epoch: 2, Batch_Num: 244/300, N_Loss: 0.007976126857101917\n",
      "Training epoch: 2, Batch_Num: 245/300, N_Loss: 0.01946297660470009\n",
      "Training epoch: 2, Batch_Num: 246/300, N_Loss: 0.0021803900599479675\n",
      "Training epoch: 2, Batch_Num: 247/300, N_Loss: 0.0036684887018054724\n",
      "Training epoch: 2, Batch_Num: 248/300, N_Loss: 0.003107252065092325\n",
      "Training epoch: 2, Batch_Num: 249/300, N_Loss: 0.007668017875403166\n",
      "Training epoch: 2, Batch_Num: 250/300, N_Loss: 0.006711545865982771\n",
      "Training epoch: 2, Batch_Num: 251/300, N_Loss: 0.005699534434825182\n",
      "Training epoch: 2, Batch_Num: 252/300, N_Loss: 0.05040840432047844\n",
      "Training epoch: 2, Batch_Num: 253/300, N_Loss: 0.003084344556555152\n",
      "Training epoch: 2, Batch_Num: 254/300, N_Loss: 0.001063431380316615\n",
      "Training epoch: 2, Batch_Num: 255/300, N_Loss: 0.0027157235890626907\n",
      "Training epoch: 2, Batch_Num: 256/300, N_Loss: 0.014367143623530865\n",
      "Training epoch: 2, Batch_Num: 257/300, N_Loss: 0.009093036875128746\n",
      "Training epoch: 2, Batch_Num: 258/300, N_Loss: 0.028556296601891518\n",
      "Training epoch: 2, Batch_Num: 259/300, N_Loss: 0.00438501313328743\n",
      "Training epoch: 2, Batch_Num: 260/300, N_Loss: 0.004129360429942608\n",
      "Training epoch: 2, Batch_Num: 261/300, N_Loss: 0.004835197702050209\n",
      "Training epoch: 2, Batch_Num: 262/300, N_Loss: 0.02087528072297573\n",
      "Training epoch: 2, Batch_Num: 263/300, N_Loss: 0.007729905657470226\n",
      "Training epoch: 2, Batch_Num: 264/300, N_Loss: 0.014105681329965591\n",
      "Training epoch: 2, Batch_Num: 265/300, N_Loss: 0.008437955752015114\n",
      "Training epoch: 2, Batch_Num: 266/300, N_Loss: 0.16181769967079163\n",
      "Training epoch: 2, Batch_Num: 267/300, N_Loss: 0.05279628932476044\n",
      "Training epoch: 2, Batch_Num: 268/300, N_Loss: 0.01907150261104107\n",
      "Training epoch: 2, Batch_Num: 269/300, N_Loss: 0.010674077086150646\n",
      "Training epoch: 2, Batch_Num: 270/300, N_Loss: 0.03723989799618721\n",
      "Training epoch: 2, Batch_Num: 271/300, N_Loss: 0.0029066193383187056\n",
      "Training epoch: 2, Batch_Num: 272/300, N_Loss: 0.006116538308560848\n",
      "Training epoch: 2, Batch_Num: 273/300, N_Loss: 0.010950591415166855\n",
      "Training epoch: 2, Batch_Num: 274/300, N_Loss: 0.0036381487734615803\n",
      "Training epoch: 2, Batch_Num: 275/300, N_Loss: 0.005244424566626549\n",
      "Training epoch: 2, Batch_Num: 276/300, N_Loss: 0.0023790248669683933\n",
      "Training epoch: 2, Batch_Num: 277/300, N_Loss: 0.0032477378845214844\n",
      "Training epoch: 2, Batch_Num: 278/300, N_Loss: 0.011899658478796482\n",
      "Training epoch: 2, Batch_Num: 279/300, N_Loss: 0.03766242042183876\n",
      "Training epoch: 2, Batch_Num: 280/300, N_Loss: 0.005219443701207638\n",
      "Training epoch: 2, Batch_Num: 281/300, N_Loss: 0.004864703398197889\n",
      "Training epoch: 2, Batch_Num: 282/300, N_Loss: 0.013032481074333191\n",
      "Training epoch: 2, Batch_Num: 283/300, N_Loss: 0.008240721188485622\n",
      "Training epoch: 2, Batch_Num: 284/300, N_Loss: 0.006754494272172451\n",
      "Training epoch: 2, Batch_Num: 285/300, N_Loss: 0.0018774333875626326\n",
      "Training epoch: 2, Batch_Num: 286/300, N_Loss: 0.00464965682476759\n",
      "Training epoch: 2, Batch_Num: 287/300, N_Loss: 0.00676195090636611\n",
      "Training epoch: 2, Batch_Num: 288/300, N_Loss: 0.0193517804145813\n",
      "Training epoch: 2, Batch_Num: 289/300, N_Loss: 0.0030843301210552454\n",
      "Training epoch: 2, Batch_Num: 290/300, N_Loss: 0.005451651755720377\n",
      "Training epoch: 2, Batch_Num: 291/300, N_Loss: 0.009271779097616673\n",
      "Training epoch: 2, Batch_Num: 292/300, N_Loss: 0.00299687753431499\n",
      "Training epoch: 2, Batch_Num: 293/300, N_Loss: 0.010490710847079754\n",
      "Training epoch: 2, Batch_Num: 294/300, N_Loss: 0.009873693808913231\n",
      "Training epoch: 2, Batch_Num: 295/300, N_Loss: 0.002746545011177659\n",
      "Training epoch: 2, Batch_Num: 296/300, N_Loss: 0.014445080421864986\n",
      "Training epoch: 2, Batch_Num: 297/300, N_Loss: 0.002673383802175522\n",
      "Training epoch: 2, Batch_Num: 298/300, N_Loss: 0.013975701294839382\n",
      "Training epoch: 2, Batch_Num: 299/300, N_Loss: 0.0014849831350147724\n",
      "Training epoch: 2, Batch_Num: 300/300, N_Loss: 0.009081538766622543\n",
      "Training epoch: 3, Batch_Num: 1/300, N_Loss: 0.03487779200077057\n",
      "Training epoch: 3, Batch_Num: 2/300, N_Loss: 0.0237923264503479\n",
      "Training epoch: 3, Batch_Num: 3/300, N_Loss: 0.008930015377700329\n",
      "Training epoch: 3, Batch_Num: 4/300, N_Loss: 0.0028903300408273935\n",
      "Training epoch: 3, Batch_Num: 5/300, N_Loss: 0.004815206862986088\n",
      "Training epoch: 3, Batch_Num: 6/300, N_Loss: 0.0037889627274125814\n",
      "Training epoch: 3, Batch_Num: 7/300, N_Loss: 0.004484876058995724\n",
      "Training epoch: 3, Batch_Num: 8/300, N_Loss: 0.02258400060236454\n",
      "Training epoch: 3, Batch_Num: 9/300, N_Loss: 0.03155166283249855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3, Batch_Num: 10/300, N_Loss: 0.002364529762417078\n",
      "Training epoch: 3, Batch_Num: 11/300, N_Loss: 0.002568240277469158\n",
      "Training epoch: 3, Batch_Num: 12/300, N_Loss: 0.005707279313355684\n",
      "Training epoch: 3, Batch_Num: 13/300, N_Loss: 0.0016194083727896214\n",
      "Training epoch: 3, Batch_Num: 14/300, N_Loss: 0.004079326521605253\n",
      "Training epoch: 3, Batch_Num: 15/300, N_Loss: 0.01226155087351799\n",
      "Training epoch: 3, Batch_Num: 16/300, N_Loss: 0.009447552263736725\n",
      "Training epoch: 3, Batch_Num: 17/300, N_Loss: 0.01975606381893158\n",
      "Training epoch: 3, Batch_Num: 18/300, N_Loss: 0.008002539165318012\n",
      "Training epoch: 3, Batch_Num: 19/300, N_Loss: 0.02202012948691845\n",
      "Training epoch: 3, Batch_Num: 20/300, N_Loss: 0.005707562435418367\n",
      "Training epoch: 3, Batch_Num: 21/300, N_Loss: 0.006105051375925541\n",
      "Training epoch: 3, Batch_Num: 22/300, N_Loss: 0.01186669897288084\n",
      "Training epoch: 3, Batch_Num: 23/300, N_Loss: 0.00752651272341609\n",
      "Training epoch: 3, Batch_Num: 24/300, N_Loss: 0.0023095295764505863\n",
      "Training epoch: 3, Batch_Num: 25/300, N_Loss: 0.0044534471817314625\n",
      "Training epoch: 3, Batch_Num: 26/300, N_Loss: 0.012234599329531193\n",
      "Training epoch: 3, Batch_Num: 27/300, N_Loss: 0.0018391539342701435\n",
      "Training epoch: 3, Batch_Num: 28/300, N_Loss: 0.007476405240595341\n",
      "Training epoch: 3, Batch_Num: 29/300, N_Loss: 0.009453199803829193\n",
      "Training epoch: 3, Batch_Num: 30/300, N_Loss: 0.0013688040198758245\n",
      "Training epoch: 3, Batch_Num: 31/300, N_Loss: 0.0035473641473799944\n",
      "Training epoch: 3, Batch_Num: 32/300, N_Loss: 0.0024997105356305838\n",
      "Training epoch: 3, Batch_Num: 33/300, N_Loss: 0.0024187795352190733\n",
      "Training epoch: 3, Batch_Num: 34/300, N_Loss: 0.0033548339270055294\n",
      "Training epoch: 3, Batch_Num: 35/300, N_Loss: 0.02990371361374855\n",
      "Training epoch: 3, Batch_Num: 36/300, N_Loss: 0.020673757418990135\n",
      "Training epoch: 3, Batch_Num: 37/300, N_Loss: 0.01740071550011635\n",
      "Training epoch: 3, Batch_Num: 38/300, N_Loss: 0.006815910805016756\n",
      "Training epoch: 3, Batch_Num: 39/300, N_Loss: 0.009995165280997753\n",
      "Training epoch: 3, Batch_Num: 40/300, N_Loss: 0.007496525067836046\n",
      "Training epoch: 3, Batch_Num: 41/300, N_Loss: 0.018948206678032875\n",
      "Training epoch: 3, Batch_Num: 42/300, N_Loss: 0.007153162732720375\n",
      "Training epoch: 3, Batch_Num: 43/300, N_Loss: 0.007041181903332472\n",
      "Training epoch: 3, Batch_Num: 44/300, N_Loss: 0.005662542302161455\n",
      "Training epoch: 3, Batch_Num: 45/300, N_Loss: 0.0024789958260953426\n",
      "Training epoch: 3, Batch_Num: 46/300, N_Loss: 0.018192855641245842\n",
      "Training epoch: 3, Batch_Num: 47/300, N_Loss: 0.002420735778287053\n",
      "Training epoch: 3, Batch_Num: 48/300, N_Loss: 0.002769045066088438\n",
      "Training epoch: 3, Batch_Num: 49/300, N_Loss: 0.04593098536133766\n",
      "Training epoch: 3, Batch_Num: 50/300, N_Loss: 0.00640097726136446\n",
      "Training epoch: 3, Batch_Num: 51/300, N_Loss: 0.03901013731956482\n",
      "Training epoch: 3, Batch_Num: 52/300, N_Loss: 0.003350365674123168\n",
      "Training epoch: 3, Batch_Num: 53/300, N_Loss: 0.013074737042188644\n",
      "Training epoch: 3, Batch_Num: 54/300, N_Loss: 0.02057982049882412\n",
      "Training epoch: 3, Batch_Num: 55/300, N_Loss: 0.004598531872034073\n",
      "Training epoch: 3, Batch_Num: 56/300, N_Loss: 0.006309959571808577\n",
      "Training epoch: 3, Batch_Num: 57/300, N_Loss: 0.0015059895813465118\n",
      "Training epoch: 3, Batch_Num: 58/300, N_Loss: 0.006599665153771639\n",
      "Training epoch: 3, Batch_Num: 59/300, N_Loss: 0.1908571571111679\n",
      "Training epoch: 3, Batch_Num: 60/300, N_Loss: 0.017486223950982094\n",
      "Training epoch: 3, Batch_Num: 61/300, N_Loss: 0.01746167615056038\n",
      "Training epoch: 3, Batch_Num: 62/300, N_Loss: 0.00214796862564981\n",
      "Training epoch: 3, Batch_Num: 63/300, N_Loss: 0.007245820946991444\n",
      "Training epoch: 3, Batch_Num: 64/300, N_Loss: 0.03286775201559067\n",
      "Training epoch: 3, Batch_Num: 65/300, N_Loss: 0.009321884252130985\n",
      "Training epoch: 3, Batch_Num: 66/300, N_Loss: 0.015157484449446201\n",
      "Training epoch: 3, Batch_Num: 67/300, N_Loss: 0.007754880469292402\n",
      "Training epoch: 3, Batch_Num: 68/300, N_Loss: 0.011704678647220135\n",
      "Training epoch: 3, Batch_Num: 69/300, N_Loss: 0.043788593262434006\n",
      "Training epoch: 3, Batch_Num: 70/300, N_Loss: 0.0009037460549734533\n",
      "Training epoch: 3, Batch_Num: 71/300, N_Loss: 0.0024387105368077755\n",
      "Training epoch: 3, Batch_Num: 72/300, N_Loss: 0.0056668976321816444\n",
      "Training epoch: 3, Batch_Num: 73/300, N_Loss: 0.0032056146301329136\n",
      "Training epoch: 3, Batch_Num: 74/300, N_Loss: 0.004882841370999813\n",
      "Training epoch: 3, Batch_Num: 75/300, N_Loss: 0.0036641382612288\n",
      "Training epoch: 3, Batch_Num: 76/300, N_Loss: 0.010735335759818554\n",
      "Training epoch: 3, Batch_Num: 77/300, N_Loss: 0.00456512114033103\n",
      "Training epoch: 3, Batch_Num: 78/300, N_Loss: 0.016426850110292435\n",
      "Training epoch: 3, Batch_Num: 79/300, N_Loss: 0.010942939668893814\n",
      "Training epoch: 3, Batch_Num: 80/300, N_Loss: 0.0032833770383149385\n",
      "Training epoch: 3, Batch_Num: 81/300, N_Loss: 0.002288607181981206\n",
      "Training epoch: 3, Batch_Num: 82/300, N_Loss: 0.009129432961344719\n",
      "Training epoch: 3, Batch_Num: 83/300, N_Loss: 0.005884702317416668\n",
      "Training epoch: 3, Batch_Num: 84/300, N_Loss: 0.006923627573996782\n",
      "Training epoch: 3, Batch_Num: 85/300, N_Loss: 0.00954747386276722\n",
      "Training epoch: 3, Batch_Num: 86/300, N_Loss: 0.0629735067486763\n",
      "Training epoch: 3, Batch_Num: 87/300, N_Loss: 0.046862032264471054\n",
      "Training epoch: 3, Batch_Num: 88/300, N_Loss: 0.02502986043691635\n",
      "Training epoch: 3, Batch_Num: 89/300, N_Loss: 0.10217985510826111\n",
      "Training epoch: 3, Batch_Num: 90/300, N_Loss: 0.01518986001610756\n",
      "Training epoch: 3, Batch_Num: 91/300, N_Loss: 0.0031569127459079027\n",
      "Training epoch: 3, Batch_Num: 92/300, N_Loss: 0.051292650401592255\n",
      "Training epoch: 3, Batch_Num: 93/300, N_Loss: 0.004666542634367943\n",
      "Training epoch: 3, Batch_Num: 94/300, N_Loss: 0.01472201943397522\n",
      "Training epoch: 3, Batch_Num: 95/300, N_Loss: 0.013704318553209305\n",
      "Training epoch: 3, Batch_Num: 96/300, N_Loss: 0.004570038989186287\n",
      "Training epoch: 3, Batch_Num: 97/300, N_Loss: 0.05620432645082474\n",
      "Training epoch: 3, Batch_Num: 98/300, N_Loss: 0.003447973635047674\n",
      "Training epoch: 3, Batch_Num: 99/300, N_Loss: 0.0681319460272789\n",
      "Training epoch: 3, Batch_Num: 100/300, N_Loss: 0.03494510427117348\n",
      "Training epoch: 3, Batch_Num: 101/300, N_Loss: 0.0008014877093955874\n",
      "Training epoch: 3, Batch_Num: 102/300, N_Loss: 0.0022471616975963116\n",
      "Training epoch: 3, Batch_Num: 103/300, N_Loss: 0.007004113402217627\n",
      "Training epoch: 3, Batch_Num: 104/300, N_Loss: 0.0025985955726355314\n",
      "Training epoch: 3, Batch_Num: 105/300, N_Loss: 0.011766980402171612\n",
      "Training epoch: 3, Batch_Num: 106/300, N_Loss: 0.040675561875104904\n",
      "Training epoch: 3, Batch_Num: 107/300, N_Loss: 0.003307924373075366\n",
      "Training epoch: 3, Batch_Num: 108/300, N_Loss: 0.0017522223060950637\n",
      "Training epoch: 3, Batch_Num: 109/300, N_Loss: 0.0009519917657598853\n",
      "Training epoch: 3, Batch_Num: 110/300, N_Loss: 0.004556544125080109\n",
      "Training epoch: 3, Batch_Num: 111/300, N_Loss: 0.00596137810498476\n",
      "Training epoch: 3, Batch_Num: 112/300, N_Loss: 0.006365304347127676\n",
      "Training epoch: 3, Batch_Num: 113/300, N_Loss: 0.004454932641237974\n",
      "Training epoch: 3, Batch_Num: 114/300, N_Loss: 0.00414786534383893\n",
      "Training epoch: 3, Batch_Num: 115/300, N_Loss: 0.01698298566043377\n",
      "Training epoch: 3, Batch_Num: 116/300, N_Loss: 0.011378640308976173\n",
      "Training epoch: 3, Batch_Num: 117/300, N_Loss: 0.002254076534882188\n",
      "Training epoch: 3, Batch_Num: 118/300, N_Loss: 0.009052985347807407\n",
      "Training epoch: 3, Batch_Num: 119/300, N_Loss: 0.0021298460196703672\n",
      "Training epoch: 3, Batch_Num: 120/300, N_Loss: 0.0025280050467699766\n",
      "Training epoch: 3, Batch_Num: 121/300, N_Loss: 0.019923333078622818\n",
      "Training epoch: 3, Batch_Num: 122/300, N_Loss: 0.0047188992612063885\n",
      "Training epoch: 3, Batch_Num: 123/300, N_Loss: 0.006807651836425066\n",
      "Training epoch: 3, Batch_Num: 124/300, N_Loss: 0.004526793956756592\n",
      "Training epoch: 3, Batch_Num: 125/300, N_Loss: 0.0035647067707031965\n",
      "Training epoch: 3, Batch_Num: 126/300, N_Loss: 0.0026232306845486164\n",
      "Training epoch: 3, Batch_Num: 127/300, N_Loss: 0.006638451479375362\n",
      "Training epoch: 3, Batch_Num: 128/300, N_Loss: 0.02387455478310585\n",
      "Training epoch: 3, Batch_Num: 129/300, N_Loss: 0.009760098531842232\n",
      "Training epoch: 3, Batch_Num: 130/300, N_Loss: 0.02963198907673359\n",
      "Training epoch: 3, Batch_Num: 131/300, N_Loss: 0.0024824070278555155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3, Batch_Num: 132/300, N_Loss: 0.004556579980999231\n",
      "Training epoch: 3, Batch_Num: 133/300, N_Loss: 0.012230819091200829\n",
      "Training epoch: 3, Batch_Num: 134/300, N_Loss: 0.01441435981541872\n",
      "Training epoch: 3, Batch_Num: 135/300, N_Loss: 0.013912755064666271\n",
      "Training epoch: 3, Batch_Num: 136/300, N_Loss: 0.0025245342403650284\n",
      "Training epoch: 3, Batch_Num: 137/300, N_Loss: 0.0027825788129121065\n",
      "Training epoch: 3, Batch_Num: 138/300, N_Loss: 0.0029857915360480547\n",
      "Training epoch: 3, Batch_Num: 139/300, N_Loss: 0.010494417510926723\n",
      "Training epoch: 3, Batch_Num: 140/300, N_Loss: 0.004191020969301462\n",
      "Training epoch: 3, Batch_Num: 141/300, N_Loss: 0.0017478861846029758\n",
      "Training epoch: 3, Batch_Num: 142/300, N_Loss: 0.0023930950555950403\n",
      "Training epoch: 3, Batch_Num: 143/300, N_Loss: 0.0020164300221949816\n",
      "Training epoch: 3, Batch_Num: 144/300, N_Loss: 0.020128604024648666\n",
      "Training epoch: 3, Batch_Num: 145/300, N_Loss: 0.0027925546746701\n",
      "Training epoch: 3, Batch_Num: 146/300, N_Loss: 0.004246331751346588\n",
      "Training epoch: 3, Batch_Num: 147/300, N_Loss: 0.0032592127099633217\n",
      "Training epoch: 3, Batch_Num: 148/300, N_Loss: 0.0008728253305889666\n",
      "Training epoch: 3, Batch_Num: 149/300, N_Loss: 0.0017737160669639707\n",
      "Training epoch: 3, Batch_Num: 150/300, N_Loss: 0.0019187971483916044\n",
      "Training epoch: 3, Batch_Num: 151/300, N_Loss: 0.0013205165741965175\n",
      "Training epoch: 3, Batch_Num: 152/300, N_Loss: 0.002186807803809643\n",
      "Training epoch: 3, Batch_Num: 153/300, N_Loss: 0.0018529575318098068\n",
      "Training epoch: 3, Batch_Num: 154/300, N_Loss: 0.0016879759496077895\n",
      "Training epoch: 3, Batch_Num: 155/300, N_Loss: 0.00984907429665327\n",
      "Training epoch: 3, Batch_Num: 156/300, N_Loss: 0.0046927775256335735\n",
      "Training epoch: 3, Batch_Num: 157/300, N_Loss: 0.018376152962446213\n",
      "Training epoch: 3, Batch_Num: 158/300, N_Loss: 0.024872105568647385\n",
      "Training epoch: 3, Batch_Num: 159/300, N_Loss: 0.008441072888672352\n",
      "Training epoch: 3, Batch_Num: 160/300, N_Loss: 0.01750340685248375\n",
      "Training epoch: 3, Batch_Num: 161/300, N_Loss: 0.004086610861122608\n",
      "Training epoch: 3, Batch_Num: 162/300, N_Loss: 0.021506544202566147\n",
      "Training epoch: 3, Batch_Num: 163/300, N_Loss: 0.053358159959316254\n",
      "Training epoch: 3, Batch_Num: 164/300, N_Loss: 0.019480103626847267\n",
      "Training epoch: 3, Batch_Num: 165/300, N_Loss: 0.027939889580011368\n",
      "Training epoch: 3, Batch_Num: 166/300, N_Loss: 0.003083104034885764\n",
      "Training epoch: 3, Batch_Num: 167/300, N_Loss: 0.007610116619616747\n",
      "Training epoch: 3, Batch_Num: 168/300, N_Loss: 0.007756820414215326\n",
      "Training epoch: 3, Batch_Num: 169/300, N_Loss: 0.01023796945810318\n",
      "Training epoch: 3, Batch_Num: 170/300, N_Loss: 0.007632277440279722\n",
      "Training epoch: 3, Batch_Num: 171/300, N_Loss: 0.0016244552098214626\n",
      "Training epoch: 3, Batch_Num: 172/300, N_Loss: 0.014494078233838081\n",
      "Training epoch: 3, Batch_Num: 173/300, N_Loss: 0.024881141260266304\n",
      "Training epoch: 3, Batch_Num: 174/300, N_Loss: 0.000743828306440264\n",
      "Training epoch: 3, Batch_Num: 175/300, N_Loss: 0.07381101697683334\n",
      "Training epoch: 3, Batch_Num: 176/300, N_Loss: 0.0005794352036900818\n",
      "Training epoch: 3, Batch_Num: 177/300, N_Loss: 0.006366852205246687\n",
      "Training epoch: 3, Batch_Num: 178/300, N_Loss: 0.012766506522893906\n",
      "Training epoch: 3, Batch_Num: 179/300, N_Loss: 0.003876433242112398\n",
      "Training epoch: 3, Batch_Num: 180/300, N_Loss: 0.006684248335659504\n",
      "Training epoch: 3, Batch_Num: 181/300, N_Loss: 0.14926043152809143\n",
      "Training epoch: 3, Batch_Num: 182/300, N_Loss: 0.00642423564568162\n",
      "Training epoch: 3, Batch_Num: 183/300, N_Loss: 0.0023054322227835655\n",
      "Training epoch: 3, Batch_Num: 184/300, N_Loss: 0.014417242258787155\n",
      "Training epoch: 3, Batch_Num: 185/300, N_Loss: 0.0018203341169282794\n",
      "Training epoch: 3, Batch_Num: 186/300, N_Loss: 0.003561176359653473\n",
      "Training epoch: 3, Batch_Num: 187/300, N_Loss: 0.054703615605831146\n",
      "Training epoch: 3, Batch_Num: 188/300, N_Loss: 0.149357408285141\n",
      "Training epoch: 3, Batch_Num: 189/300, N_Loss: 0.00490340031683445\n",
      "Training epoch: 3, Batch_Num: 190/300, N_Loss: 0.004987043794244528\n",
      "Training epoch: 3, Batch_Num: 191/300, N_Loss: 0.010409129783511162\n",
      "Training epoch: 3, Batch_Num: 192/300, N_Loss: 0.0471387654542923\n",
      "Training epoch: 3, Batch_Num: 193/300, N_Loss: 0.02378525584936142\n",
      "Training epoch: 3, Batch_Num: 194/300, N_Loss: 0.006333543919026852\n",
      "Training epoch: 3, Batch_Num: 195/300, N_Loss: 0.0009632129804231226\n",
      "Training epoch: 3, Batch_Num: 196/300, N_Loss: 0.012772136367857456\n",
      "Training epoch: 3, Batch_Num: 197/300, N_Loss: 0.003182406071573496\n",
      "Training epoch: 3, Batch_Num: 198/300, N_Loss: 0.012471402063965797\n",
      "Training epoch: 3, Batch_Num: 199/300, N_Loss: 0.002210221951827407\n",
      "Training epoch: 3, Batch_Num: 200/300, N_Loss: 0.00781609769910574\n",
      "Training epoch: 3, Batch_Num: 201/300, N_Loss: 0.02580905705690384\n",
      "Training epoch: 3, Batch_Num: 202/300, N_Loss: 0.002666809130460024\n",
      "Training epoch: 3, Batch_Num: 203/300, N_Loss: 0.010257121175527573\n",
      "Training epoch: 3, Batch_Num: 204/300, N_Loss: 0.005887454375624657\n",
      "Training epoch: 3, Batch_Num: 205/300, N_Loss: 0.008506583981215954\n",
      "Training epoch: 3, Batch_Num: 206/300, N_Loss: 0.004611185751855373\n",
      "Training epoch: 3, Batch_Num: 207/300, N_Loss: 0.005106195341795683\n",
      "Training epoch: 3, Batch_Num: 208/300, N_Loss: 0.01626448519527912\n",
      "Training epoch: 3, Batch_Num: 209/300, N_Loss: 0.017277561128139496\n",
      "Training epoch: 3, Batch_Num: 210/300, N_Loss: 0.005248726345598698\n",
      "Training epoch: 3, Batch_Num: 211/300, N_Loss: 0.011340145952999592\n",
      "Training epoch: 3, Batch_Num: 212/300, N_Loss: 0.0070195551961660385\n",
      "Training epoch: 3, Batch_Num: 213/300, N_Loss: 0.006508642807602882\n",
      "Training epoch: 3, Batch_Num: 214/300, N_Loss: 0.0017422223463654518\n",
      "Training epoch: 3, Batch_Num: 215/300, N_Loss: 0.008114180527627468\n",
      "Training epoch: 3, Batch_Num: 216/300, N_Loss: 0.01714632660150528\n",
      "Training epoch: 3, Batch_Num: 217/300, N_Loss: 0.03153117373585701\n",
      "Training epoch: 3, Batch_Num: 218/300, N_Loss: 0.0024481546133756638\n",
      "Training epoch: 3, Batch_Num: 219/300, N_Loss: 0.0012706979177892208\n",
      "Training epoch: 3, Batch_Num: 220/300, N_Loss: 0.11290721595287323\n",
      "Training epoch: 3, Batch_Num: 221/300, N_Loss: 0.012932131998240948\n",
      "Training epoch: 3, Batch_Num: 222/300, N_Loss: 0.014952356927096844\n",
      "Training epoch: 3, Batch_Num: 223/300, N_Loss: 0.0021972106769680977\n",
      "Training epoch: 3, Batch_Num: 224/300, N_Loss: 0.007888905704021454\n",
      "Training epoch: 3, Batch_Num: 225/300, N_Loss: 0.006582492031157017\n",
      "Training epoch: 3, Batch_Num: 226/300, N_Loss: 0.005820117425173521\n",
      "Training epoch: 3, Batch_Num: 227/300, N_Loss: 0.01328850258141756\n",
      "Training epoch: 3, Batch_Num: 228/300, N_Loss: 0.005317971110343933\n",
      "Training epoch: 3, Batch_Num: 229/300, N_Loss: 0.0012138051679357886\n",
      "Training epoch: 3, Batch_Num: 230/300, N_Loss: 0.0019378385040909052\n",
      "Training epoch: 3, Batch_Num: 231/300, N_Loss: 0.014407447539269924\n",
      "Training epoch: 3, Batch_Num: 232/300, N_Loss: 0.00509002385661006\n",
      "Training epoch: 3, Batch_Num: 233/300, N_Loss: 0.017178164795041084\n",
      "Training epoch: 3, Batch_Num: 234/300, N_Loss: 0.0023150613997131586\n",
      "Training epoch: 3, Batch_Num: 235/300, N_Loss: 0.0015176605666056275\n",
      "Training epoch: 3, Batch_Num: 236/300, N_Loss: 0.007973006926476955\n",
      "Training epoch: 3, Batch_Num: 237/300, N_Loss: 0.014151622541248798\n",
      "Training epoch: 3, Batch_Num: 238/300, N_Loss: 0.006134113296866417\n",
      "Training epoch: 3, Batch_Num: 239/300, N_Loss: 0.004302797839045525\n",
      "Training epoch: 3, Batch_Num: 240/300, N_Loss: 0.005669938400387764\n",
      "Training epoch: 3, Batch_Num: 241/300, N_Loss: 0.001348281977698207\n",
      "Training epoch: 3, Batch_Num: 242/300, N_Loss: 0.003556804498657584\n",
      "Training epoch: 3, Batch_Num: 243/300, N_Loss: 0.010706963948905468\n",
      "Training epoch: 3, Batch_Num: 244/300, N_Loss: 0.0043055214919149876\n",
      "Training epoch: 3, Batch_Num: 245/300, N_Loss: 0.01434535626322031\n",
      "Training epoch: 3, Batch_Num: 246/300, N_Loss: 0.0016544923419132829\n",
      "Training epoch: 3, Batch_Num: 247/300, N_Loss: 0.012094802223145962\n",
      "Training epoch: 3, Batch_Num: 248/300, N_Loss: 0.010187036357820034\n",
      "Training epoch: 3, Batch_Num: 249/300, N_Loss: 0.16969776153564453\n",
      "Training epoch: 3, Batch_Num: 250/300, N_Loss: 0.05385119467973709\n",
      "Training epoch: 3, Batch_Num: 251/300, N_Loss: 0.004539479501545429\n",
      "Training epoch: 3, Batch_Num: 252/300, N_Loss: 0.009647608734667301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3, Batch_Num: 253/300, N_Loss: 0.006369466427713633\n",
      "Training epoch: 3, Batch_Num: 254/300, N_Loss: 0.013388298451900482\n",
      "Training epoch: 3, Batch_Num: 255/300, N_Loss: 0.02296917326748371\n",
      "Training epoch: 3, Batch_Num: 256/300, N_Loss: 0.09776203334331512\n",
      "Training epoch: 3, Batch_Num: 257/300, N_Loss: 0.004698325879871845\n",
      "Training epoch: 3, Batch_Num: 258/300, N_Loss: 0.004255808889865875\n",
      "Training epoch: 3, Batch_Num: 259/300, N_Loss: 0.004538253881037235\n",
      "Training epoch: 3, Batch_Num: 260/300, N_Loss: 0.00595773383975029\n",
      "Training epoch: 3, Batch_Num: 261/300, N_Loss: 0.011077850125730038\n",
      "Training epoch: 3, Batch_Num: 262/300, N_Loss: 0.0009878126438707113\n",
      "Training epoch: 3, Batch_Num: 263/300, N_Loss: 0.028214003890752792\n",
      "Training epoch: 3, Batch_Num: 264/300, N_Loss: 0.029544150456786156\n",
      "Training epoch: 3, Batch_Num: 265/300, N_Loss: 0.012917621061205864\n",
      "Training epoch: 3, Batch_Num: 266/300, N_Loss: 0.004150796215981245\n",
      "Training epoch: 3, Batch_Num: 267/300, N_Loss: 0.0037306146696209908\n",
      "Training epoch: 3, Batch_Num: 268/300, N_Loss: 0.005566398613154888\n",
      "Training epoch: 3, Batch_Num: 269/300, N_Loss: 0.0029225267935544252\n",
      "Training epoch: 3, Batch_Num: 270/300, N_Loss: 0.005579712800681591\n",
      "Training epoch: 3, Batch_Num: 271/300, N_Loss: 0.009063954465091228\n",
      "Training epoch: 3, Batch_Num: 272/300, N_Loss: 0.01938258670270443\n",
      "Training epoch: 3, Batch_Num: 273/300, N_Loss: 0.004805992357432842\n",
      "Training epoch: 3, Batch_Num: 274/300, N_Loss: 0.011006884276866913\n",
      "Training epoch: 3, Batch_Num: 275/300, N_Loss: 0.0028421529568731785\n",
      "Training epoch: 3, Batch_Num: 276/300, N_Loss: 0.0029629268683493137\n",
      "Training epoch: 3, Batch_Num: 277/300, N_Loss: 0.003877011826261878\n",
      "Training epoch: 3, Batch_Num: 278/300, N_Loss: 0.014344911091029644\n",
      "Training epoch: 3, Batch_Num: 279/300, N_Loss: 0.001961909234523773\n",
      "Training epoch: 3, Batch_Num: 280/300, N_Loss: 0.005257784388959408\n",
      "Training epoch: 3, Batch_Num: 281/300, N_Loss: 0.0019280218984931707\n",
      "Training epoch: 3, Batch_Num: 282/300, N_Loss: 0.02628193609416485\n",
      "Training epoch: 3, Batch_Num: 283/300, N_Loss: 0.004501441027969122\n",
      "Training epoch: 3, Batch_Num: 284/300, N_Loss: 0.0013865045038983226\n",
      "Training epoch: 3, Batch_Num: 285/300, N_Loss: 0.0005297936149872839\n",
      "Training epoch: 3, Batch_Num: 286/300, N_Loss: 0.004098671954125166\n",
      "Training epoch: 3, Batch_Num: 287/300, N_Loss: 0.005099671892821789\n",
      "Training epoch: 3, Batch_Num: 288/300, N_Loss: 0.0012802607379853725\n",
      "Training epoch: 3, Batch_Num: 289/300, N_Loss: 0.011587602086365223\n",
      "Training epoch: 3, Batch_Num: 290/300, N_Loss: 0.002450635191053152\n",
      "Training epoch: 3, Batch_Num: 291/300, N_Loss: 0.019013971090316772\n",
      "Training epoch: 3, Batch_Num: 292/300, N_Loss: 0.005250365007668734\n",
      "Training epoch: 3, Batch_Num: 293/300, N_Loss: 0.0021941084414720535\n",
      "Training epoch: 3, Batch_Num: 294/300, N_Loss: 0.006185624282807112\n",
      "Training epoch: 3, Batch_Num: 295/300, N_Loss: 0.007225964218378067\n",
      "Training epoch: 3, Batch_Num: 296/300, N_Loss: 0.06783997267484665\n",
      "Training epoch: 3, Batch_Num: 297/300, N_Loss: 0.1287175863981247\n",
      "Training epoch: 3, Batch_Num: 298/300, N_Loss: 0.007168739102780819\n",
      "Training epoch: 3, Batch_Num: 299/300, N_Loss: 0.05709381029009819\n",
      "Training epoch: 3, Batch_Num: 300/300, N_Loss: 0.0014461261453107\n",
      "Training epoch: 4, Batch_Num: 1/300, N_Loss: 0.0427585132420063\n",
      "Training epoch: 4, Batch_Num: 2/300, N_Loss: 0.0028339845594018698\n",
      "Training epoch: 4, Batch_Num: 3/300, N_Loss: 0.0023651106748729944\n",
      "Training epoch: 4, Batch_Num: 4/300, N_Loss: 0.012185604311525822\n",
      "Training epoch: 4, Batch_Num: 5/300, N_Loss: 0.0038065118715167046\n",
      "Training epoch: 4, Batch_Num: 6/300, N_Loss: 0.0033886267337948084\n",
      "Training epoch: 4, Batch_Num: 7/300, N_Loss: 0.0054059140384197235\n",
      "Training epoch: 4, Batch_Num: 8/300, N_Loss: 0.0034598989877849817\n",
      "Training epoch: 4, Batch_Num: 9/300, N_Loss: 0.006717875134199858\n",
      "Training epoch: 4, Batch_Num: 10/300, N_Loss: 0.0016785649349913\n",
      "Training epoch: 4, Batch_Num: 11/300, N_Loss: 0.007026131730526686\n",
      "Training epoch: 4, Batch_Num: 12/300, N_Loss: 0.0006833056104369462\n",
      "Training epoch: 4, Batch_Num: 13/300, N_Loss: 0.002322353655472398\n",
      "Training epoch: 4, Batch_Num: 14/300, N_Loss: 0.14184918999671936\n",
      "Training epoch: 4, Batch_Num: 15/300, N_Loss: 0.0026320400647819042\n",
      "Training epoch: 4, Batch_Num: 16/300, N_Loss: 0.018903184682130814\n",
      "Training epoch: 4, Batch_Num: 17/300, N_Loss: 0.015032652765512466\n",
      "Training epoch: 4, Batch_Num: 18/300, N_Loss: 0.014758659526705742\n",
      "Training epoch: 4, Batch_Num: 19/300, N_Loss: 0.004274602048099041\n",
      "Training epoch: 4, Batch_Num: 20/300, N_Loss: 0.007071713451296091\n",
      "Training epoch: 4, Batch_Num: 21/300, N_Loss: 0.019864192232489586\n",
      "Training epoch: 4, Batch_Num: 22/300, N_Loss: 0.012548623606562614\n",
      "Training epoch: 4, Batch_Num: 23/300, N_Loss: 0.005330066196620464\n",
      "Training epoch: 4, Batch_Num: 24/300, N_Loss: 0.025340402498841286\n",
      "Training epoch: 4, Batch_Num: 25/300, N_Loss: 0.007816637866199017\n",
      "Training epoch: 4, Batch_Num: 26/300, N_Loss: 0.012683077715337276\n",
      "Training epoch: 4, Batch_Num: 27/300, N_Loss: 0.0368473045527935\n",
      "Training epoch: 4, Batch_Num: 28/300, N_Loss: 0.0012274833861738443\n",
      "Training epoch: 4, Batch_Num: 29/300, N_Loss: 0.007270385045558214\n",
      "Training epoch: 4, Batch_Num: 30/300, N_Loss: 0.0030054717790335417\n",
      "Training epoch: 4, Batch_Num: 31/300, N_Loss: 0.0006464370526373386\n",
      "Training epoch: 4, Batch_Num: 32/300, N_Loss: 0.004989828914403915\n",
      "Training epoch: 4, Batch_Num: 33/300, N_Loss: 0.09165085107088089\n",
      "Training epoch: 4, Batch_Num: 34/300, N_Loss: 0.0013494205195456743\n",
      "Training epoch: 4, Batch_Num: 35/300, N_Loss: 0.008816441521048546\n",
      "Training epoch: 4, Batch_Num: 36/300, N_Loss: 0.002578041749075055\n",
      "Training epoch: 4, Batch_Num: 37/300, N_Loss: 0.0024073892273008823\n",
      "Training epoch: 4, Batch_Num: 38/300, N_Loss: 0.04583943262696266\n",
      "Training epoch: 4, Batch_Num: 39/300, N_Loss: 0.008854455314576626\n",
      "Training epoch: 4, Batch_Num: 40/300, N_Loss: 0.050913408398628235\n",
      "Training epoch: 4, Batch_Num: 41/300, N_Loss: 0.00945495255291462\n",
      "Training epoch: 4, Batch_Num: 42/300, N_Loss: 0.008012757636606693\n",
      "Training epoch: 4, Batch_Num: 43/300, N_Loss: 0.005014174617826939\n",
      "Training epoch: 4, Batch_Num: 44/300, N_Loss: 0.018144682049751282\n",
      "Training epoch: 4, Batch_Num: 45/300, N_Loss: 0.004466631915420294\n",
      "Training epoch: 4, Batch_Num: 46/300, N_Loss: 0.053917113691568375\n",
      "Training epoch: 4, Batch_Num: 47/300, N_Loss: 0.01529128011316061\n",
      "Training epoch: 4, Batch_Num: 48/300, N_Loss: 0.0022143288515508175\n",
      "Training epoch: 4, Batch_Num: 49/300, N_Loss: 0.005523210857063532\n",
      "Training epoch: 4, Batch_Num: 50/300, N_Loss: 0.0056221154518425465\n",
      "Training epoch: 4, Batch_Num: 51/300, N_Loss: 0.006319778505712748\n",
      "Training epoch: 4, Batch_Num: 52/300, N_Loss: 0.007013102527707815\n",
      "Training epoch: 4, Batch_Num: 53/300, N_Loss: 0.007668505422770977\n",
      "Training epoch: 4, Batch_Num: 54/300, N_Loss: 0.002187996171414852\n",
      "Training epoch: 4, Batch_Num: 55/300, N_Loss: 0.011807316914200783\n",
      "Training epoch: 4, Batch_Num: 56/300, N_Loss: 0.0761270523071289\n",
      "Training epoch: 4, Batch_Num: 57/300, N_Loss: 0.0011381047079339623\n",
      "Training epoch: 4, Batch_Num: 58/300, N_Loss: 0.006196934264153242\n",
      "Training epoch: 4, Batch_Num: 59/300, N_Loss: 0.06261726468801498\n",
      "Training epoch: 4, Batch_Num: 60/300, N_Loss: 0.004276562016457319\n",
      "Training epoch: 4, Batch_Num: 61/300, N_Loss: 0.006560612004250288\n",
      "Training epoch: 4, Batch_Num: 62/300, N_Loss: 0.01037051621824503\n",
      "Training epoch: 4, Batch_Num: 63/300, N_Loss: 0.002206816803663969\n",
      "Training epoch: 4, Batch_Num: 64/300, N_Loss: 0.009229175746440887\n",
      "Training epoch: 4, Batch_Num: 65/300, N_Loss: 0.014881357550621033\n",
      "Training epoch: 4, Batch_Num: 66/300, N_Loss: 0.03734607994556427\n",
      "Training epoch: 4, Batch_Num: 67/300, N_Loss: 0.005471951328217983\n",
      "Training epoch: 4, Batch_Num: 68/300, N_Loss: 0.002818197710439563\n",
      "Training epoch: 4, Batch_Num: 69/300, N_Loss: 0.05357935279607773\n",
      "Training epoch: 4, Batch_Num: 70/300, N_Loss: 0.009715666063129902\n",
      "Training epoch: 4, Batch_Num: 71/300, N_Loss: 0.12143520265817642\n",
      "Training epoch: 4, Batch_Num: 72/300, N_Loss: 0.008239476010203362\n",
      "Training epoch: 4, Batch_Num: 73/300, N_Loss: 0.001084155635908246\n",
      "Training epoch: 4, Batch_Num: 74/300, N_Loss: 0.002328745787963271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4, Batch_Num: 75/300, N_Loss: 0.002869211370125413\n",
      "Training epoch: 4, Batch_Num: 76/300, N_Loss: 0.00239937799051404\n",
      "Training epoch: 4, Batch_Num: 77/300, N_Loss: 0.0034355318639427423\n",
      "Training epoch: 4, Batch_Num: 78/300, N_Loss: 0.003560128156095743\n",
      "Training epoch: 4, Batch_Num: 79/300, N_Loss: 0.02916676551103592\n",
      "Training epoch: 4, Batch_Num: 80/300, N_Loss: 0.0015991851687431335\n",
      "Training epoch: 4, Batch_Num: 81/300, N_Loss: 0.002617166843265295\n",
      "Training epoch: 4, Batch_Num: 82/300, N_Loss: 0.003375794505700469\n",
      "Training epoch: 4, Batch_Num: 83/300, N_Loss: 0.0006687942077405751\n",
      "Training epoch: 4, Batch_Num: 84/300, N_Loss: 0.004562806338071823\n",
      "Training epoch: 4, Batch_Num: 85/300, N_Loss: 0.01394421886652708\n",
      "Training epoch: 4, Batch_Num: 86/300, N_Loss: 0.08419787138700485\n",
      "Training epoch: 4, Batch_Num: 87/300, N_Loss: 0.017477424815297127\n",
      "Training epoch: 4, Batch_Num: 88/300, N_Loss: 0.012104880064725876\n",
      "Training epoch: 4, Batch_Num: 89/300, N_Loss: 0.016518516466021538\n",
      "Training epoch: 4, Batch_Num: 90/300, N_Loss: 0.014765724539756775\n",
      "Training epoch: 4, Batch_Num: 91/300, N_Loss: 0.0044451323337852955\n",
      "Training epoch: 4, Batch_Num: 92/300, N_Loss: 0.005621208809316158\n",
      "Training epoch: 4, Batch_Num: 93/300, N_Loss: 0.004485840909183025\n",
      "Training epoch: 4, Batch_Num: 94/300, N_Loss: 0.0018602057825773954\n",
      "Training epoch: 4, Batch_Num: 95/300, N_Loss: 0.006726948078721762\n",
      "Training epoch: 4, Batch_Num: 96/300, N_Loss: 0.01984003186225891\n",
      "Training epoch: 4, Batch_Num: 97/300, N_Loss: 0.03003925457596779\n",
      "Training epoch: 4, Batch_Num: 98/300, N_Loss: 0.005443894769996405\n",
      "Training epoch: 4, Batch_Num: 99/300, N_Loss: 0.0059597124345600605\n",
      "Training epoch: 4, Batch_Num: 100/300, N_Loss: 0.0066704959608614445\n",
      "Training epoch: 4, Batch_Num: 101/300, N_Loss: 0.002022339263930917\n",
      "Training epoch: 4, Batch_Num: 102/300, N_Loss: 0.004639654885977507\n",
      "Training epoch: 4, Batch_Num: 103/300, N_Loss: 0.008484849706292152\n",
      "Training epoch: 4, Batch_Num: 104/300, N_Loss: 0.001365477335639298\n",
      "Training epoch: 4, Batch_Num: 105/300, N_Loss: 0.01541425846517086\n",
      "Training epoch: 4, Batch_Num: 106/300, N_Loss: 0.015918588265776634\n",
      "Training epoch: 4, Batch_Num: 107/300, N_Loss: 0.0017761829076334834\n",
      "Training epoch: 4, Batch_Num: 108/300, N_Loss: 0.014753798022866249\n",
      "Training epoch: 4, Batch_Num: 109/300, N_Loss: 0.006017879582941532\n",
      "Training epoch: 4, Batch_Num: 110/300, N_Loss: 0.010791569948196411\n",
      "Training epoch: 4, Batch_Num: 111/300, N_Loss: 0.0025643762201070786\n",
      "Training epoch: 4, Batch_Num: 112/300, N_Loss: 0.0017125814920291305\n",
      "Training epoch: 4, Batch_Num: 113/300, N_Loss: 0.003680093912407756\n",
      "Training epoch: 4, Batch_Num: 114/300, N_Loss: 0.00529500562697649\n",
      "Training epoch: 4, Batch_Num: 115/300, N_Loss: 0.01782676950097084\n",
      "Training epoch: 4, Batch_Num: 116/300, N_Loss: 0.016542671248316765\n",
      "Training epoch: 4, Batch_Num: 117/300, N_Loss: 0.0016824794001877308\n",
      "Training epoch: 4, Batch_Num: 118/300, N_Loss: 0.001801112201064825\n",
      "Training epoch: 4, Batch_Num: 119/300, N_Loss: 0.00797575619071722\n",
      "Training epoch: 4, Batch_Num: 120/300, N_Loss: 0.0028991601429879665\n",
      "Training epoch: 4, Batch_Num: 121/300, N_Loss: 0.004935629200190306\n",
      "Training epoch: 4, Batch_Num: 122/300, N_Loss: 0.009082005359232426\n",
      "Training epoch: 4, Batch_Num: 123/300, N_Loss: 0.008097315207123756\n",
      "Training epoch: 4, Batch_Num: 124/300, N_Loss: 0.011754040606319904\n",
      "Training epoch: 4, Batch_Num: 125/300, N_Loss: 0.005910270381718874\n",
      "Training epoch: 4, Batch_Num: 126/300, N_Loss: 0.015729540959000587\n",
      "Training epoch: 4, Batch_Num: 127/300, N_Loss: 0.0023045255802571774\n",
      "Training epoch: 4, Batch_Num: 128/300, N_Loss: 0.0022589515428990126\n",
      "Training epoch: 4, Batch_Num: 129/300, N_Loss: 0.0148366279900074\n",
      "Training epoch: 4, Batch_Num: 130/300, N_Loss: 0.009979584254324436\n",
      "Training epoch: 4, Batch_Num: 131/300, N_Loss: 0.003969034180045128\n",
      "Training epoch: 4, Batch_Num: 132/300, N_Loss: 0.010464824736118317\n",
      "Training epoch: 4, Batch_Num: 133/300, N_Loss: 0.004641882609575987\n",
      "Training epoch: 4, Batch_Num: 134/300, N_Loss: 0.007748813834041357\n",
      "Training epoch: 4, Batch_Num: 135/300, N_Loss: 0.0032625352032482624\n",
      "Training epoch: 4, Batch_Num: 136/300, N_Loss: 0.0020922778639942408\n",
      "Training epoch: 4, Batch_Num: 137/300, N_Loss: 0.005026135127991438\n",
      "Training epoch: 4, Batch_Num: 138/300, N_Loss: 0.011114365421235561\n",
      "Training epoch: 4, Batch_Num: 139/300, N_Loss: 0.0024823409039527178\n",
      "Training epoch: 4, Batch_Num: 140/300, N_Loss: 0.0052240584045648575\n",
      "Training epoch: 4, Batch_Num: 141/300, N_Loss: 0.005339120049029589\n",
      "Training epoch: 4, Batch_Num: 142/300, N_Loss: 0.0024917549453675747\n",
      "Training epoch: 4, Batch_Num: 143/300, N_Loss: 0.018656766042113304\n",
      "Training epoch: 4, Batch_Num: 144/300, N_Loss: 0.007233031094074249\n",
      "Training epoch: 4, Batch_Num: 145/300, N_Loss: 0.0056913308799266815\n",
      "Training epoch: 4, Batch_Num: 146/300, N_Loss: 0.015209350734949112\n",
      "Training epoch: 4, Batch_Num: 147/300, N_Loss: 0.00542147271335125\n",
      "Training epoch: 4, Batch_Num: 148/300, N_Loss: 0.003496282733976841\n",
      "Training epoch: 4, Batch_Num: 149/300, N_Loss: 0.0037363911978900433\n",
      "Training epoch: 4, Batch_Num: 150/300, N_Loss: 0.003792797215282917\n",
      "Training epoch: 4, Batch_Num: 151/300, N_Loss: 0.004814025945961475\n",
      "Training epoch: 4, Batch_Num: 152/300, N_Loss: 0.0010897958418354392\n",
      "Training epoch: 4, Batch_Num: 153/300, N_Loss: 0.008744914084672928\n",
      "Training epoch: 4, Batch_Num: 154/300, N_Loss: 0.003786552930250764\n",
      "Training epoch: 4, Batch_Num: 155/300, N_Loss: 0.003815548960119486\n",
      "Training epoch: 4, Batch_Num: 156/300, N_Loss: 0.005249686073511839\n",
      "Training epoch: 4, Batch_Num: 157/300, N_Loss: 0.018034351989626884\n",
      "Training epoch: 4, Batch_Num: 158/300, N_Loss: 0.006544366478919983\n",
      "Training epoch: 4, Batch_Num: 159/300, N_Loss: 0.014403659850358963\n",
      "Training epoch: 4, Batch_Num: 160/300, N_Loss: 0.019810369238257408\n",
      "Training epoch: 4, Batch_Num: 161/300, N_Loss: 0.03823520615696907\n",
      "Training epoch: 4, Batch_Num: 162/300, N_Loss: 0.006637674290686846\n",
      "Training epoch: 4, Batch_Num: 163/300, N_Loss: 0.02770845592021942\n",
      "Training epoch: 4, Batch_Num: 164/300, N_Loss: 0.05836416035890579\n",
      "Training epoch: 4, Batch_Num: 165/300, N_Loss: 0.017196791246533394\n",
      "Training epoch: 4, Batch_Num: 166/300, N_Loss: 0.022213777527213097\n",
      "Training epoch: 4, Batch_Num: 167/300, N_Loss: 0.016662031412124634\n",
      "Training epoch: 4, Batch_Num: 168/300, N_Loss: 0.04252525791525841\n",
      "Training epoch: 4, Batch_Num: 169/300, N_Loss: 0.018072813749313354\n",
      "Training epoch: 4, Batch_Num: 170/300, N_Loss: 0.0013881037011742592\n",
      "Training epoch: 4, Batch_Num: 171/300, N_Loss: 0.003228570567443967\n",
      "Training epoch: 4, Batch_Num: 172/300, N_Loss: 0.004211304243654013\n",
      "Training epoch: 4, Batch_Num: 173/300, N_Loss: 0.003964674659073353\n",
      "Training epoch: 4, Batch_Num: 174/300, N_Loss: 0.10275060683488846\n",
      "Training epoch: 4, Batch_Num: 175/300, N_Loss: 0.04235571622848511\n",
      "Training epoch: 4, Batch_Num: 176/300, N_Loss: 0.0053989398293197155\n",
      "Training epoch: 4, Batch_Num: 177/300, N_Loss: 0.0031281807459890842\n",
      "Training epoch: 4, Batch_Num: 178/300, N_Loss: 0.007186315022408962\n",
      "Training epoch: 4, Batch_Num: 179/300, N_Loss: 0.005094514694064856\n",
      "Training epoch: 4, Batch_Num: 180/300, N_Loss: 0.01105248648673296\n",
      "Training epoch: 4, Batch_Num: 181/300, N_Loss: 0.01325653214007616\n",
      "Training epoch: 4, Batch_Num: 182/300, N_Loss: 0.0022653003688901663\n",
      "Training epoch: 4, Batch_Num: 183/300, N_Loss: 0.015326075255870819\n",
      "Training epoch: 4, Batch_Num: 184/300, N_Loss: 0.010056788101792336\n",
      "Training epoch: 4, Batch_Num: 185/300, N_Loss: 0.007614704314619303\n",
      "Training epoch: 4, Batch_Num: 186/300, N_Loss: 0.022287605330348015\n",
      "Training epoch: 4, Batch_Num: 187/300, N_Loss: 0.00392447505146265\n",
      "Training epoch: 4, Batch_Num: 188/300, N_Loss: 0.010653340257704258\n",
      "Training epoch: 4, Batch_Num: 189/300, N_Loss: 0.000900345912668854\n",
      "Training epoch: 4, Batch_Num: 190/300, N_Loss: 0.00868456531316042\n",
      "Training epoch: 4, Batch_Num: 191/300, N_Loss: 0.008318346925079823\n",
      "Training epoch: 4, Batch_Num: 192/300, N_Loss: 0.0021187979727983475\n",
      "Training epoch: 4, Batch_Num: 193/300, N_Loss: 0.022132856771349907\n",
      "Training epoch: 4, Batch_Num: 194/300, N_Loss: 0.016049103811383247\n",
      "Training epoch: 4, Batch_Num: 195/300, N_Loss: 0.03703497722744942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4, Batch_Num: 196/300, N_Loss: 0.002577234525233507\n",
      "Training epoch: 4, Batch_Num: 197/300, N_Loss: 0.003413515631109476\n",
      "Training epoch: 4, Batch_Num: 198/300, N_Loss: 0.01836170069873333\n",
      "Training epoch: 4, Batch_Num: 199/300, N_Loss: 0.0015664397506043315\n",
      "Training epoch: 4, Batch_Num: 200/300, N_Loss: 0.002884463407099247\n",
      "Training epoch: 4, Batch_Num: 201/300, N_Loss: 0.017291279509663582\n",
      "Training epoch: 4, Batch_Num: 202/300, N_Loss: 0.003683336079120636\n",
      "Training epoch: 4, Batch_Num: 203/300, N_Loss: 0.0023829713463783264\n",
      "Training epoch: 4, Batch_Num: 204/300, N_Loss: 0.0024931191001087427\n",
      "Training epoch: 4, Batch_Num: 205/300, N_Loss: 0.0016383890761062503\n",
      "Training epoch: 4, Batch_Num: 206/300, N_Loss: 0.01206248253583908\n",
      "Training epoch: 4, Batch_Num: 207/300, N_Loss: 0.0027805480640381575\n",
      "Training epoch: 4, Batch_Num: 208/300, N_Loss: 0.014958308078348637\n",
      "Training epoch: 4, Batch_Num: 209/300, N_Loss: 0.007370427250862122\n",
      "Training epoch: 4, Batch_Num: 210/300, N_Loss: 0.012295505031943321\n",
      "Training epoch: 4, Batch_Num: 211/300, N_Loss: 0.014381554909050465\n",
      "Training epoch: 4, Batch_Num: 212/300, N_Loss: 0.026073165237903595\n",
      "Training epoch: 4, Batch_Num: 213/300, N_Loss: 0.005802951753139496\n",
      "Training epoch: 4, Batch_Num: 214/300, N_Loss: 0.002195097040385008\n",
      "Training epoch: 4, Batch_Num: 215/300, N_Loss: 0.0015729290898889303\n",
      "Training epoch: 4, Batch_Num: 216/300, N_Loss: 0.0822531133890152\n",
      "Training epoch: 4, Batch_Num: 217/300, N_Loss: 0.0008692381088621914\n",
      "Training epoch: 4, Batch_Num: 218/300, N_Loss: 0.01326227467507124\n",
      "Training epoch: 4, Batch_Num: 219/300, N_Loss: 0.0193114522844553\n",
      "Training epoch: 4, Batch_Num: 220/300, N_Loss: 0.0071801794692873955\n",
      "Training epoch: 4, Batch_Num: 221/300, N_Loss: 0.03601465001702309\n",
      "Training epoch: 4, Batch_Num: 222/300, N_Loss: 0.018259137868881226\n",
      "Training epoch: 4, Batch_Num: 223/300, N_Loss: 0.0032546294387429953\n",
      "Training epoch: 4, Batch_Num: 224/300, N_Loss: 0.0023838388733565807\n",
      "Training epoch: 4, Batch_Num: 225/300, N_Loss: 0.005271327216178179\n",
      "Training epoch: 4, Batch_Num: 226/300, N_Loss: 0.10625214129686356\n",
      "Training epoch: 4, Batch_Num: 227/300, N_Loss: 0.01151739526540041\n",
      "Training epoch: 4, Batch_Num: 228/300, N_Loss: 0.02600422501564026\n",
      "Training epoch: 4, Batch_Num: 229/300, N_Loss: 0.01699642650783062\n",
      "Training epoch: 4, Batch_Num: 230/300, N_Loss: 0.006574328523129225\n",
      "Training epoch: 4, Batch_Num: 231/300, N_Loss: 0.0014854409964755177\n",
      "Training epoch: 4, Batch_Num: 232/300, N_Loss: 0.004548379220068455\n",
      "Training epoch: 4, Batch_Num: 233/300, N_Loss: 0.013218998908996582\n",
      "Training epoch: 4, Batch_Num: 234/300, N_Loss: 0.0025100873317569494\n",
      "Training epoch: 4, Batch_Num: 235/300, N_Loss: 0.024510551244020462\n",
      "Training epoch: 4, Batch_Num: 236/300, N_Loss: 0.020155442878603935\n",
      "Training epoch: 4, Batch_Num: 237/300, N_Loss: 0.006580907851457596\n",
      "Training epoch: 4, Batch_Num: 238/300, N_Loss: 0.02070196159183979\n",
      "Training epoch: 4, Batch_Num: 239/300, N_Loss: 0.07498934864997864\n",
      "Training epoch: 4, Batch_Num: 240/300, N_Loss: 0.06778957694768906\n",
      "Training epoch: 4, Batch_Num: 241/300, N_Loss: 0.004407218657433987\n",
      "Training epoch: 4, Batch_Num: 242/300, N_Loss: 0.009161284193396568\n",
      "Training epoch: 4, Batch_Num: 243/300, N_Loss: 0.0032164249569177628\n",
      "Training epoch: 4, Batch_Num: 244/300, N_Loss: 0.016710853204131126\n",
      "Training epoch: 4, Batch_Num: 245/300, N_Loss: 0.0021264732349663973\n",
      "Training epoch: 4, Batch_Num: 246/300, N_Loss: 0.02415868453681469\n",
      "Training epoch: 4, Batch_Num: 247/300, N_Loss: 0.005756597965955734\n",
      "Training epoch: 4, Batch_Num: 248/300, N_Loss: 0.010342521592974663\n",
      "Training epoch: 4, Batch_Num: 249/300, N_Loss: 0.012026221491396427\n",
      "Training epoch: 4, Batch_Num: 250/300, N_Loss: 0.002944998210296035\n",
      "Training epoch: 4, Batch_Num: 251/300, N_Loss: 0.005222244653850794\n",
      "Training epoch: 4, Batch_Num: 252/300, N_Loss: 0.018029114231467247\n",
      "Training epoch: 4, Batch_Num: 253/300, N_Loss: 0.013775710947811604\n",
      "Training epoch: 4, Batch_Num: 254/300, N_Loss: 0.005156347062438726\n",
      "Training epoch: 4, Batch_Num: 255/300, N_Loss: 0.005115050822496414\n",
      "Training epoch: 4, Batch_Num: 256/300, N_Loss: 0.006285238545387983\n",
      "Training epoch: 4, Batch_Num: 257/300, N_Loss: 0.0017145501915365458\n",
      "Training epoch: 4, Batch_Num: 258/300, N_Loss: 0.0019519927445799112\n",
      "Training epoch: 4, Batch_Num: 259/300, N_Loss: 0.001130924210883677\n",
      "Training epoch: 4, Batch_Num: 260/300, N_Loss: 0.011582299135625362\n",
      "Training epoch: 4, Batch_Num: 261/300, N_Loss: 0.0024846713058650494\n",
      "Training epoch: 4, Batch_Num: 262/300, N_Loss: 0.004845169838517904\n",
      "Training epoch: 4, Batch_Num: 263/300, N_Loss: 0.004030242562294006\n",
      "Training epoch: 4, Batch_Num: 264/300, N_Loss: 0.00653199665248394\n",
      "Training epoch: 4, Batch_Num: 265/300, N_Loss: 0.01475739199668169\n",
      "Training epoch: 4, Batch_Num: 266/300, N_Loss: 0.007853765040636063\n",
      "Training epoch: 4, Batch_Num: 267/300, N_Loss: 0.002121023600921035\n",
      "Training epoch: 4, Batch_Num: 268/300, N_Loss: 0.014307979494333267\n",
      "Training epoch: 4, Batch_Num: 269/300, N_Loss: 0.003225750522688031\n",
      "Training epoch: 4, Batch_Num: 270/300, N_Loss: 0.004991129506379366\n",
      "Training epoch: 4, Batch_Num: 271/300, N_Loss: 0.003260204102844\n",
      "Training epoch: 4, Batch_Num: 272/300, N_Loss: 0.12118739634752274\n",
      "Training epoch: 4, Batch_Num: 273/300, N_Loss: 0.003702628891915083\n",
      "Training epoch: 4, Batch_Num: 274/300, N_Loss: 0.004892602097243071\n",
      "Training epoch: 4, Batch_Num: 275/300, N_Loss: 0.04612760245800018\n",
      "Training epoch: 4, Batch_Num: 276/300, N_Loss: 0.01290286798030138\n",
      "Training epoch: 4, Batch_Num: 277/300, N_Loss: 0.019791405647993088\n",
      "Training epoch: 4, Batch_Num: 278/300, N_Loss: 0.0029143420979380608\n",
      "Training epoch: 4, Batch_Num: 279/300, N_Loss: 0.0036320884246379137\n",
      "Training epoch: 4, Batch_Num: 280/300, N_Loss: 0.002231973223388195\n",
      "Training epoch: 4, Batch_Num: 281/300, N_Loss: 0.01391738373786211\n",
      "Training epoch: 4, Batch_Num: 282/300, N_Loss: 0.02213699370622635\n",
      "Training epoch: 4, Batch_Num: 283/300, N_Loss: 0.007947834208607674\n",
      "Training epoch: 4, Batch_Num: 284/300, N_Loss: 0.009331371635198593\n",
      "Training epoch: 4, Batch_Num: 285/300, N_Loss: 0.005483987275511026\n",
      "Training epoch: 4, Batch_Num: 286/300, N_Loss: 0.018441349267959595\n",
      "Training epoch: 4, Batch_Num: 287/300, N_Loss: 0.0013994536129757762\n",
      "Training epoch: 4, Batch_Num: 288/300, N_Loss: 0.012702965177595615\n",
      "Training epoch: 4, Batch_Num: 289/300, N_Loss: 0.06084843724966049\n",
      "Training epoch: 4, Batch_Num: 290/300, N_Loss: 0.005915174260735512\n",
      "Training epoch: 4, Batch_Num: 291/300, N_Loss: 0.007830170914530754\n",
      "Training epoch: 4, Batch_Num: 292/300, N_Loss: 0.039059851318597794\n",
      "Training epoch: 4, Batch_Num: 293/300, N_Loss: 0.005198129452764988\n",
      "Training epoch: 4, Batch_Num: 294/300, N_Loss: 0.02558077685534954\n",
      "Training epoch: 4, Batch_Num: 295/300, N_Loss: 0.001224295236170292\n",
      "Training epoch: 4, Batch_Num: 296/300, N_Loss: 0.0028204394038766623\n",
      "Training epoch: 4, Batch_Num: 297/300, N_Loss: 0.006653814110904932\n",
      "Training epoch: 4, Batch_Num: 298/300, N_Loss: 0.0024763995315879583\n",
      "Training epoch: 4, Batch_Num: 299/300, N_Loss: 0.030680207535624504\n",
      "Training epoch: 4, Batch_Num: 300/300, N_Loss: 0.0025238029193133116\n",
      "Training epoch: 5, Batch_Num: 1/300, N_Loss: 0.03770420327782631\n",
      "Training epoch: 5, Batch_Num: 2/300, N_Loss: 0.005405190400779247\n",
      "Training epoch: 5, Batch_Num: 3/300, N_Loss: 0.0273793525993824\n",
      "Training epoch: 5, Batch_Num: 4/300, N_Loss: 0.021772656589746475\n",
      "Training epoch: 5, Batch_Num: 5/300, N_Loss: 0.008142326027154922\n",
      "Training epoch: 5, Batch_Num: 6/300, N_Loss: 0.06759714335203171\n",
      "Training epoch: 5, Batch_Num: 7/300, N_Loss: 0.001891406369395554\n",
      "Training epoch: 5, Batch_Num: 8/300, N_Loss: 0.0072138309478759766\n",
      "Training epoch: 5, Batch_Num: 9/300, N_Loss: 0.01591416820883751\n",
      "Training epoch: 5, Batch_Num: 10/300, N_Loss: 0.0016631881007924676\n",
      "Training epoch: 5, Batch_Num: 11/300, N_Loss: 0.004772812593728304\n",
      "Training epoch: 5, Batch_Num: 12/300, N_Loss: 0.008479384705424309\n",
      "Training epoch: 5, Batch_Num: 13/300, N_Loss: 0.017695587128400803\n",
      "Training epoch: 5, Batch_Num: 14/300, N_Loss: 0.006246491335332394\n",
      "Training epoch: 5, Batch_Num: 15/300, N_Loss: 0.013794216327369213\n",
      "Training epoch: 5, Batch_Num: 16/300, N_Loss: 0.004476721398532391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5, Batch_Num: 17/300, N_Loss: 0.011466794647276402\n",
      "Training epoch: 5, Batch_Num: 18/300, N_Loss: 0.011091445572674274\n",
      "Training epoch: 5, Batch_Num: 19/300, N_Loss: 0.04588378593325615\n",
      "Training epoch: 5, Batch_Num: 20/300, N_Loss: 0.005557332653552294\n",
      "Training epoch: 5, Batch_Num: 21/300, N_Loss: 0.005333004519343376\n",
      "Training epoch: 5, Batch_Num: 22/300, N_Loss: 0.0014955556252971292\n",
      "Training epoch: 5, Batch_Num: 23/300, N_Loss: 0.00202182843349874\n",
      "Training epoch: 5, Batch_Num: 24/300, N_Loss: 0.016711337491869926\n",
      "Training epoch: 5, Batch_Num: 25/300, N_Loss: 0.0015002611326053739\n",
      "Training epoch: 5, Batch_Num: 26/300, N_Loss: 0.01164327934384346\n",
      "Training epoch: 5, Batch_Num: 27/300, N_Loss: 0.008199265226721764\n",
      "Training epoch: 5, Batch_Num: 28/300, N_Loss: 0.0017859921790659428\n",
      "Training epoch: 5, Batch_Num: 29/300, N_Loss: 0.023330114781856537\n",
      "Training epoch: 5, Batch_Num: 30/300, N_Loss: 0.030152365565299988\n",
      "Training epoch: 5, Batch_Num: 31/300, N_Loss: 0.002718245144933462\n",
      "Training epoch: 5, Batch_Num: 32/300, N_Loss: 0.06365735828876495\n",
      "Training epoch: 5, Batch_Num: 33/300, N_Loss: 0.005079073365777731\n",
      "Training epoch: 5, Batch_Num: 34/300, N_Loss: 0.012174323201179504\n",
      "Training epoch: 5, Batch_Num: 35/300, N_Loss: 0.00235564517788589\n",
      "Training epoch: 5, Batch_Num: 36/300, N_Loss: 0.013366861268877983\n",
      "Training epoch: 5, Batch_Num: 37/300, N_Loss: 0.023106295615434647\n",
      "Training epoch: 5, Batch_Num: 38/300, N_Loss: 0.0023146220482885838\n",
      "Training epoch: 5, Batch_Num: 39/300, N_Loss: 0.0024758640211075544\n",
      "Training epoch: 5, Batch_Num: 40/300, N_Loss: 0.014835089445114136\n",
      "Training epoch: 5, Batch_Num: 41/300, N_Loss: 0.005873846355825663\n",
      "Training epoch: 5, Batch_Num: 42/300, N_Loss: 0.04552023112773895\n",
      "Training epoch: 5, Batch_Num: 43/300, N_Loss: 0.002767169615253806\n",
      "Training epoch: 5, Batch_Num: 44/300, N_Loss: 0.00253930501639843\n",
      "Training epoch: 5, Batch_Num: 45/300, N_Loss: 0.08722848445177078\n",
      "Training epoch: 5, Batch_Num: 46/300, N_Loss: 0.00468241423368454\n",
      "Training epoch: 5, Batch_Num: 47/300, N_Loss: 0.006228146143257618\n",
      "Training epoch: 5, Batch_Num: 48/300, N_Loss: 0.006795333232730627\n",
      "Training epoch: 5, Batch_Num: 49/300, N_Loss: 0.012712492607533932\n",
      "Training epoch: 5, Batch_Num: 50/300, N_Loss: 0.0017402548110112548\n",
      "Training epoch: 5, Batch_Num: 51/300, N_Loss: 0.0023319858592003584\n",
      "Training epoch: 5, Batch_Num: 52/300, N_Loss: 0.005994139704853296\n",
      "Training epoch: 5, Batch_Num: 53/300, N_Loss: 0.008858565241098404\n",
      "Training epoch: 5, Batch_Num: 54/300, N_Loss: 0.004163173958659172\n",
      "Training epoch: 5, Batch_Num: 55/300, N_Loss: 0.05460798740386963\n",
      "Training epoch: 5, Batch_Num: 56/300, N_Loss: 0.0034021413885056973\n",
      "Training epoch: 5, Batch_Num: 57/300, N_Loss: 0.0005951600032858551\n",
      "Training epoch: 5, Batch_Num: 58/300, N_Loss: 0.005624689627438784\n",
      "Training epoch: 5, Batch_Num: 59/300, N_Loss: 0.006277826614677906\n",
      "Training epoch: 5, Batch_Num: 60/300, N_Loss: 0.1729494035243988\n",
      "Training epoch: 5, Batch_Num: 61/300, N_Loss: 0.0023591816425323486\n",
      "Training epoch: 5, Batch_Num: 62/300, N_Loss: 0.003968357108533382\n",
      "Training epoch: 5, Batch_Num: 63/300, N_Loss: 0.004907714668661356\n",
      "Training epoch: 5, Batch_Num: 64/300, N_Loss: 0.0032225127797573805\n",
      "Training epoch: 5, Batch_Num: 65/300, N_Loss: 0.017600299790501595\n",
      "Training epoch: 5, Batch_Num: 66/300, N_Loss: 0.025872964411973953\n",
      "Training epoch: 5, Batch_Num: 67/300, N_Loss: 0.004042277578264475\n",
      "Training epoch: 5, Batch_Num: 68/300, N_Loss: 0.00858327466994524\n",
      "Training epoch: 5, Batch_Num: 69/300, N_Loss: 0.0012802336132153869\n",
      "Training epoch: 5, Batch_Num: 70/300, N_Loss: 0.003743747016415\n",
      "Training epoch: 5, Batch_Num: 71/300, N_Loss: 0.005045733414590359\n",
      "Training epoch: 5, Batch_Num: 72/300, N_Loss: 0.08787401020526886\n",
      "Training epoch: 5, Batch_Num: 73/300, N_Loss: 0.0038861308712512255\n",
      "Training epoch: 5, Batch_Num: 74/300, N_Loss: 0.0031805613543838263\n",
      "Training epoch: 5, Batch_Num: 75/300, N_Loss: 0.0016318808775395155\n",
      "Training epoch: 5, Batch_Num: 76/300, N_Loss: 0.003773311385884881\n",
      "Training epoch: 5, Batch_Num: 77/300, N_Loss: 0.0016642581904307008\n",
      "Training epoch: 5, Batch_Num: 78/300, N_Loss: 0.0666588842868805\n",
      "Training epoch: 5, Batch_Num: 79/300, N_Loss: 0.010089372284710407\n",
      "Training epoch: 5, Batch_Num: 80/300, N_Loss: 0.0020434632897377014\n",
      "Training epoch: 5, Batch_Num: 81/300, N_Loss: 0.013389925472438335\n",
      "Training epoch: 5, Batch_Num: 82/300, N_Loss: 0.004070600960403681\n",
      "Training epoch: 5, Batch_Num: 83/300, N_Loss: 0.04721156507730484\n",
      "Training epoch: 5, Batch_Num: 84/300, N_Loss: 0.04015766456723213\n",
      "Training epoch: 5, Batch_Num: 85/300, N_Loss: 0.014030501246452332\n",
      "Training epoch: 5, Batch_Num: 86/300, N_Loss: 0.0057095251977443695\n",
      "Training epoch: 5, Batch_Num: 87/300, N_Loss: 0.0014439616352319717\n",
      "Training epoch: 5, Batch_Num: 88/300, N_Loss: 0.036405112594366074\n",
      "Training epoch: 5, Batch_Num: 89/300, N_Loss: 0.00551763316616416\n",
      "Training epoch: 5, Batch_Num: 90/300, N_Loss: 0.001828364678658545\n",
      "Training epoch: 5, Batch_Num: 91/300, N_Loss: 0.0021570853423327208\n",
      "Training epoch: 5, Batch_Num: 92/300, N_Loss: 0.002271302044391632\n",
      "Training epoch: 5, Batch_Num: 93/300, N_Loss: 0.00116301781963557\n",
      "Training epoch: 5, Batch_Num: 94/300, N_Loss: 0.02671218104660511\n",
      "Training epoch: 5, Batch_Num: 95/300, N_Loss: 0.00834665447473526\n",
      "Training epoch: 5, Batch_Num: 96/300, N_Loss: 0.06924319267272949\n",
      "Training epoch: 5, Batch_Num: 97/300, N_Loss: 0.005316248629242182\n",
      "Training epoch: 5, Batch_Num: 98/300, N_Loss: 0.0026955504436045885\n",
      "Training epoch: 5, Batch_Num: 99/300, N_Loss: 0.012044851668179035\n",
      "Training epoch: 5, Batch_Num: 100/300, N_Loss: 0.015329846180975437\n",
      "Training epoch: 5, Batch_Num: 101/300, N_Loss: 0.009668329730629921\n",
      "Training epoch: 5, Batch_Num: 102/300, N_Loss: 0.007549379486590624\n",
      "Training epoch: 5, Batch_Num: 103/300, N_Loss: 0.002156937727704644\n",
      "Training epoch: 5, Batch_Num: 104/300, N_Loss: 0.015311040915548801\n",
      "Training epoch: 5, Batch_Num: 105/300, N_Loss: 0.005458295810967684\n",
      "Training epoch: 5, Batch_Num: 106/300, N_Loss: 0.006975849159061909\n",
      "Training epoch: 5, Batch_Num: 107/300, N_Loss: 0.012192810885608196\n",
      "Training epoch: 5, Batch_Num: 108/300, N_Loss: 0.010338465683162212\n",
      "Training epoch: 5, Batch_Num: 109/300, N_Loss: 0.02953467331826687\n",
      "Training epoch: 5, Batch_Num: 110/300, N_Loss: 0.02235124446451664\n",
      "Training epoch: 5, Batch_Num: 111/300, N_Loss: 0.008204583078622818\n",
      "Training epoch: 5, Batch_Num: 112/300, N_Loss: 0.0013495883904397488\n",
      "Training epoch: 5, Batch_Num: 113/300, N_Loss: 0.0057477522641420364\n",
      "Training epoch: 5, Batch_Num: 114/300, N_Loss: 0.006243265699595213\n",
      "Training epoch: 5, Batch_Num: 115/300, N_Loss: 0.0014376428443938494\n",
      "Training epoch: 5, Batch_Num: 116/300, N_Loss: 0.0066886418499052525\n",
      "Training epoch: 5, Batch_Num: 117/300, N_Loss: 0.008988365530967712\n",
      "Training epoch: 5, Batch_Num: 118/300, N_Loss: 0.024667762219905853\n",
      "Training epoch: 5, Batch_Num: 119/300, N_Loss: 0.002543770708143711\n",
      "Training epoch: 5, Batch_Num: 120/300, N_Loss: 0.016033275052905083\n",
      "Training epoch: 5, Batch_Num: 121/300, N_Loss: 0.008083831518888474\n",
      "Training epoch: 5, Batch_Num: 122/300, N_Loss: 0.004243148025125265\n",
      "Training epoch: 5, Batch_Num: 123/300, N_Loss: 0.014232909306883812\n",
      "Training epoch: 5, Batch_Num: 124/300, N_Loss: 0.003525714622810483\n",
      "Training epoch: 5, Batch_Num: 125/300, N_Loss: 0.0017326157540082932\n",
      "Training epoch: 5, Batch_Num: 126/300, N_Loss: 0.025588760152459145\n",
      "Training epoch: 5, Batch_Num: 127/300, N_Loss: 0.013991523534059525\n",
      "Training epoch: 5, Batch_Num: 128/300, N_Loss: 0.0028835234697908163\n",
      "Training epoch: 5, Batch_Num: 129/300, N_Loss: 0.0077813430689275265\n",
      "Training epoch: 5, Batch_Num: 130/300, N_Loss: 0.001731259748339653\n",
      "Training epoch: 5, Batch_Num: 131/300, N_Loss: 0.014480606652796268\n",
      "Training epoch: 5, Batch_Num: 132/300, N_Loss: 0.01531145628541708\n",
      "Training epoch: 5, Batch_Num: 133/300, N_Loss: 0.013671054504811764\n",
      "Training epoch: 5, Batch_Num: 134/300, N_Loss: 0.003466035472229123\n",
      "Training epoch: 5, Batch_Num: 135/300, N_Loss: 0.017935777083039284\n",
      "Training epoch: 5, Batch_Num: 136/300, N_Loss: 0.015337388031184673\n",
      "Training epoch: 5, Batch_Num: 137/300, N_Loss: 0.003268822794780135\n",
      "Training epoch: 5, Batch_Num: 138/300, N_Loss: 0.0019819459412246943\n",
      "Training epoch: 5, Batch_Num: 139/300, N_Loss: 0.04657677188515663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5, Batch_Num: 140/300, N_Loss: 0.0029055126942694187\n",
      "Training epoch: 5, Batch_Num: 141/300, N_Loss: 0.0023738571908324957\n",
      "Training epoch: 5, Batch_Num: 142/300, N_Loss: 0.022134819999337196\n",
      "Training epoch: 5, Batch_Num: 143/300, N_Loss: 0.002564700786024332\n",
      "Training epoch: 5, Batch_Num: 144/300, N_Loss: 0.0062264446169137955\n",
      "Training epoch: 5, Batch_Num: 145/300, N_Loss: 0.0010776894632726908\n",
      "Training epoch: 5, Batch_Num: 146/300, N_Loss: 0.0028351072687655687\n",
      "Training epoch: 5, Batch_Num: 147/300, N_Loss: 0.01010697428137064\n",
      "Training epoch: 5, Batch_Num: 148/300, N_Loss: 0.02861337922513485\n",
      "Training epoch: 5, Batch_Num: 149/300, N_Loss: 0.004778650589287281\n",
      "Training epoch: 5, Batch_Num: 150/300, N_Loss: 0.005388662219047546\n",
      "Training epoch: 5, Batch_Num: 151/300, N_Loss: 0.003285807091742754\n",
      "Training epoch: 5, Batch_Num: 152/300, N_Loss: 0.046341393142938614\n",
      "Training epoch: 5, Batch_Num: 153/300, N_Loss: 0.0048638200387358665\n",
      "Training epoch: 5, Batch_Num: 154/300, N_Loss: 0.01833178475499153\n",
      "Training epoch: 5, Batch_Num: 155/300, N_Loss: 0.004007830284535885\n",
      "Training epoch: 5, Batch_Num: 156/300, N_Loss: 0.03297687694430351\n",
      "Training epoch: 5, Batch_Num: 157/300, N_Loss: 0.0012816092930734158\n",
      "Training epoch: 5, Batch_Num: 158/300, N_Loss: 0.00637444481253624\n",
      "Training epoch: 5, Batch_Num: 159/300, N_Loss: 0.009561223909258842\n",
      "Training epoch: 5, Batch_Num: 160/300, N_Loss: 0.005609033163636923\n",
      "Training epoch: 5, Batch_Num: 161/300, N_Loss: 0.008243028074502945\n",
      "Training epoch: 5, Batch_Num: 162/300, N_Loss: 0.002144664293155074\n",
      "Training epoch: 5, Batch_Num: 163/300, N_Loss: 0.010251017287373543\n",
      "Training epoch: 5, Batch_Num: 164/300, N_Loss: 0.13383890688419342\n",
      "Training epoch: 5, Batch_Num: 165/300, N_Loss: 0.011215013451874256\n",
      "Training epoch: 5, Batch_Num: 166/300, N_Loss: 0.023335568606853485\n",
      "Training epoch: 5, Batch_Num: 167/300, N_Loss: 0.04321295768022537\n",
      "Training epoch: 5, Batch_Num: 168/300, N_Loss: 0.0075446609407663345\n",
      "Training epoch: 5, Batch_Num: 169/300, N_Loss: 0.005449174903333187\n",
      "Training epoch: 5, Batch_Num: 170/300, N_Loss: 0.013112182728946209\n",
      "Training epoch: 5, Batch_Num: 171/300, N_Loss: 0.005473227705806494\n",
      "Training epoch: 5, Batch_Num: 172/300, N_Loss: 0.0022340393625199795\n",
      "Training epoch: 5, Batch_Num: 173/300, N_Loss: 0.0028945861849933863\n",
      "Training epoch: 5, Batch_Num: 174/300, N_Loss: 0.015036940574645996\n",
      "Training epoch: 5, Batch_Num: 175/300, N_Loss: 0.0022412962280213833\n",
      "Training epoch: 5, Batch_Num: 176/300, N_Loss: 0.007384136784821749\n",
      "Training epoch: 5, Batch_Num: 177/300, N_Loss: 0.007788698188960552\n",
      "Training epoch: 5, Batch_Num: 178/300, N_Loss: 0.009577477350831032\n",
      "Training epoch: 5, Batch_Num: 179/300, N_Loss: 0.01885019987821579\n",
      "Training epoch: 5, Batch_Num: 180/300, N_Loss: 0.00641290657222271\n",
      "Training epoch: 5, Batch_Num: 181/300, N_Loss: 0.002700185403227806\n",
      "Training epoch: 5, Batch_Num: 182/300, N_Loss: 0.01435155887156725\n",
      "Training epoch: 5, Batch_Num: 183/300, N_Loss: 0.008557579480111599\n",
      "Training epoch: 5, Batch_Num: 184/300, N_Loss: 0.004351344890892506\n",
      "Training epoch: 5, Batch_Num: 185/300, N_Loss: 0.002645498141646385\n",
      "Training epoch: 5, Batch_Num: 186/300, N_Loss: 0.009422493167221546\n",
      "Training epoch: 5, Batch_Num: 187/300, N_Loss: 0.009662983939051628\n",
      "Training epoch: 5, Batch_Num: 188/300, N_Loss: 0.013634484261274338\n",
      "Training epoch: 5, Batch_Num: 189/300, N_Loss: 0.001824457198381424\n",
      "Training epoch: 5, Batch_Num: 190/300, N_Loss: 0.0012243000091984868\n",
      "Training epoch: 5, Batch_Num: 191/300, N_Loss: 0.005356580950319767\n",
      "Training epoch: 5, Batch_Num: 192/300, N_Loss: 0.003878323594108224\n",
      "Training epoch: 5, Batch_Num: 193/300, N_Loss: 0.023949146270751953\n",
      "Training epoch: 5, Batch_Num: 194/300, N_Loss: 0.002218724926933646\n",
      "Training epoch: 5, Batch_Num: 195/300, N_Loss: 0.003984104376286268\n",
      "Training epoch: 5, Batch_Num: 196/300, N_Loss: 0.008083849214017391\n",
      "Training epoch: 5, Batch_Num: 197/300, N_Loss: 0.10968992114067078\n",
      "Training epoch: 5, Batch_Num: 198/300, N_Loss: 0.01033107377588749\n",
      "Training epoch: 5, Batch_Num: 199/300, N_Loss: 0.0006138444878160954\n",
      "Training epoch: 5, Batch_Num: 200/300, N_Loss: 0.09119649976491928\n",
      "Training epoch: 5, Batch_Num: 201/300, N_Loss: 0.0021982803009450436\n",
      "Training epoch: 5, Batch_Num: 202/300, N_Loss: 0.0101131871342659\n",
      "Training epoch: 5, Batch_Num: 203/300, N_Loss: 0.004214709158986807\n",
      "Training epoch: 5, Batch_Num: 204/300, N_Loss: 0.0015216934261843562\n",
      "Training epoch: 5, Batch_Num: 205/300, N_Loss: 0.1538570374250412\n",
      "Training epoch: 5, Batch_Num: 206/300, N_Loss: 0.017633982002735138\n",
      "Training epoch: 5, Batch_Num: 207/300, N_Loss: 0.014805479906499386\n",
      "Training epoch: 5, Batch_Num: 208/300, N_Loss: 0.043756548315286636\n",
      "Training epoch: 5, Batch_Num: 209/300, N_Loss: 0.0016897491877898574\n",
      "Training epoch: 5, Batch_Num: 210/300, N_Loss: 0.004295314196497202\n",
      "Training epoch: 5, Batch_Num: 211/300, N_Loss: 0.0027594943530857563\n",
      "Training epoch: 5, Batch_Num: 212/300, N_Loss: 0.014549906365573406\n",
      "Training epoch: 5, Batch_Num: 213/300, N_Loss: 0.00914604589343071\n",
      "Training epoch: 5, Batch_Num: 214/300, N_Loss: 0.012838422320783138\n",
      "Training epoch: 5, Batch_Num: 215/300, N_Loss: 0.0038126488216221333\n",
      "Training epoch: 5, Batch_Num: 216/300, N_Loss: 0.0020685140043497086\n",
      "Training epoch: 5, Batch_Num: 217/300, N_Loss: 0.0010630629258230329\n",
      "Training epoch: 5, Batch_Num: 218/300, N_Loss: 0.0044455030001699924\n",
      "Training epoch: 5, Batch_Num: 219/300, N_Loss: 0.0020366201642900705\n",
      "Training epoch: 5, Batch_Num: 220/300, N_Loss: 0.006796503439545631\n",
      "Training epoch: 5, Batch_Num: 221/300, N_Loss: 0.00580105185508728\n",
      "Training epoch: 5, Batch_Num: 222/300, N_Loss: 0.004696095362305641\n",
      "Training epoch: 5, Batch_Num: 223/300, N_Loss: 0.015595820732414722\n",
      "Training epoch: 5, Batch_Num: 224/300, N_Loss: 0.0007336240960285068\n",
      "Training epoch: 5, Batch_Num: 225/300, N_Loss: 0.0049539306201040745\n",
      "Training epoch: 5, Batch_Num: 226/300, N_Loss: 0.0017801898065954447\n",
      "Training epoch: 5, Batch_Num: 227/300, N_Loss: 0.01301334798336029\n",
      "Training epoch: 5, Batch_Num: 228/300, N_Loss: 0.02530144527554512\n",
      "Training epoch: 5, Batch_Num: 229/300, N_Loss: 0.0021816587541252375\n",
      "Training epoch: 5, Batch_Num: 230/300, N_Loss: 0.03880529850721359\n",
      "Training epoch: 5, Batch_Num: 231/300, N_Loss: 0.011399820446968079\n",
      "Training epoch: 5, Batch_Num: 232/300, N_Loss: 0.004729785490781069\n",
      "Training epoch: 5, Batch_Num: 233/300, N_Loss: 0.005631034728139639\n",
      "Training epoch: 5, Batch_Num: 234/300, N_Loss: 0.004435685463249683\n",
      "Training epoch: 5, Batch_Num: 235/300, N_Loss: 0.007804353255778551\n",
      "Training epoch: 5, Batch_Num: 236/300, N_Loss: 0.022768964990973473\n",
      "Training epoch: 5, Batch_Num: 237/300, N_Loss: 0.002867110539227724\n",
      "Training epoch: 5, Batch_Num: 238/300, N_Loss: 0.0020579027477651834\n",
      "Training epoch: 5, Batch_Num: 239/300, N_Loss: 0.000568358285818249\n",
      "Training epoch: 5, Batch_Num: 240/300, N_Loss: 0.003977561369538307\n",
      "Training epoch: 5, Batch_Num: 241/300, N_Loss: 0.001250885776244104\n",
      "Training epoch: 5, Batch_Num: 242/300, N_Loss: 0.004905737470835447\n",
      "Training epoch: 5, Batch_Num: 243/300, N_Loss: 0.0038814027793705463\n",
      "Training epoch: 5, Batch_Num: 244/300, N_Loss: 0.19060495495796204\n",
      "Training epoch: 5, Batch_Num: 245/300, N_Loss: 0.011211535893380642\n",
      "Training epoch: 5, Batch_Num: 246/300, N_Loss: 0.015162085182964802\n",
      "Training epoch: 5, Batch_Num: 247/300, N_Loss: 0.02563454583287239\n",
      "Training epoch: 5, Batch_Num: 248/300, N_Loss: 0.029024124145507812\n",
      "Training epoch: 5, Batch_Num: 249/300, N_Loss: 0.05718357488512993\n",
      "Training epoch: 5, Batch_Num: 250/300, N_Loss: 0.0015187528915703297\n",
      "Training epoch: 5, Batch_Num: 251/300, N_Loss: 0.01221831887960434\n",
      "Training epoch: 5, Batch_Num: 252/300, N_Loss: 0.004622357897460461\n",
      "Training epoch: 5, Batch_Num: 253/300, N_Loss: 0.0031994685996323824\n",
      "Training epoch: 5, Batch_Num: 254/300, N_Loss: 0.006344717927277088\n",
      "Training epoch: 5, Batch_Num: 255/300, N_Loss: 0.0022570525761693716\n",
      "Training epoch: 5, Batch_Num: 256/300, N_Loss: 0.007364313583821058\n",
      "Training epoch: 5, Batch_Num: 257/300, N_Loss: 0.05227281153202057\n",
      "Training epoch: 5, Batch_Num: 258/300, N_Loss: 0.008439483121037483\n",
      "Training epoch: 5, Batch_Num: 259/300, N_Loss: 0.0022660845424979925\n",
      "Training epoch: 5, Batch_Num: 260/300, N_Loss: 0.0025031042750924826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5, Batch_Num: 261/300, N_Loss: 0.001606645411811769\n",
      "Training epoch: 5, Batch_Num: 262/300, N_Loss: 0.020959200337529182\n",
      "Training epoch: 5, Batch_Num: 263/300, N_Loss: 0.07052092254161835\n",
      "Training epoch: 5, Batch_Num: 264/300, N_Loss: 0.016690241172909737\n",
      "Training epoch: 5, Batch_Num: 265/300, N_Loss: 0.011570817790925503\n",
      "Training epoch: 5, Batch_Num: 266/300, N_Loss: 0.007055606693029404\n",
      "Training epoch: 5, Batch_Num: 267/300, N_Loss: 0.006174772512167692\n",
      "Training epoch: 5, Batch_Num: 268/300, N_Loss: 0.0021500440780073404\n",
      "Training epoch: 5, Batch_Num: 269/300, N_Loss: 0.015110569074749947\n",
      "Training epoch: 5, Batch_Num: 270/300, N_Loss: 0.008229229599237442\n",
      "Training epoch: 5, Batch_Num: 271/300, N_Loss: 0.007542599458247423\n",
      "Training epoch: 5, Batch_Num: 272/300, N_Loss: 0.004256953485310078\n",
      "Training epoch: 5, Batch_Num: 273/300, N_Loss: 0.004228813573718071\n",
      "Training epoch: 5, Batch_Num: 274/300, N_Loss: 0.0009170295088551939\n",
      "Training epoch: 5, Batch_Num: 275/300, N_Loss: 0.020766086876392365\n",
      "Training epoch: 5, Batch_Num: 276/300, N_Loss: 0.00865140836685896\n",
      "Training epoch: 5, Batch_Num: 277/300, N_Loss: 0.005696203093975782\n",
      "Training epoch: 5, Batch_Num: 278/300, N_Loss: 0.006091580260545015\n",
      "Training epoch: 5, Batch_Num: 279/300, N_Loss: 0.0019948105327785015\n",
      "Training epoch: 5, Batch_Num: 280/300, N_Loss: 0.07543157786130905\n",
      "Training epoch: 5, Batch_Num: 281/300, N_Loss: 0.006095753982663155\n",
      "Training epoch: 5, Batch_Num: 282/300, N_Loss: 0.006691541522741318\n",
      "Training epoch: 5, Batch_Num: 283/300, N_Loss: 0.005248678848147392\n",
      "Training epoch: 5, Batch_Num: 284/300, N_Loss: 0.005832151975482702\n",
      "Training epoch: 5, Batch_Num: 285/300, N_Loss: 0.014088652096688747\n",
      "Training epoch: 5, Batch_Num: 286/300, N_Loss: 0.003741553286090493\n",
      "Training epoch: 5, Batch_Num: 287/300, N_Loss: 0.007544273976236582\n",
      "Training epoch: 5, Batch_Num: 288/300, N_Loss: 0.0075658168643713\n",
      "Training epoch: 5, Batch_Num: 289/300, N_Loss: 0.0049688126891851425\n",
      "Training epoch: 5, Batch_Num: 290/300, N_Loss: 0.01801229640841484\n",
      "Training epoch: 5, Batch_Num: 291/300, N_Loss: 0.015118494629859924\n",
      "Training epoch: 5, Batch_Num: 292/300, N_Loss: 0.01838085986673832\n",
      "Training epoch: 5, Batch_Num: 293/300, N_Loss: 0.025908049196004868\n",
      "Training epoch: 5, Batch_Num: 294/300, N_Loss: 0.013257082551717758\n",
      "Training epoch: 5, Batch_Num: 295/300, N_Loss: 0.003451524768024683\n",
      "Training epoch: 5, Batch_Num: 296/300, N_Loss: 0.0021128414664417505\n",
      "Training epoch: 5, Batch_Num: 297/300, N_Loss: 0.008395004086196423\n",
      "Training epoch: 5, Batch_Num: 298/300, N_Loss: 0.0041928659193217754\n",
      "Training epoch: 5, Batch_Num: 299/300, N_Loss: 0.008948912844061852\n",
      "Training epoch: 5, Batch_Num: 300/300, N_Loss: 0.010940146632492542\n",
      "EPOCH: 5, MAE_HR: 432.6, MSE_HR: 558.5\n",
      "BEST MAE_HR: 432.6, BEST MSE_HR: 558.5, BEST Epoch: 5.00\n",
      "Training epoch: 6, Batch_Num: 1/300, N_Loss: 0.0058016590774059296\n",
      "Training epoch: 6, Batch_Num: 2/300, N_Loss: 0.0024324809201061726\n",
      "Training epoch: 6, Batch_Num: 3/300, N_Loss: 0.004010859411209822\n",
      "Training epoch: 6, Batch_Num: 4/300, N_Loss: 0.0020912084728479385\n",
      "Training epoch: 6, Batch_Num: 5/300, N_Loss: 0.0068190270103514194\n",
      "Training epoch: 6, Batch_Num: 6/300, N_Loss: 0.00289999064989388\n",
      "Training epoch: 6, Batch_Num: 7/300, N_Loss: 0.025535624474287033\n",
      "Training epoch: 6, Batch_Num: 8/300, N_Loss: 0.003259161487221718\n",
      "Training epoch: 6, Batch_Num: 9/300, N_Loss: 0.013213490135967731\n",
      "Training epoch: 6, Batch_Num: 10/300, N_Loss: 0.0017014250624924898\n",
      "Training epoch: 6, Batch_Num: 11/300, N_Loss: 0.005390726029872894\n",
      "Training epoch: 6, Batch_Num: 12/300, N_Loss: 0.00669266888871789\n",
      "Training epoch: 6, Batch_Num: 13/300, N_Loss: 0.009923744015395641\n",
      "Training epoch: 6, Batch_Num: 14/300, N_Loss: 0.0026824166998267174\n",
      "Training epoch: 6, Batch_Num: 15/300, N_Loss: 0.015377952717244625\n",
      "Training epoch: 6, Batch_Num: 16/300, N_Loss: 0.006375028286129236\n",
      "Training epoch: 6, Batch_Num: 17/300, N_Loss: 0.01694919355213642\n",
      "Training epoch: 6, Batch_Num: 18/300, N_Loss: 0.02602006122469902\n",
      "Training epoch: 6, Batch_Num: 19/300, N_Loss: 0.00946904718875885\n",
      "Training epoch: 6, Batch_Num: 20/300, N_Loss: 0.0027793720364570618\n",
      "Training epoch: 6, Batch_Num: 21/300, N_Loss: 0.0019241281552240252\n",
      "Training epoch: 6, Batch_Num: 22/300, N_Loss: 0.0010927976109087467\n",
      "Training epoch: 6, Batch_Num: 23/300, N_Loss: 0.0008375259349122643\n",
      "Training epoch: 6, Batch_Num: 24/300, N_Loss: 0.0014006182318553329\n",
      "Training epoch: 6, Batch_Num: 25/300, N_Loss: 0.02274675853550434\n",
      "Training epoch: 6, Batch_Num: 26/300, N_Loss: 0.004258769564330578\n",
      "Training epoch: 6, Batch_Num: 27/300, N_Loss: 0.005257549695670605\n",
      "Training epoch: 6, Batch_Num: 28/300, N_Loss: 0.01735585555434227\n",
      "Training epoch: 6, Batch_Num: 29/300, N_Loss: 0.0005491604097187519\n",
      "Training epoch: 6, Batch_Num: 30/300, N_Loss: 0.0032894329633563757\n",
      "Training epoch: 6, Batch_Num: 31/300, N_Loss: 0.004665375221520662\n",
      "Training epoch: 6, Batch_Num: 32/300, N_Loss: 0.003627023659646511\n",
      "Training epoch: 6, Batch_Num: 33/300, N_Loss: 0.023085812106728554\n",
      "Training epoch: 6, Batch_Num: 34/300, N_Loss: 0.0025496615562587976\n",
      "Training epoch: 6, Batch_Num: 35/300, N_Loss: 0.06379283219575882\n",
      "Training epoch: 6, Batch_Num: 36/300, N_Loss: 0.01846504397690296\n",
      "Training epoch: 6, Batch_Num: 37/300, N_Loss: 0.003935207147151232\n",
      "Training epoch: 6, Batch_Num: 38/300, N_Loss: 0.011806495487689972\n",
      "Training epoch: 6, Batch_Num: 39/300, N_Loss: 0.0023401123471558094\n",
      "Training epoch: 6, Batch_Num: 40/300, N_Loss: 0.0022308393381536007\n",
      "Training epoch: 6, Batch_Num: 41/300, N_Loss: 0.002975545125082135\n",
      "Training epoch: 6, Batch_Num: 42/300, N_Loss: 0.0021915750112384558\n",
      "Training epoch: 6, Batch_Num: 43/300, N_Loss: 0.004385402426123619\n",
      "Training epoch: 6, Batch_Num: 44/300, N_Loss: 0.01010546088218689\n",
      "Training epoch: 6, Batch_Num: 45/300, N_Loss: 0.005995312239974737\n",
      "Training epoch: 6, Batch_Num: 46/300, N_Loss: 0.004535736981779337\n",
      "Training epoch: 6, Batch_Num: 47/300, N_Loss: 0.021455425769090652\n",
      "Training epoch: 6, Batch_Num: 48/300, N_Loss: 0.00953690242022276\n",
      "Training epoch: 6, Batch_Num: 49/300, N_Loss: 0.018125353381037712\n",
      "Training epoch: 6, Batch_Num: 50/300, N_Loss: 0.01577419601380825\n",
      "Training epoch: 6, Batch_Num: 51/300, N_Loss: 0.0075405193492770195\n",
      "Training epoch: 6, Batch_Num: 52/300, N_Loss: 0.006384906359016895\n",
      "Training epoch: 6, Batch_Num: 53/300, N_Loss: 0.018650809302926064\n",
      "Training epoch: 6, Batch_Num: 54/300, N_Loss: 0.009995345957577229\n",
      "Training epoch: 6, Batch_Num: 55/300, N_Loss: 0.0023377500474452972\n",
      "Training epoch: 6, Batch_Num: 56/300, N_Loss: 0.009058237075805664\n",
      "Training epoch: 6, Batch_Num: 57/300, N_Loss: 0.005981723312288523\n",
      "Training epoch: 6, Batch_Num: 58/300, N_Loss: 0.0136867118999362\n",
      "Training epoch: 6, Batch_Num: 59/300, N_Loss: 0.007114823907613754\n",
      "Training epoch: 6, Batch_Num: 60/300, N_Loss: 0.0037489263340830803\n",
      "Training epoch: 6, Batch_Num: 61/300, N_Loss: 0.006180325988680124\n",
      "Training epoch: 6, Batch_Num: 62/300, N_Loss: 0.0010330767836421728\n",
      "Training epoch: 6, Batch_Num: 63/300, N_Loss: 0.005546403117477894\n",
      "Training epoch: 6, Batch_Num: 64/300, N_Loss: 0.028031203895807266\n",
      "Training epoch: 6, Batch_Num: 65/300, N_Loss: 0.013652719557285309\n",
      "Training epoch: 6, Batch_Num: 66/300, N_Loss: 0.0036567754577845335\n",
      "Training epoch: 6, Batch_Num: 67/300, N_Loss: 0.009295545518398285\n",
      "Training epoch: 6, Batch_Num: 68/300, N_Loss: 0.00827690027654171\n",
      "Training epoch: 6, Batch_Num: 69/300, N_Loss: 0.039544180035591125\n",
      "Training epoch: 6, Batch_Num: 70/300, N_Loss: 0.003701883601024747\n",
      "Training epoch: 6, Batch_Num: 71/300, N_Loss: 0.006849644705653191\n",
      "Training epoch: 6, Batch_Num: 72/300, N_Loss: 0.0061968844383955\n",
      "Training epoch: 6, Batch_Num: 73/300, N_Loss: 0.0040598842315375805\n",
      "Training epoch: 6, Batch_Num: 74/300, N_Loss: 0.0471654087305069\n",
      "Training epoch: 6, Batch_Num: 75/300, N_Loss: 0.002570778364315629\n",
      "Training epoch: 6, Batch_Num: 76/300, N_Loss: 0.006163850426673889\n",
      "Training epoch: 6, Batch_Num: 77/300, N_Loss: 0.008861273527145386\n",
      "Training epoch: 6, Batch_Num: 78/300, N_Loss: 0.015100933611392975\n",
      "Training epoch: 6, Batch_Num: 79/300, N_Loss: 0.004502126481384039\n",
      "Training epoch: 6, Batch_Num: 80/300, N_Loss: 0.004964057821780443\n",
      "Training epoch: 6, Batch_Num: 81/300, N_Loss: 0.0016521117649972439\n",
      "Training epoch: 6, Batch_Num: 82/300, N_Loss: 0.0019780241418629885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 6, Batch_Num: 83/300, N_Loss: 0.007844510488212109\n",
      "Training epoch: 6, Batch_Num: 84/300, N_Loss: 0.005859944503754377\n",
      "Training epoch: 6, Batch_Num: 85/300, N_Loss: 0.0018872921355068684\n",
      "Training epoch: 6, Batch_Num: 86/300, N_Loss: 0.0021426123566925526\n",
      "Training epoch: 6, Batch_Num: 87/300, N_Loss: 0.006282815244048834\n",
      "Training epoch: 6, Batch_Num: 88/300, N_Loss: 0.0031881253235042095\n",
      "Training epoch: 6, Batch_Num: 89/300, N_Loss: 0.017047997564077377\n",
      "Training epoch: 6, Batch_Num: 90/300, N_Loss: 0.004910958930850029\n",
      "Training epoch: 6, Batch_Num: 91/300, N_Loss: 0.004221836570650339\n",
      "Training epoch: 6, Batch_Num: 92/300, N_Loss: 0.1142478808760643\n",
      "Training epoch: 6, Batch_Num: 93/300, N_Loss: 0.0013804694171994925\n",
      "Training epoch: 6, Batch_Num: 94/300, N_Loss: 0.01371655985713005\n",
      "Training epoch: 6, Batch_Num: 95/300, N_Loss: 0.06623996794223785\n",
      "Training epoch: 6, Batch_Num: 96/300, N_Loss: 0.05860887095332146\n",
      "Training epoch: 6, Batch_Num: 97/300, N_Loss: 0.010893024504184723\n",
      "Training epoch: 6, Batch_Num: 98/300, N_Loss: 0.005316977854818106\n",
      "Training epoch: 6, Batch_Num: 99/300, N_Loss: 0.024050017818808556\n",
      "Training epoch: 6, Batch_Num: 100/300, N_Loss: 0.013379293493926525\n",
      "Training epoch: 6, Batch_Num: 101/300, N_Loss: 0.004029399249702692\n",
      "Training epoch: 6, Batch_Num: 102/300, N_Loss: 0.0040130410343408585\n",
      "Training epoch: 6, Batch_Num: 103/300, N_Loss: 0.003331944113597274\n",
      "Training epoch: 6, Batch_Num: 104/300, N_Loss: 0.00982621405273676\n",
      "Training epoch: 6, Batch_Num: 105/300, N_Loss: 0.007882300764322281\n",
      "Training epoch: 6, Batch_Num: 106/300, N_Loss: 0.0046385726891458035\n",
      "Training epoch: 6, Batch_Num: 107/300, N_Loss: 0.007345695048570633\n",
      "Training epoch: 6, Batch_Num: 108/300, N_Loss: 0.012579601258039474\n",
      "Training epoch: 6, Batch_Num: 109/300, N_Loss: 0.02122725360095501\n",
      "Training epoch: 6, Batch_Num: 110/300, N_Loss: 0.0014961526030674577\n",
      "Training epoch: 6, Batch_Num: 111/300, N_Loss: 0.019911780953407288\n",
      "Training epoch: 6, Batch_Num: 112/300, N_Loss: 0.012488625012338161\n",
      "Training epoch: 6, Batch_Num: 113/300, N_Loss: 0.02243507094681263\n",
      "Training epoch: 6, Batch_Num: 114/300, N_Loss: 0.010682065039873123\n",
      "Training epoch: 6, Batch_Num: 115/300, N_Loss: 0.015944484621286392\n",
      "Training epoch: 6, Batch_Num: 116/300, N_Loss: 0.0010969798313453794\n",
      "Training epoch: 6, Batch_Num: 117/300, N_Loss: 0.007476566359400749\n",
      "Training epoch: 6, Batch_Num: 118/300, N_Loss: 0.00666737649589777\n",
      "Training epoch: 6, Batch_Num: 119/300, N_Loss: 0.007720681373029947\n",
      "Training epoch: 6, Batch_Num: 120/300, N_Loss: 0.002008649520576\n",
      "Training epoch: 6, Batch_Num: 121/300, N_Loss: 0.013972214423120022\n",
      "Training epoch: 6, Batch_Num: 122/300, N_Loss: 0.005477684549987316\n",
      "Training epoch: 6, Batch_Num: 123/300, N_Loss: 0.005801293067634106\n",
      "Training epoch: 6, Batch_Num: 124/300, N_Loss: 0.016252150759100914\n",
      "Training epoch: 6, Batch_Num: 125/300, N_Loss: 0.0585285983979702\n",
      "Training epoch: 6, Batch_Num: 126/300, N_Loss: 0.006078398320823908\n",
      "Training epoch: 6, Batch_Num: 127/300, N_Loss: 0.00126732571516186\n",
      "Training epoch: 6, Batch_Num: 128/300, N_Loss: 0.015089012682437897\n",
      "Training epoch: 6, Batch_Num: 129/300, N_Loss: 0.0009919324656948447\n",
      "Training epoch: 6, Batch_Num: 130/300, N_Loss: 0.002081501530483365\n",
      "Training epoch: 6, Batch_Num: 131/300, N_Loss: 0.008392811752855778\n",
      "Training epoch: 6, Batch_Num: 132/300, N_Loss: 0.0033774732146412134\n",
      "Training epoch: 6, Batch_Num: 133/300, N_Loss: 0.014303814619779587\n",
      "Training epoch: 6, Batch_Num: 134/300, N_Loss: 0.08381903916597366\n",
      "Training epoch: 6, Batch_Num: 135/300, N_Loss: 0.006680035497993231\n",
      "Training epoch: 6, Batch_Num: 136/300, N_Loss: 0.0031898622401058674\n",
      "Training epoch: 6, Batch_Num: 137/300, N_Loss: 0.02161012776196003\n",
      "Training epoch: 6, Batch_Num: 138/300, N_Loss: 0.0015074900584295392\n",
      "Training epoch: 6, Batch_Num: 139/300, N_Loss: 0.0076146675273776054\n",
      "Training epoch: 6, Batch_Num: 140/300, N_Loss: 0.003849618136882782\n",
      "Training epoch: 6, Batch_Num: 141/300, N_Loss: 0.004043502267450094\n",
      "Training epoch: 6, Batch_Num: 142/300, N_Loss: 0.0067460504360497\n",
      "Training epoch: 6, Batch_Num: 143/300, N_Loss: 0.005232417024672031\n",
      "Training epoch: 6, Batch_Num: 144/300, N_Loss: 0.0022674654610455036\n",
      "Training epoch: 6, Batch_Num: 145/300, N_Loss: 0.009480752982199192\n",
      "Training epoch: 6, Batch_Num: 146/300, N_Loss: 0.01357665192335844\n",
      "Training epoch: 6, Batch_Num: 147/300, N_Loss: 0.00411539152264595\n",
      "Training epoch: 6, Batch_Num: 148/300, N_Loss: 0.033695951104164124\n",
      "Training epoch: 6, Batch_Num: 149/300, N_Loss: 0.014319019392132759\n",
      "Training epoch: 6, Batch_Num: 150/300, N_Loss: 0.004073003772646189\n",
      "Training epoch: 6, Batch_Num: 151/300, N_Loss: 0.07208221405744553\n",
      "Training epoch: 6, Batch_Num: 152/300, N_Loss: 0.013195068575441837\n",
      "Training epoch: 6, Batch_Num: 153/300, N_Loss: 0.0045480309054255486\n",
      "Training epoch: 6, Batch_Num: 154/300, N_Loss: 0.011072574183344841\n",
      "Training epoch: 6, Batch_Num: 155/300, N_Loss: 0.025172078981995583\n",
      "Training epoch: 6, Batch_Num: 156/300, N_Loss: 0.0033125102054327726\n",
      "Training epoch: 6, Batch_Num: 157/300, N_Loss: 0.0008855338674038649\n",
      "Training epoch: 6, Batch_Num: 158/300, N_Loss: 0.011970476247370243\n",
      "Training epoch: 6, Batch_Num: 159/300, N_Loss: 0.005132938735187054\n",
      "Training epoch: 6, Batch_Num: 160/300, N_Loss: 0.012066173367202282\n",
      "Training epoch: 6, Batch_Num: 161/300, N_Loss: 0.06757082045078278\n",
      "Training epoch: 6, Batch_Num: 162/300, N_Loss: 0.11825688183307648\n",
      "Training epoch: 6, Batch_Num: 163/300, N_Loss: 0.004185771569609642\n",
      "Training epoch: 6, Batch_Num: 164/300, N_Loss: 0.0010032320860773325\n",
      "Training epoch: 6, Batch_Num: 165/300, N_Loss: 0.0039357990026474\n",
      "Training epoch: 6, Batch_Num: 166/300, N_Loss: 0.00922434777021408\n",
      "Training epoch: 6, Batch_Num: 167/300, N_Loss: 0.0019118690397590399\n",
      "Training epoch: 6, Batch_Num: 168/300, N_Loss: 0.0022601240780204535\n",
      "Training epoch: 6, Batch_Num: 169/300, N_Loss: 0.008581451140344143\n",
      "Training epoch: 6, Batch_Num: 170/300, N_Loss: 0.0053655654191970825\n",
      "Training epoch: 6, Batch_Num: 171/300, N_Loss: 0.009112058207392693\n",
      "Training epoch: 6, Batch_Num: 172/300, N_Loss: 0.003348486265167594\n",
      "Training epoch: 6, Batch_Num: 173/300, N_Loss: 0.0006138275493867695\n",
      "Training epoch: 6, Batch_Num: 174/300, N_Loss: 0.004034546669572592\n",
      "Training epoch: 6, Batch_Num: 175/300, N_Loss: 0.0041551911272108555\n",
      "Training epoch: 6, Batch_Num: 176/300, N_Loss: 0.002406714716926217\n",
      "Training epoch: 6, Batch_Num: 177/300, N_Loss: 0.05149476230144501\n",
      "Training epoch: 6, Batch_Num: 178/300, N_Loss: 0.002876576501876116\n",
      "Training epoch: 6, Batch_Num: 179/300, N_Loss: 0.005864252801984549\n",
      "Training epoch: 6, Batch_Num: 180/300, N_Loss: 0.005666010081768036\n",
      "Training epoch: 6, Batch_Num: 181/300, N_Loss: 0.007039022631943226\n",
      "Training epoch: 6, Batch_Num: 182/300, N_Loss: 0.03634002059698105\n",
      "Training epoch: 6, Batch_Num: 183/300, N_Loss: 0.009605164639651775\n",
      "Training epoch: 6, Batch_Num: 184/300, N_Loss: 0.0017723413184285164\n",
      "Training epoch: 6, Batch_Num: 185/300, N_Loss: 0.011710146442055702\n",
      "Training epoch: 6, Batch_Num: 186/300, N_Loss: 0.005664432421326637\n",
      "Training epoch: 6, Batch_Num: 187/300, N_Loss: 0.01985030807554722\n",
      "Training epoch: 6, Batch_Num: 188/300, N_Loss: 0.004555055405944586\n",
      "Training epoch: 6, Batch_Num: 189/300, N_Loss: 0.0024706677068024874\n",
      "Training epoch: 6, Batch_Num: 190/300, N_Loss: 0.0024206191301345825\n",
      "Training epoch: 6, Batch_Num: 191/300, N_Loss: 0.007724740542471409\n",
      "Training epoch: 6, Batch_Num: 192/300, N_Loss: 0.005117012187838554\n",
      "Training epoch: 6, Batch_Num: 193/300, N_Loss: 0.026689928025007248\n",
      "Training epoch: 6, Batch_Num: 194/300, N_Loss: 0.007497926242649555\n",
      "Training epoch: 6, Batch_Num: 195/300, N_Loss: 0.021059537306427956\n",
      "Training epoch: 6, Batch_Num: 196/300, N_Loss: 0.010731926187872887\n",
      "Training epoch: 6, Batch_Num: 197/300, N_Loss: 0.001491590403020382\n",
      "Training epoch: 6, Batch_Num: 198/300, N_Loss: 0.0014671152457594872\n",
      "Training epoch: 6, Batch_Num: 199/300, N_Loss: 0.1126585602760315\n",
      "Training epoch: 6, Batch_Num: 200/300, N_Loss: 0.009905960410833359\n",
      "Training epoch: 6, Batch_Num: 201/300, N_Loss: 0.01988983526825905\n",
      "Training epoch: 6, Batch_Num: 202/300, N_Loss: 0.10589420795440674\n",
      "Training epoch: 6, Batch_Num: 203/300, N_Loss: 0.009759373031556606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 6, Batch_Num: 204/300, N_Loss: 0.004771013278514147\n",
      "Training epoch: 6, Batch_Num: 205/300, N_Loss: 0.018669620156288147\n",
      "Training epoch: 6, Batch_Num: 206/300, N_Loss: 0.010941402055323124\n",
      "Training epoch: 6, Batch_Num: 207/300, N_Loss: 0.004795384593307972\n",
      "Training epoch: 6, Batch_Num: 208/300, N_Loss: 0.011867009103298187\n",
      "Training epoch: 6, Batch_Num: 209/300, N_Loss: 0.006626609247177839\n",
      "Training epoch: 6, Batch_Num: 210/300, N_Loss: 0.001072441809810698\n",
      "Training epoch: 6, Batch_Num: 211/300, N_Loss: 0.012745463289320469\n",
      "Training epoch: 6, Batch_Num: 212/300, N_Loss: 0.025918645784258842\n",
      "Training epoch: 6, Batch_Num: 213/300, N_Loss: 0.038755591958761215\n",
      "Training epoch: 6, Batch_Num: 214/300, N_Loss: 0.0009135374566540122\n",
      "Training epoch: 6, Batch_Num: 215/300, N_Loss: 0.002377093303948641\n",
      "Training epoch: 6, Batch_Num: 216/300, N_Loss: 0.002403226913884282\n",
      "Training epoch: 6, Batch_Num: 217/300, N_Loss: 0.006316879764199257\n",
      "Training epoch: 6, Batch_Num: 218/300, N_Loss: 0.013693719170987606\n",
      "Training epoch: 6, Batch_Num: 219/300, N_Loss: 0.0018131649121642113\n",
      "Training epoch: 6, Batch_Num: 220/300, N_Loss: 0.007145067676901817\n",
      "Training epoch: 6, Batch_Num: 221/300, N_Loss: 0.005463222041726112\n",
      "Training epoch: 6, Batch_Num: 222/300, N_Loss: 0.006368772592395544\n",
      "Training epoch: 6, Batch_Num: 223/300, N_Loss: 0.019534630700945854\n",
      "Training epoch: 6, Batch_Num: 224/300, N_Loss: 0.0015391146298497915\n",
      "Training epoch: 6, Batch_Num: 225/300, N_Loss: 0.0386580228805542\n",
      "Training epoch: 6, Batch_Num: 226/300, N_Loss: 0.017836542800068855\n",
      "Training epoch: 6, Batch_Num: 227/300, N_Loss: 0.03279637545347214\n",
      "Training epoch: 6, Batch_Num: 228/300, N_Loss: 0.013757516629993916\n",
      "Training epoch: 6, Batch_Num: 229/300, N_Loss: 0.0014262812910601497\n",
      "Training epoch: 6, Batch_Num: 230/300, N_Loss: 0.021145252510905266\n",
      "Training epoch: 6, Batch_Num: 231/300, N_Loss: 0.10740366578102112\n",
      "Training epoch: 6, Batch_Num: 232/300, N_Loss: 0.0028310525231063366\n",
      "Training epoch: 6, Batch_Num: 233/300, N_Loss: 0.03826886788010597\n",
      "Training epoch: 6, Batch_Num: 234/300, N_Loss: 0.003280970733612776\n",
      "Training epoch: 6, Batch_Num: 235/300, N_Loss: 0.04205172508955002\n",
      "Training epoch: 6, Batch_Num: 236/300, N_Loss: 0.008487087674438953\n",
      "Training epoch: 6, Batch_Num: 237/300, N_Loss: 0.010772202163934708\n",
      "Training epoch: 6, Batch_Num: 238/300, N_Loss: 0.014868772588670254\n",
      "Training epoch: 6, Batch_Num: 239/300, N_Loss: 0.002047640969976783\n",
      "Training epoch: 6, Batch_Num: 240/300, N_Loss: 0.013387606479227543\n",
      "Training epoch: 6, Batch_Num: 241/300, N_Loss: 0.009022969752550125\n",
      "Training epoch: 6, Batch_Num: 242/300, N_Loss: 0.025557398796081543\n",
      "Training epoch: 6, Batch_Num: 243/300, N_Loss: 0.015172269195318222\n",
      "Training epoch: 6, Batch_Num: 244/300, N_Loss: 0.030590493232011795\n",
      "Training epoch: 6, Batch_Num: 245/300, N_Loss: 0.006922188214957714\n",
      "Training epoch: 6, Batch_Num: 246/300, N_Loss: 0.004128060769289732\n",
      "Training epoch: 6, Batch_Num: 247/300, N_Loss: 0.003623319324105978\n",
      "Training epoch: 6, Batch_Num: 248/300, N_Loss: 0.0254116989672184\n",
      "Training epoch: 6, Batch_Num: 249/300, N_Loss: 0.003161313943564892\n",
      "Training epoch: 6, Batch_Num: 250/300, N_Loss: 0.07134345918893814\n",
      "Training epoch: 6, Batch_Num: 251/300, N_Loss: 0.040753696113824844\n",
      "Training epoch: 6, Batch_Num: 252/300, N_Loss: 0.007350335828959942\n",
      "Training epoch: 6, Batch_Num: 253/300, N_Loss: 0.009687892161309719\n",
      "Training epoch: 6, Batch_Num: 254/300, N_Loss: 0.006529041565954685\n",
      "Training epoch: 6, Batch_Num: 255/300, N_Loss: 0.017405252903699875\n",
      "Training epoch: 6, Batch_Num: 256/300, N_Loss: 0.0030304917600005865\n",
      "Training epoch: 6, Batch_Num: 257/300, N_Loss: 0.0025322623550891876\n",
      "Training epoch: 6, Batch_Num: 258/300, N_Loss: 0.004302256274968386\n",
      "Training epoch: 6, Batch_Num: 259/300, N_Loss: 0.005654194392263889\n",
      "Training epoch: 6, Batch_Num: 260/300, N_Loss: 0.002448383020237088\n",
      "Training epoch: 6, Batch_Num: 261/300, N_Loss: 0.005457635503262281\n",
      "Training epoch: 6, Batch_Num: 262/300, N_Loss: 0.01745275780558586\n",
      "Training epoch: 6, Batch_Num: 263/300, N_Loss: 0.0018282672390341759\n",
      "Training epoch: 6, Batch_Num: 264/300, N_Loss: 0.001331261359155178\n",
      "Training epoch: 6, Batch_Num: 265/300, N_Loss: 0.0016530657885596156\n",
      "Training epoch: 6, Batch_Num: 266/300, N_Loss: 0.003046223893761635\n",
      "Training epoch: 6, Batch_Num: 267/300, N_Loss: 0.04733141139149666\n",
      "Training epoch: 6, Batch_Num: 268/300, N_Loss: 0.004792343359440565\n",
      "Training epoch: 6, Batch_Num: 269/300, N_Loss: 0.006306673865765333\n",
      "Training epoch: 6, Batch_Num: 270/300, N_Loss: 0.01692349649965763\n",
      "Training epoch: 6, Batch_Num: 271/300, N_Loss: 0.014236303977668285\n",
      "Training epoch: 6, Batch_Num: 272/300, N_Loss: 0.0039392090402543545\n",
      "Training epoch: 6, Batch_Num: 273/300, N_Loss: 0.0018766727298498154\n",
      "Training epoch: 6, Batch_Num: 274/300, N_Loss: 0.015407989732921124\n",
      "Training epoch: 6, Batch_Num: 275/300, N_Loss: 0.014211373403668404\n",
      "Training epoch: 6, Batch_Num: 276/300, N_Loss: 0.006281753070652485\n",
      "Training epoch: 6, Batch_Num: 277/300, N_Loss: 0.007433748804032803\n",
      "Training epoch: 6, Batch_Num: 278/300, N_Loss: 0.026063743978738785\n",
      "Training epoch: 6, Batch_Num: 279/300, N_Loss: 0.006540172733366489\n",
      "Training epoch: 6, Batch_Num: 280/300, N_Loss: 0.0013395504793152213\n",
      "Training epoch: 6, Batch_Num: 281/300, N_Loss: 0.00964400451630354\n",
      "Training epoch: 6, Batch_Num: 282/300, N_Loss: 0.030283048748970032\n",
      "Training epoch: 6, Batch_Num: 283/300, N_Loss: 0.0021972295362502337\n",
      "Training epoch: 6, Batch_Num: 284/300, N_Loss: 0.02255481667816639\n",
      "Training epoch: 6, Batch_Num: 285/300, N_Loss: 0.0023856789339333773\n",
      "Training epoch: 6, Batch_Num: 286/300, N_Loss: 0.0058098454028368\n",
      "Training epoch: 6, Batch_Num: 287/300, N_Loss: 0.007913949899375439\n",
      "Training epoch: 6, Batch_Num: 288/300, N_Loss: 0.002906851237639785\n",
      "Training epoch: 6, Batch_Num: 289/300, N_Loss: 0.0028711208142340183\n",
      "Training epoch: 6, Batch_Num: 290/300, N_Loss: 0.0711669847369194\n",
      "Training epoch: 6, Batch_Num: 291/300, N_Loss: 0.013073023408651352\n",
      "Training epoch: 6, Batch_Num: 292/300, N_Loss: 0.01470048725605011\n",
      "Training epoch: 6, Batch_Num: 293/300, N_Loss: 0.0006619609193876386\n",
      "Training epoch: 6, Batch_Num: 294/300, N_Loss: 0.002840631175786257\n",
      "Training epoch: 6, Batch_Num: 295/300, N_Loss: 0.00980366114526987\n",
      "Training epoch: 6, Batch_Num: 296/300, N_Loss: 0.00930059514939785\n",
      "Training epoch: 6, Batch_Num: 297/300, N_Loss: 0.0016548497369512916\n",
      "Training epoch: 6, Batch_Num: 298/300, N_Loss: 0.01025085523724556\n",
      "Training epoch: 6, Batch_Num: 299/300, N_Loss: 0.034707777202129364\n",
      "Training epoch: 6, Batch_Num: 300/300, N_Loss: 0.0008164241444319487\n",
      "Training epoch: 7, Batch_Num: 1/300, N_Loss: 0.0017900108359754086\n",
      "Training epoch: 7, Batch_Num: 2/300, N_Loss: 0.019339781254529953\n",
      "Training epoch: 7, Batch_Num: 3/300, N_Loss: 0.004934530705213547\n",
      "Training epoch: 7, Batch_Num: 4/300, N_Loss: 0.0013914760202169418\n",
      "Training epoch: 7, Batch_Num: 5/300, N_Loss: 0.0026137400418519974\n",
      "Training epoch: 7, Batch_Num: 6/300, N_Loss: 0.006638023536652327\n",
      "Training epoch: 7, Batch_Num: 7/300, N_Loss: 0.0055338493548333645\n",
      "Training epoch: 7, Batch_Num: 8/300, N_Loss: 0.014385795220732689\n",
      "Training epoch: 7, Batch_Num: 9/300, N_Loss: 0.0015563186025246978\n",
      "Training epoch: 7, Batch_Num: 10/300, N_Loss: 0.005791158881038427\n",
      "Training epoch: 7, Batch_Num: 11/300, N_Loss: 0.006312462966889143\n",
      "Training epoch: 7, Batch_Num: 12/300, N_Loss: 0.002869685646146536\n",
      "Training epoch: 7, Batch_Num: 13/300, N_Loss: 0.003451420459896326\n",
      "Training epoch: 7, Batch_Num: 14/300, N_Loss: 0.0008188214269466698\n",
      "Training epoch: 7, Batch_Num: 15/300, N_Loss: 0.0101637477055192\n",
      "Training epoch: 7, Batch_Num: 16/300, N_Loss: 0.003535268595442176\n",
      "Training epoch: 7, Batch_Num: 17/300, N_Loss: 0.017129115760326385\n",
      "Training epoch: 7, Batch_Num: 18/300, N_Loss: 0.001736024976707995\n",
      "Training epoch: 7, Batch_Num: 19/300, N_Loss: 0.015853645280003548\n",
      "Training epoch: 7, Batch_Num: 20/300, N_Loss: 0.0018355370266363025\n",
      "Training epoch: 7, Batch_Num: 21/300, N_Loss: 0.005448914133012295\n",
      "Training epoch: 7, Batch_Num: 22/300, N_Loss: 0.01285984180867672\n",
      "Training epoch: 7, Batch_Num: 23/300, N_Loss: 0.09528208523988724\n",
      "Training epoch: 7, Batch_Num: 24/300, N_Loss: 0.0026690918020904064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 7, Batch_Num: 25/300, N_Loss: 0.018058616667985916\n",
      "Training epoch: 7, Batch_Num: 26/300, N_Loss: 0.01989918388426304\n",
      "Training epoch: 7, Batch_Num: 27/300, N_Loss: 0.005026503931730986\n",
      "Training epoch: 7, Batch_Num: 28/300, N_Loss: 0.0020931316539645195\n",
      "Training epoch: 7, Batch_Num: 29/300, N_Loss: 0.006408429238945246\n",
      "Training epoch: 7, Batch_Num: 30/300, N_Loss: 0.0244219359010458\n",
      "Training epoch: 7, Batch_Num: 31/300, N_Loss: 0.0006106457440182567\n",
      "Training epoch: 7, Batch_Num: 32/300, N_Loss: 0.0009021831792779267\n",
      "Training epoch: 7, Batch_Num: 33/300, N_Loss: 0.02857992984354496\n",
      "Training epoch: 7, Batch_Num: 34/300, N_Loss: 0.09597281366586685\n",
      "Training epoch: 7, Batch_Num: 35/300, N_Loss: 0.03348938003182411\n",
      "Training epoch: 7, Batch_Num: 36/300, N_Loss: 0.014241962693631649\n",
      "Training epoch: 7, Batch_Num: 37/300, N_Loss: 0.009446286596357822\n",
      "Training epoch: 7, Batch_Num: 38/300, N_Loss: 0.0020053989719599485\n",
      "Training epoch: 7, Batch_Num: 39/300, N_Loss: 0.04741857573390007\n",
      "Training epoch: 7, Batch_Num: 40/300, N_Loss: 0.03279663249850273\n",
      "Training epoch: 7, Batch_Num: 41/300, N_Loss: 0.002325582318007946\n",
      "Training epoch: 7, Batch_Num: 42/300, N_Loss: 0.014867023564875126\n",
      "Training epoch: 7, Batch_Num: 43/300, N_Loss: 0.005644025281071663\n",
      "Training epoch: 7, Batch_Num: 44/300, N_Loss: 0.003883396741002798\n",
      "Training epoch: 7, Batch_Num: 45/300, N_Loss: 0.015059120021760464\n",
      "Training epoch: 7, Batch_Num: 46/300, N_Loss: 0.016335921362042427\n",
      "Training epoch: 7, Batch_Num: 47/300, N_Loss: 0.0010987381683662534\n",
      "Training epoch: 7, Batch_Num: 48/300, N_Loss: 0.012518303468823433\n",
      "Training epoch: 7, Batch_Num: 49/300, N_Loss: 0.0022744874004274607\n",
      "Training epoch: 7, Batch_Num: 50/300, N_Loss: 0.01352396234869957\n",
      "Training epoch: 7, Batch_Num: 51/300, N_Loss: 0.0031465122010558844\n",
      "Training epoch: 7, Batch_Num: 52/300, N_Loss: 0.014324964955449104\n",
      "Training epoch: 7, Batch_Num: 53/300, N_Loss: 0.0024389957543462515\n",
      "Training epoch: 7, Batch_Num: 54/300, N_Loss: 0.002549765631556511\n",
      "Training epoch: 7, Batch_Num: 55/300, N_Loss: 0.0046640848740935326\n",
      "Training epoch: 7, Batch_Num: 56/300, N_Loss: 0.0012097636936232448\n",
      "Training epoch: 7, Batch_Num: 57/300, N_Loss: 0.003275022143498063\n",
      "Training epoch: 7, Batch_Num: 58/300, N_Loss: 0.005699543748050928\n",
      "Training epoch: 7, Batch_Num: 59/300, N_Loss: 0.008438518270850182\n",
      "Training epoch: 7, Batch_Num: 60/300, N_Loss: 0.010668044909834862\n",
      "Training epoch: 7, Batch_Num: 61/300, N_Loss: 0.005296993535012007\n",
      "Training epoch: 7, Batch_Num: 62/300, N_Loss: 0.003200263250619173\n",
      "Training epoch: 7, Batch_Num: 63/300, N_Loss: 0.010480789467692375\n",
      "Training epoch: 7, Batch_Num: 64/300, N_Loss: 0.035966064780950546\n",
      "Training epoch: 7, Batch_Num: 65/300, N_Loss: 0.0008865726413205266\n",
      "Training epoch: 7, Batch_Num: 66/300, N_Loss: 0.006527661811560392\n",
      "Training epoch: 7, Batch_Num: 67/300, N_Loss: 0.0045109898783266544\n",
      "Training epoch: 7, Batch_Num: 68/300, N_Loss: 0.01645873300731182\n",
      "Training epoch: 7, Batch_Num: 69/300, N_Loss: 0.009462909772992134\n",
      "Training epoch: 7, Batch_Num: 70/300, N_Loss: 0.0017114677466452122\n",
      "Training epoch: 7, Batch_Num: 71/300, N_Loss: 0.01539075281471014\n",
      "Training epoch: 7, Batch_Num: 72/300, N_Loss: 0.0021031664218753576\n",
      "Training epoch: 7, Batch_Num: 73/300, N_Loss: 0.005469969008117914\n",
      "Training epoch: 7, Batch_Num: 74/300, N_Loss: 0.006935620680451393\n",
      "Training epoch: 7, Batch_Num: 75/300, N_Loss: 0.0036629256792366505\n",
      "Training epoch: 7, Batch_Num: 76/300, N_Loss: 0.001799962599761784\n",
      "Training epoch: 7, Batch_Num: 77/300, N_Loss: 0.0027016913518309593\n",
      "Training epoch: 7, Batch_Num: 78/300, N_Loss: 0.0018257488263770938\n",
      "Training epoch: 7, Batch_Num: 79/300, N_Loss: 0.002965625375509262\n",
      "Training epoch: 7, Batch_Num: 80/300, N_Loss: 0.004166645463556051\n",
      "Training epoch: 7, Batch_Num: 81/300, N_Loss: 0.004131901543587446\n",
      "Training epoch: 7, Batch_Num: 82/300, N_Loss: 0.014712678268551826\n",
      "Training epoch: 7, Batch_Num: 83/300, N_Loss: 0.010065426118671894\n",
      "Training epoch: 7, Batch_Num: 84/300, N_Loss: 0.0028687899466603994\n",
      "Training epoch: 7, Batch_Num: 85/300, N_Loss: 0.03651180490851402\n",
      "Training epoch: 7, Batch_Num: 86/300, N_Loss: 0.006654603406786919\n",
      "Training epoch: 7, Batch_Num: 87/300, N_Loss: 0.03134365752339363\n",
      "Training epoch: 7, Batch_Num: 88/300, N_Loss: 0.012562964111566544\n",
      "Training epoch: 7, Batch_Num: 89/300, N_Loss: 0.12831388413906097\n",
      "Training epoch: 7, Batch_Num: 90/300, N_Loss: 0.00538896257057786\n",
      "Training epoch: 7, Batch_Num: 91/300, N_Loss: 0.006104137282818556\n",
      "Training epoch: 7, Batch_Num: 92/300, N_Loss: 0.007132539991289377\n",
      "Training epoch: 7, Batch_Num: 93/300, N_Loss: 0.002110986039042473\n",
      "Training epoch: 7, Batch_Num: 94/300, N_Loss: 0.018442360684275627\n",
      "Training epoch: 7, Batch_Num: 95/300, N_Loss: 0.008285703137516975\n",
      "Training epoch: 7, Batch_Num: 96/300, N_Loss: 0.008436363190412521\n",
      "Training epoch: 7, Batch_Num: 97/300, N_Loss: 0.008976694196462631\n",
      "Training epoch: 7, Batch_Num: 98/300, N_Loss: 0.014466163702309132\n",
      "Training epoch: 7, Batch_Num: 99/300, N_Loss: 0.004349900409579277\n",
      "Training epoch: 7, Batch_Num: 100/300, N_Loss: 0.02715042419731617\n",
      "Training epoch: 7, Batch_Num: 101/300, N_Loss: 0.01400856114923954\n",
      "Training epoch: 7, Batch_Num: 102/300, N_Loss: 0.0010691933566704392\n",
      "Training epoch: 7, Batch_Num: 103/300, N_Loss: 0.007436273619532585\n",
      "Training epoch: 7, Batch_Num: 104/300, N_Loss: 0.020674750208854675\n",
      "Training epoch: 7, Batch_Num: 105/300, N_Loss: 0.0026890896260738373\n",
      "Training epoch: 7, Batch_Num: 106/300, N_Loss: 0.026127586141228676\n",
      "Training epoch: 7, Batch_Num: 107/300, N_Loss: 0.005406205076724291\n",
      "Training epoch: 7, Batch_Num: 108/300, N_Loss: 0.012448495253920555\n",
      "Training epoch: 7, Batch_Num: 109/300, N_Loss: 0.004893139004707336\n",
      "Training epoch: 7, Batch_Num: 110/300, N_Loss: 0.011270184069871902\n",
      "Training epoch: 7, Batch_Num: 111/300, N_Loss: 0.0041447295807302\n",
      "Training epoch: 7, Batch_Num: 112/300, N_Loss: 0.004297045525163412\n",
      "Training epoch: 7, Batch_Num: 113/300, N_Loss: 0.00361879775300622\n",
      "Training epoch: 7, Batch_Num: 114/300, N_Loss: 0.0013399196323007345\n",
      "Training epoch: 7, Batch_Num: 115/300, N_Loss: 0.004967703018337488\n",
      "Training epoch: 7, Batch_Num: 116/300, N_Loss: 0.0033170864917337894\n",
      "Training epoch: 7, Batch_Num: 117/300, N_Loss: 0.00348124117590487\n",
      "Training epoch: 7, Batch_Num: 118/300, N_Loss: 0.006092191208153963\n",
      "Training epoch: 7, Batch_Num: 119/300, N_Loss: 0.04237811639904976\n",
      "Training epoch: 7, Batch_Num: 120/300, N_Loss: 0.021278824657201767\n",
      "Training epoch: 7, Batch_Num: 121/300, N_Loss: 0.047176286578178406\n",
      "Training epoch: 7, Batch_Num: 122/300, N_Loss: 0.01382426731288433\n",
      "Training epoch: 7, Batch_Num: 123/300, N_Loss: 0.009633096866309643\n",
      "Training epoch: 7, Batch_Num: 124/300, N_Loss: 0.008019139058887959\n",
      "Training epoch: 7, Batch_Num: 125/300, N_Loss: 0.0024709769058972597\n",
      "Training epoch: 7, Batch_Num: 126/300, N_Loss: 0.03864432871341705\n",
      "Training epoch: 7, Batch_Num: 127/300, N_Loss: 0.0056100753135979176\n",
      "Training epoch: 7, Batch_Num: 128/300, N_Loss: 0.0033069492783397436\n",
      "Training epoch: 7, Batch_Num: 129/300, N_Loss: 0.006825581658631563\n",
      "Training epoch: 7, Batch_Num: 130/300, N_Loss: 0.006320975720882416\n",
      "Training epoch: 7, Batch_Num: 131/300, N_Loss: 0.003363808151334524\n",
      "Training epoch: 7, Batch_Num: 132/300, N_Loss: 0.003255646675825119\n",
      "Training epoch: 7, Batch_Num: 133/300, N_Loss: 0.00156920135486871\n",
      "Training epoch: 7, Batch_Num: 134/300, N_Loss: 0.019332509487867355\n",
      "Training epoch: 7, Batch_Num: 135/300, N_Loss: 0.007557735778391361\n",
      "Training epoch: 7, Batch_Num: 136/300, N_Loss: 0.012151453644037247\n",
      "Training epoch: 7, Batch_Num: 137/300, N_Loss: 0.0038654410745948553\n",
      "Training epoch: 7, Batch_Num: 138/300, N_Loss: 0.046684518456459045\n",
      "Training epoch: 7, Batch_Num: 139/300, N_Loss: 0.06267213821411133\n",
      "Training epoch: 7, Batch_Num: 140/300, N_Loss: 0.08994914591312408\n",
      "Training epoch: 7, Batch_Num: 141/300, N_Loss: 0.007434242404997349\n",
      "Training epoch: 7, Batch_Num: 142/300, N_Loss: 0.0013742138398811221\n",
      "Training epoch: 7, Batch_Num: 143/300, N_Loss: 0.03204897791147232\n",
      "Training epoch: 7, Batch_Num: 144/300, N_Loss: 0.014691664837300777\n",
      "Training epoch: 7, Batch_Num: 145/300, N_Loss: 0.009070740081369877\n",
      "Training epoch: 7, Batch_Num: 146/300, N_Loss: 0.0016372757963836193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 7, Batch_Num: 147/300, N_Loss: 0.005038184579461813\n",
      "Training epoch: 7, Batch_Num: 148/300, N_Loss: 0.010600421577692032\n",
      "Training epoch: 7, Batch_Num: 149/300, N_Loss: 0.020964546129107475\n",
      "Training epoch: 7, Batch_Num: 150/300, N_Loss: 0.0034343788865953684\n",
      "Training epoch: 7, Batch_Num: 151/300, N_Loss: 0.05017722398042679\n",
      "Training epoch: 7, Batch_Num: 152/300, N_Loss: 0.04093761742115021\n",
      "Training epoch: 7, Batch_Num: 153/300, N_Loss: 0.001315913163125515\n",
      "Training epoch: 7, Batch_Num: 154/300, N_Loss: 0.003915648907423019\n",
      "Training epoch: 7, Batch_Num: 155/300, N_Loss: 0.006998344790190458\n",
      "Training epoch: 7, Batch_Num: 156/300, N_Loss: 0.06444552540779114\n",
      "Training epoch: 7, Batch_Num: 157/300, N_Loss: 0.0017021395033225417\n",
      "Training epoch: 7, Batch_Num: 158/300, N_Loss: 0.007799194660037756\n",
      "Training epoch: 7, Batch_Num: 159/300, N_Loss: 0.045529238879680634\n",
      "Training epoch: 7, Batch_Num: 160/300, N_Loss: 0.006738095544278622\n",
      "Training epoch: 7, Batch_Num: 161/300, N_Loss: 0.005733567755669355\n",
      "Training epoch: 7, Batch_Num: 162/300, N_Loss: 0.00393805094063282\n",
      "Training epoch: 7, Batch_Num: 163/300, N_Loss: 0.10543433576822281\n",
      "Training epoch: 7, Batch_Num: 164/300, N_Loss: 0.16624049842357635\n",
      "Training epoch: 7, Batch_Num: 165/300, N_Loss: 0.07382319867610931\n",
      "Training epoch: 7, Batch_Num: 166/300, N_Loss: 0.006665388122200966\n",
      "Training epoch: 7, Batch_Num: 167/300, N_Loss: 0.052102167159318924\n",
      "Training epoch: 7, Batch_Num: 168/300, N_Loss: 0.0016097816405817866\n",
      "Training epoch: 7, Batch_Num: 169/300, N_Loss: 0.002695956965908408\n",
      "Training epoch: 7, Batch_Num: 170/300, N_Loss: 0.0013934538001194596\n",
      "Training epoch: 7, Batch_Num: 171/300, N_Loss: 0.020400801673531532\n",
      "Training epoch: 7, Batch_Num: 172/300, N_Loss: 0.006855286192148924\n",
      "Training epoch: 7, Batch_Num: 173/300, N_Loss: 0.00374159449711442\n",
      "Training epoch: 7, Batch_Num: 174/300, N_Loss: 0.0017629177309572697\n",
      "Training epoch: 7, Batch_Num: 175/300, N_Loss: 0.002384460298344493\n",
      "Training epoch: 7, Batch_Num: 176/300, N_Loss: 0.0018565177451819181\n",
      "Training epoch: 7, Batch_Num: 177/300, N_Loss: 0.025648731738328934\n",
      "Training epoch: 7, Batch_Num: 178/300, N_Loss: 0.0156675074249506\n",
      "Training epoch: 7, Batch_Num: 179/300, N_Loss: 0.005523251835256815\n",
      "Training epoch: 7, Batch_Num: 180/300, N_Loss: 0.03454013913869858\n",
      "Training epoch: 7, Batch_Num: 181/300, N_Loss: 0.08490645885467529\n",
      "Training epoch: 7, Batch_Num: 182/300, N_Loss: 0.0031035118736326694\n",
      "Training epoch: 7, Batch_Num: 183/300, N_Loss: 0.0018955350387841463\n",
      "Training epoch: 7, Batch_Num: 184/300, N_Loss: 0.002569282427430153\n",
      "Training epoch: 7, Batch_Num: 185/300, N_Loss: 0.0018270134460180998\n",
      "Training epoch: 7, Batch_Num: 186/300, N_Loss: 0.007627427577972412\n",
      "Training epoch: 7, Batch_Num: 187/300, N_Loss: 0.005238780286163092\n",
      "Training epoch: 7, Batch_Num: 188/300, N_Loss: 0.008987504988908768\n",
      "Training epoch: 7, Batch_Num: 189/300, N_Loss: 0.013806321658194065\n",
      "Training epoch: 7, Batch_Num: 190/300, N_Loss: 0.00384127045981586\n",
      "Training epoch: 7, Batch_Num: 191/300, N_Loss: 0.002188473241403699\n",
      "Training epoch: 7, Batch_Num: 192/300, N_Loss: 0.013228659518063068\n",
      "Training epoch: 7, Batch_Num: 193/300, N_Loss: 0.023525914177298546\n",
      "Training epoch: 7, Batch_Num: 194/300, N_Loss: 0.017884213477373123\n",
      "Training epoch: 7, Batch_Num: 195/300, N_Loss: 0.004461063537746668\n",
      "Training epoch: 7, Batch_Num: 196/300, N_Loss: 0.008770467713475227\n",
      "Training epoch: 7, Batch_Num: 197/300, N_Loss: 0.002743770834058523\n",
      "Training epoch: 7, Batch_Num: 198/300, N_Loss: 0.03104870393872261\n",
      "Training epoch: 7, Batch_Num: 199/300, N_Loss: 0.0022162157110869884\n",
      "Training epoch: 7, Batch_Num: 200/300, N_Loss: 0.010155485942959785\n",
      "Training epoch: 7, Batch_Num: 201/300, N_Loss: 0.00876845046877861\n",
      "Training epoch: 7, Batch_Num: 202/300, N_Loss: 0.010871043428778648\n",
      "Training epoch: 7, Batch_Num: 203/300, N_Loss: 0.0016400531167164445\n",
      "Training epoch: 7, Batch_Num: 204/300, N_Loss: 0.010435467585921288\n",
      "Training epoch: 7, Batch_Num: 205/300, N_Loss: 0.0011745771626010537\n",
      "Training epoch: 7, Batch_Num: 206/300, N_Loss: 0.0052517675794661045\n",
      "Training epoch: 7, Batch_Num: 207/300, N_Loss: 0.006540399510413408\n",
      "Training epoch: 7, Batch_Num: 208/300, N_Loss: 0.0032954555936157703\n",
      "Training epoch: 7, Batch_Num: 209/300, N_Loss: 0.003772149560973048\n",
      "Training epoch: 7, Batch_Num: 210/300, N_Loss: 0.007959216833114624\n",
      "Training epoch: 7, Batch_Num: 211/300, N_Loss: 0.007012969348579645\n",
      "Training epoch: 7, Batch_Num: 212/300, N_Loss: 0.008610007353127003\n",
      "Training epoch: 7, Batch_Num: 213/300, N_Loss: 0.03216816112399101\n",
      "Training epoch: 7, Batch_Num: 214/300, N_Loss: 0.018360432237386703\n",
      "Training epoch: 7, Batch_Num: 215/300, N_Loss: 0.03648888319730759\n",
      "Training epoch: 7, Batch_Num: 216/300, N_Loss: 0.011425958015024662\n",
      "Training epoch: 7, Batch_Num: 217/300, N_Loss: 0.004553649108856916\n",
      "Training epoch: 7, Batch_Num: 218/300, N_Loss: 0.0053603993728756905\n",
      "Training epoch: 7, Batch_Num: 219/300, N_Loss: 0.0060277762822806835\n",
      "Training epoch: 7, Batch_Num: 220/300, N_Loss: 0.026588255539536476\n",
      "Training epoch: 7, Batch_Num: 221/300, N_Loss: 0.006639932282269001\n",
      "Training epoch: 7, Batch_Num: 222/300, N_Loss: 0.005784857552498579\n",
      "Training epoch: 7, Batch_Num: 223/300, N_Loss: 0.0065276590175926685\n",
      "Training epoch: 7, Batch_Num: 224/300, N_Loss: 0.0029258106369525194\n",
      "Training epoch: 7, Batch_Num: 225/300, N_Loss: 0.020041828975081444\n",
      "Training epoch: 7, Batch_Num: 226/300, N_Loss: 0.006135391071438789\n",
      "Training epoch: 7, Batch_Num: 227/300, N_Loss: 0.011482644826173782\n",
      "Training epoch: 7, Batch_Num: 228/300, N_Loss: 0.006488351617008448\n",
      "Training epoch: 7, Batch_Num: 229/300, N_Loss: 0.0019890808034688234\n",
      "Training epoch: 7, Batch_Num: 230/300, N_Loss: 0.02989540435373783\n",
      "Training epoch: 7, Batch_Num: 231/300, N_Loss: 0.003834093688055873\n",
      "Training epoch: 7, Batch_Num: 232/300, N_Loss: 0.021906675770878792\n",
      "Training epoch: 7, Batch_Num: 233/300, N_Loss: 0.004596357699483633\n",
      "Training epoch: 7, Batch_Num: 234/300, N_Loss: 0.010495029389858246\n",
      "Training epoch: 7, Batch_Num: 235/300, N_Loss: 0.018785063177347183\n",
      "Training epoch: 7, Batch_Num: 236/300, N_Loss: 0.001174980541691184\n",
      "Training epoch: 7, Batch_Num: 237/300, N_Loss: 0.014954537153244019\n",
      "Training epoch: 7, Batch_Num: 238/300, N_Loss: 0.00554498890414834\n",
      "Training epoch: 7, Batch_Num: 239/300, N_Loss: 0.018851323053240776\n",
      "Training epoch: 7, Batch_Num: 240/300, N_Loss: 0.003096366534009576\n",
      "Training epoch: 7, Batch_Num: 241/300, N_Loss: 0.010124367661774158\n",
      "Training epoch: 7, Batch_Num: 242/300, N_Loss: 0.0025983627419918776\n",
      "Training epoch: 7, Batch_Num: 243/300, N_Loss: 0.01278934720903635\n",
      "Training epoch: 7, Batch_Num: 244/300, N_Loss: 0.002626166446134448\n",
      "Training epoch: 7, Batch_Num: 245/300, N_Loss: 0.01050114817917347\n",
      "Training epoch: 7, Batch_Num: 246/300, N_Loss: 0.09644400328397751\n",
      "Training epoch: 7, Batch_Num: 247/300, N_Loss: 0.0013694997178390622\n",
      "Training epoch: 7, Batch_Num: 248/300, N_Loss: 0.011433606036007404\n",
      "Training epoch: 7, Batch_Num: 249/300, N_Loss: 0.0026971211191266775\n",
      "Training epoch: 7, Batch_Num: 250/300, N_Loss: 0.048582352697849274\n",
      "Training epoch: 7, Batch_Num: 251/300, N_Loss: 0.01804594323039055\n",
      "Training epoch: 7, Batch_Num: 252/300, N_Loss: 0.004492917563766241\n",
      "Training epoch: 7, Batch_Num: 253/300, N_Loss: 0.009188047610223293\n",
      "Training epoch: 7, Batch_Num: 254/300, N_Loss: 0.014847714453935623\n",
      "Training epoch: 7, Batch_Num: 255/300, N_Loss: 0.00385672552511096\n",
      "Training epoch: 7, Batch_Num: 256/300, N_Loss: 0.012780340388417244\n",
      "Training epoch: 7, Batch_Num: 257/300, N_Loss: 0.008900124579668045\n",
      "Training epoch: 7, Batch_Num: 258/300, N_Loss: 0.005526050925254822\n",
      "Training epoch: 7, Batch_Num: 259/300, N_Loss: 0.00293180625885725\n",
      "Training epoch: 7, Batch_Num: 260/300, N_Loss: 0.029049677774310112\n",
      "Training epoch: 7, Batch_Num: 261/300, N_Loss: 0.003631429746747017\n",
      "Training epoch: 7, Batch_Num: 262/300, N_Loss: 0.025428621098399162\n",
      "Training epoch: 7, Batch_Num: 263/300, N_Loss: 0.004088628571480513\n",
      "Training epoch: 7, Batch_Num: 264/300, N_Loss: 0.018665030598640442\n",
      "Training epoch: 7, Batch_Num: 265/300, N_Loss: 0.004343077074736357\n",
      "Training epoch: 7, Batch_Num: 266/300, N_Loss: 0.030700236558914185\n",
      "Training epoch: 7, Batch_Num: 267/300, N_Loss: 0.007856525480747223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 7, Batch_Num: 268/300, N_Loss: 0.004334526602178812\n",
      "Training epoch: 7, Batch_Num: 269/300, N_Loss: 0.0042188274674117565\n",
      "Training epoch: 7, Batch_Num: 270/300, N_Loss: 0.0027088485658168793\n",
      "Training epoch: 7, Batch_Num: 271/300, N_Loss: 0.030012857168912888\n",
      "Training epoch: 7, Batch_Num: 272/300, N_Loss: 0.05621803551912308\n",
      "Training epoch: 7, Batch_Num: 273/300, N_Loss: 0.0075874170288443565\n",
      "Training epoch: 7, Batch_Num: 274/300, N_Loss: 0.004153769928961992\n",
      "Training epoch: 7, Batch_Num: 275/300, N_Loss: 0.03028983436524868\n",
      "Training epoch: 7, Batch_Num: 276/300, N_Loss: 0.0021092004608362913\n",
      "Training epoch: 7, Batch_Num: 277/300, N_Loss: 0.01077541895210743\n",
      "Training epoch: 7, Batch_Num: 278/300, N_Loss: 0.001176723511889577\n",
      "Training epoch: 7, Batch_Num: 279/300, N_Loss: 0.002269288757815957\n",
      "Training epoch: 7, Batch_Num: 280/300, N_Loss: 0.0016112818848341703\n",
      "Training epoch: 7, Batch_Num: 281/300, N_Loss: 0.015372148714959621\n",
      "Training epoch: 7, Batch_Num: 282/300, N_Loss: 0.005736915394663811\n",
      "Training epoch: 7, Batch_Num: 283/300, N_Loss: 0.005313752684742212\n",
      "Training epoch: 7, Batch_Num: 284/300, N_Loss: 0.00557404849678278\n",
      "Training epoch: 7, Batch_Num: 285/300, N_Loss: 0.014075445011258125\n",
      "Training epoch: 7, Batch_Num: 286/300, N_Loss: 0.020023418590426445\n",
      "Training epoch: 7, Batch_Num: 287/300, N_Loss: 0.0030909355264157057\n",
      "Training epoch: 7, Batch_Num: 288/300, N_Loss: 0.0056065781973302364\n",
      "Training epoch: 7, Batch_Num: 289/300, N_Loss: 0.00730297202244401\n",
      "Training epoch: 7, Batch_Num: 290/300, N_Loss: 0.009160622954368591\n",
      "Training epoch: 7, Batch_Num: 291/300, N_Loss: 0.006551430560648441\n",
      "Training epoch: 7, Batch_Num: 292/300, N_Loss: 0.008693080395460129\n",
      "Training epoch: 7, Batch_Num: 293/300, N_Loss: 0.0059206788428127766\n",
      "Training epoch: 7, Batch_Num: 294/300, N_Loss: 0.006327990908175707\n",
      "Training epoch: 7, Batch_Num: 295/300, N_Loss: 0.012272282503545284\n",
      "Training epoch: 7, Batch_Num: 296/300, N_Loss: 0.020618861541152\n",
      "Training epoch: 7, Batch_Num: 297/300, N_Loss: 0.01972927153110504\n",
      "Training epoch: 7, Batch_Num: 298/300, N_Loss: 0.0018230988644063473\n",
      "Training epoch: 7, Batch_Num: 299/300, N_Loss: 0.02855101227760315\n",
      "Training epoch: 7, Batch_Num: 300/300, N_Loss: 0.0015052289236336946\n",
      "Training epoch: 8, Batch_Num: 1/300, N_Loss: 0.008013958111405373\n",
      "Training epoch: 8, Batch_Num: 2/300, N_Loss: 0.0011377823539078236\n",
      "Training epoch: 8, Batch_Num: 3/300, N_Loss: 0.019810743629932404\n",
      "Training epoch: 8, Batch_Num: 4/300, N_Loss: 0.011425640434026718\n",
      "Training epoch: 8, Batch_Num: 5/300, N_Loss: 0.004232610575854778\n",
      "Training epoch: 8, Batch_Num: 6/300, N_Loss: 0.01313368882983923\n",
      "Training epoch: 8, Batch_Num: 7/300, N_Loss: 0.0050295148976147175\n",
      "Training epoch: 8, Batch_Num: 8/300, N_Loss: 0.00530735868960619\n",
      "Training epoch: 8, Batch_Num: 9/300, N_Loss: 0.01927080564200878\n",
      "Training epoch: 8, Batch_Num: 10/300, N_Loss: 0.003463104600086808\n",
      "Training epoch: 8, Batch_Num: 11/300, N_Loss: 0.002657980192452669\n",
      "Training epoch: 8, Batch_Num: 12/300, N_Loss: 0.004332202021032572\n",
      "Training epoch: 8, Batch_Num: 13/300, N_Loss: 0.01937485858798027\n",
      "Training epoch: 8, Batch_Num: 14/300, N_Loss: 0.001917384215630591\n",
      "Training epoch: 8, Batch_Num: 15/300, N_Loss: 0.0012624962255358696\n",
      "Training epoch: 8, Batch_Num: 16/300, N_Loss: 0.0014350501587614417\n",
      "Training epoch: 8, Batch_Num: 17/300, N_Loss: 0.0026536656077951193\n",
      "Training epoch: 8, Batch_Num: 18/300, N_Loss: 0.016471128910779953\n",
      "Training epoch: 8, Batch_Num: 19/300, N_Loss: 0.008962444961071014\n",
      "Training epoch: 8, Batch_Num: 20/300, N_Loss: 0.004242266528308392\n",
      "Training epoch: 8, Batch_Num: 21/300, N_Loss: 0.006685934029519558\n",
      "Training epoch: 8, Batch_Num: 22/300, N_Loss: 0.02161811850965023\n",
      "Training epoch: 8, Batch_Num: 23/300, N_Loss: 0.0021472966764122248\n",
      "Training epoch: 8, Batch_Num: 24/300, N_Loss: 0.08718530088663101\n",
      "Training epoch: 8, Batch_Num: 25/300, N_Loss: 0.0010274630039930344\n",
      "Training epoch: 8, Batch_Num: 26/300, N_Loss: 0.016327379271388054\n",
      "Training epoch: 8, Batch_Num: 27/300, N_Loss: 0.013839668594300747\n",
      "Training epoch: 8, Batch_Num: 28/300, N_Loss: 0.0013073409209027886\n",
      "Training epoch: 8, Batch_Num: 29/300, N_Loss: 0.01044262945652008\n",
      "Training epoch: 8, Batch_Num: 30/300, N_Loss: 0.002665436128154397\n",
      "Training epoch: 8, Batch_Num: 31/300, N_Loss: 0.024044455960392952\n",
      "Training epoch: 8, Batch_Num: 32/300, N_Loss: 0.005936602596193552\n",
      "Training epoch: 8, Batch_Num: 33/300, N_Loss: 0.0006482000462710857\n",
      "Training epoch: 8, Batch_Num: 34/300, N_Loss: 0.0017655128613114357\n",
      "Training epoch: 8, Batch_Num: 35/300, N_Loss: 0.005675069522112608\n",
      "Training epoch: 8, Batch_Num: 36/300, N_Loss: 0.003873481648042798\n",
      "Training epoch: 8, Batch_Num: 37/300, N_Loss: 0.0026876390911638737\n",
      "Training epoch: 8, Batch_Num: 38/300, N_Loss: 0.016576429829001427\n",
      "Training epoch: 8, Batch_Num: 39/300, N_Loss: 0.009197366423904896\n",
      "Training epoch: 8, Batch_Num: 40/300, N_Loss: 0.022394156083464622\n",
      "Training epoch: 8, Batch_Num: 41/300, N_Loss: 0.001201519276946783\n",
      "Training epoch: 8, Batch_Num: 42/300, N_Loss: 0.005031027365475893\n",
      "Training epoch: 8, Batch_Num: 43/300, N_Loss: 0.004946164321154356\n",
      "Training epoch: 8, Batch_Num: 44/300, N_Loss: 0.04043250530958176\n",
      "Training epoch: 8, Batch_Num: 45/300, N_Loss: 0.00309151504188776\n",
      "Training epoch: 8, Batch_Num: 46/300, N_Loss: 0.00714334100484848\n",
      "Training epoch: 8, Batch_Num: 47/300, N_Loss: 0.009799434803426266\n",
      "Training epoch: 8, Batch_Num: 48/300, N_Loss: 0.03987288847565651\n",
      "Training epoch: 8, Batch_Num: 49/300, N_Loss: 0.0035505834966897964\n",
      "Training epoch: 8, Batch_Num: 50/300, N_Loss: 0.004661595448851585\n",
      "Training epoch: 8, Batch_Num: 51/300, N_Loss: 0.03916513919830322\n",
      "Training epoch: 8, Batch_Num: 52/300, N_Loss: 0.00840254221111536\n",
      "Training epoch: 8, Batch_Num: 53/300, N_Loss: 0.008938589133322239\n",
      "Training epoch: 8, Batch_Num: 54/300, N_Loss: 0.006152742076665163\n",
      "Training epoch: 8, Batch_Num: 55/300, N_Loss: 0.013569350354373455\n",
      "Training epoch: 8, Batch_Num: 56/300, N_Loss: 0.001050940016284585\n",
      "Training epoch: 8, Batch_Num: 57/300, N_Loss: 0.01820763386785984\n",
      "Training epoch: 8, Batch_Num: 58/300, N_Loss: 0.002230999758467078\n",
      "Training epoch: 8, Batch_Num: 59/300, N_Loss: 0.0041980743408203125\n",
      "Training epoch: 8, Batch_Num: 60/300, N_Loss: 0.005678477231413126\n",
      "Training epoch: 8, Batch_Num: 61/300, N_Loss: 0.007238214369863272\n",
      "Training epoch: 8, Batch_Num: 62/300, N_Loss: 0.017131872475147247\n",
      "Training epoch: 8, Batch_Num: 63/300, N_Loss: 0.006798809394240379\n",
      "Training epoch: 8, Batch_Num: 64/300, N_Loss: 0.013513116165995598\n",
      "Training epoch: 8, Batch_Num: 65/300, N_Loss: 0.012171920388936996\n",
      "Training epoch: 8, Batch_Num: 66/300, N_Loss: 0.0020456602796912193\n",
      "Training epoch: 8, Batch_Num: 67/300, N_Loss: 0.020463144406676292\n",
      "Training epoch: 8, Batch_Num: 68/300, N_Loss: 0.06410929560661316\n",
      "Training epoch: 8, Batch_Num: 69/300, N_Loss: 0.015177297405898571\n",
      "Training epoch: 8, Batch_Num: 70/300, N_Loss: 0.0037464662455022335\n",
      "Training epoch: 8, Batch_Num: 71/300, N_Loss: 0.004977736622095108\n",
      "Training epoch: 8, Batch_Num: 72/300, N_Loss: 0.017019717022776604\n",
      "Training epoch: 8, Batch_Num: 73/300, N_Loss: 0.011522217653691769\n",
      "Training epoch: 8, Batch_Num: 74/300, N_Loss: 0.008773510344326496\n",
      "Training epoch: 8, Batch_Num: 75/300, N_Loss: 0.014737055636942387\n",
      "Training epoch: 8, Batch_Num: 76/300, N_Loss: 0.012990850955247879\n",
      "Training epoch: 8, Batch_Num: 77/300, N_Loss: 0.02681864984333515\n",
      "Training epoch: 8, Batch_Num: 78/300, N_Loss: 0.013303256593644619\n",
      "Training epoch: 8, Batch_Num: 79/300, N_Loss: 0.0255114808678627\n",
      "Training epoch: 8, Batch_Num: 80/300, N_Loss: 0.02085680142045021\n",
      "Training epoch: 8, Batch_Num: 81/300, N_Loss: 0.022158397361636162\n",
      "Training epoch: 8, Batch_Num: 82/300, N_Loss: 0.054757241159677505\n",
      "Training epoch: 8, Batch_Num: 83/300, N_Loss: 0.010403001680970192\n",
      "Training epoch: 8, Batch_Num: 84/300, N_Loss: 0.001546841929666698\n",
      "Training epoch: 8, Batch_Num: 85/300, N_Loss: 0.003399841021746397\n",
      "Training epoch: 8, Batch_Num: 86/300, N_Loss: 0.018094466999173164\n",
      "Training epoch: 8, Batch_Num: 87/300, N_Loss: 0.006992504000663757\n",
      "Training epoch: 8, Batch_Num: 88/300, N_Loss: 0.002789800986647606\n",
      "Training epoch: 8, Batch_Num: 89/300, N_Loss: 0.0015695577021688223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 8, Batch_Num: 90/300, N_Loss: 0.028275376185774803\n",
      "Training epoch: 8, Batch_Num: 91/300, N_Loss: 0.0027476439718157053\n",
      "Training epoch: 8, Batch_Num: 92/300, N_Loss: 0.008144449442625046\n",
      "Training epoch: 8, Batch_Num: 93/300, N_Loss: 0.0015298882499337196\n",
      "Training epoch: 8, Batch_Num: 94/300, N_Loss: 0.001540773082524538\n",
      "Training epoch: 8, Batch_Num: 95/300, N_Loss: 0.004564613103866577\n",
      "Training epoch: 8, Batch_Num: 96/300, N_Loss: 0.006487619131803513\n",
      "Training epoch: 8, Batch_Num: 97/300, N_Loss: 0.0014440346276387572\n",
      "Training epoch: 8, Batch_Num: 98/300, N_Loss: 0.01677118055522442\n",
      "Training epoch: 8, Batch_Num: 99/300, N_Loss: 0.05851026996970177\n",
      "Training epoch: 8, Batch_Num: 100/300, N_Loss: 0.06703583896160126\n",
      "Training epoch: 8, Batch_Num: 101/300, N_Loss: 0.013438805006444454\n",
      "Training epoch: 8, Batch_Num: 102/300, N_Loss: 0.007431959267705679\n",
      "Training epoch: 8, Batch_Num: 103/300, N_Loss: 0.004177285358309746\n",
      "Training epoch: 8, Batch_Num: 104/300, N_Loss: 0.0021814352367073298\n",
      "Training epoch: 8, Batch_Num: 105/300, N_Loss: 0.011885261163115501\n",
      "Training epoch: 8, Batch_Num: 106/300, N_Loss: 0.005060336086899042\n",
      "Training epoch: 8, Batch_Num: 107/300, N_Loss: 0.018641522154211998\n",
      "Training epoch: 8, Batch_Num: 108/300, N_Loss: 0.003966453950852156\n",
      "Training epoch: 8, Batch_Num: 109/300, N_Loss: 0.00964338332414627\n",
      "Training epoch: 8, Batch_Num: 110/300, N_Loss: 0.002222817623987794\n",
      "Training epoch: 8, Batch_Num: 111/300, N_Loss: 0.005269749090075493\n",
      "Training epoch: 8, Batch_Num: 112/300, N_Loss: 0.031853850930929184\n",
      "Training epoch: 8, Batch_Num: 113/300, N_Loss: 0.005522693041712046\n",
      "Training epoch: 8, Batch_Num: 114/300, N_Loss: 0.0053919414058327675\n",
      "Training epoch: 8, Batch_Num: 115/300, N_Loss: 0.007641477044671774\n",
      "Training epoch: 8, Batch_Num: 116/300, N_Loss: 0.029868440702557564\n",
      "Training epoch: 8, Batch_Num: 117/300, N_Loss: 0.01112932525575161\n",
      "Training epoch: 8, Batch_Num: 118/300, N_Loss: 0.006723624654114246\n",
      "Training epoch: 8, Batch_Num: 119/300, N_Loss: 0.008557409048080444\n",
      "Training epoch: 8, Batch_Num: 120/300, N_Loss: 0.010147773660719395\n",
      "Training epoch: 8, Batch_Num: 121/300, N_Loss: 0.00408117426559329\n",
      "Training epoch: 8, Batch_Num: 122/300, N_Loss: 0.14102628827095032\n",
      "Training epoch: 8, Batch_Num: 123/300, N_Loss: 0.029712960124015808\n",
      "Training epoch: 8, Batch_Num: 124/300, N_Loss: 0.006636695936322212\n",
      "Training epoch: 8, Batch_Num: 125/300, N_Loss: 0.0029138089157640934\n",
      "Training epoch: 8, Batch_Num: 126/300, N_Loss: 0.009623459540307522\n",
      "Training epoch: 8, Batch_Num: 127/300, N_Loss: 0.03321549668908119\n",
      "Training epoch: 8, Batch_Num: 128/300, N_Loss: 0.1378643661737442\n",
      "Training epoch: 8, Batch_Num: 129/300, N_Loss: 0.007320309989154339\n",
      "Training epoch: 8, Batch_Num: 130/300, N_Loss: 0.005075004417449236\n",
      "Training epoch: 8, Batch_Num: 131/300, N_Loss: 0.012764587998390198\n",
      "Training epoch: 8, Batch_Num: 132/300, N_Loss: 0.019141972064971924\n",
      "Training epoch: 8, Batch_Num: 133/300, N_Loss: 0.005580447614192963\n",
      "Training epoch: 8, Batch_Num: 134/300, N_Loss: 0.022705519571900368\n",
      "Training epoch: 8, Batch_Num: 135/300, N_Loss: 0.014219347387552261\n",
      "Training epoch: 8, Batch_Num: 136/300, N_Loss: 0.0008716121083125472\n",
      "Training epoch: 8, Batch_Num: 137/300, N_Loss: 0.000595614779740572\n",
      "Training epoch: 8, Batch_Num: 138/300, N_Loss: 0.004514599684625864\n",
      "Training epoch: 8, Batch_Num: 139/300, N_Loss: 0.00089641084196046\n",
      "Training epoch: 8, Batch_Num: 140/300, N_Loss: 0.007350502535700798\n",
      "Training epoch: 8, Batch_Num: 141/300, N_Loss: 0.0037770322524011135\n",
      "Training epoch: 8, Batch_Num: 142/300, N_Loss: 0.014798595570027828\n",
      "Training epoch: 8, Batch_Num: 143/300, N_Loss: 0.02276701293885708\n",
      "Training epoch: 8, Batch_Num: 144/300, N_Loss: 0.021633561700582504\n",
      "Training epoch: 8, Batch_Num: 145/300, N_Loss: 0.0023866414558142424\n",
      "Training epoch: 8, Batch_Num: 146/300, N_Loss: 0.0035973244812339544\n",
      "Training epoch: 8, Batch_Num: 147/300, N_Loss: 0.0033633513376116753\n",
      "Training epoch: 8, Batch_Num: 148/300, N_Loss: 0.025747234001755714\n",
      "Training epoch: 8, Batch_Num: 149/300, N_Loss: 0.009454473853111267\n",
      "Training epoch: 8, Batch_Num: 150/300, N_Loss: 0.022580109536647797\n",
      "Training epoch: 8, Batch_Num: 151/300, N_Loss: 0.035101164132356644\n",
      "Training epoch: 8, Batch_Num: 152/300, N_Loss: 0.008345996029675007\n",
      "Training epoch: 8, Batch_Num: 153/300, N_Loss: 0.0014091467019170523\n",
      "Training epoch: 8, Batch_Num: 154/300, N_Loss: 0.021337712183594704\n",
      "Training epoch: 8, Batch_Num: 155/300, N_Loss: 0.0024179781321436167\n",
      "Training epoch: 8, Batch_Num: 156/300, N_Loss: 0.0021664598025381565\n",
      "Training epoch: 8, Batch_Num: 157/300, N_Loss: 0.006943078245967627\n",
      "Training epoch: 8, Batch_Num: 158/300, N_Loss: 0.007886221632361412\n",
      "Training epoch: 8, Batch_Num: 159/300, N_Loss: 0.007085634861141443\n",
      "Training epoch: 8, Batch_Num: 160/300, N_Loss: 0.0061800372786819935\n",
      "Training epoch: 8, Batch_Num: 161/300, N_Loss: 0.005629036109894514\n",
      "Training epoch: 8, Batch_Num: 162/300, N_Loss: 0.06955967098474503\n",
      "Training epoch: 8, Batch_Num: 163/300, N_Loss: 0.0032128712628036737\n",
      "Training epoch: 8, Batch_Num: 164/300, N_Loss: 0.0032384165097028017\n",
      "Training epoch: 8, Batch_Num: 165/300, N_Loss: 0.0013149473816156387\n",
      "Training epoch: 8, Batch_Num: 166/300, N_Loss: 0.004715802613645792\n",
      "Training epoch: 8, Batch_Num: 167/300, N_Loss: 0.004380915779620409\n",
      "Training epoch: 8, Batch_Num: 168/300, N_Loss: 0.006441093515604734\n",
      "Training epoch: 8, Batch_Num: 169/300, N_Loss: 0.015806887298822403\n",
      "Training epoch: 8, Batch_Num: 170/300, N_Loss: 0.006336485967040062\n",
      "Training epoch: 8, Batch_Num: 171/300, N_Loss: 0.011050721630454063\n",
      "Training epoch: 8, Batch_Num: 172/300, N_Loss: 0.000704382371623069\n",
      "Training epoch: 8, Batch_Num: 173/300, N_Loss: 0.0033579107839614153\n",
      "Training epoch: 8, Batch_Num: 174/300, N_Loss: 0.013534052297472954\n",
      "Training epoch: 8, Batch_Num: 175/300, N_Loss: 0.00602221954613924\n",
      "Training epoch: 8, Batch_Num: 176/300, N_Loss: 0.0013185483403503895\n",
      "Training epoch: 8, Batch_Num: 177/300, N_Loss: 0.0065018972381949425\n",
      "Training epoch: 8, Batch_Num: 178/300, N_Loss: 0.14607356488704681\n",
      "Training epoch: 8, Batch_Num: 179/300, N_Loss: 0.09873425960540771\n",
      "Training epoch: 8, Batch_Num: 180/300, N_Loss: 0.002864793175831437\n",
      "Training epoch: 8, Batch_Num: 181/300, N_Loss: 0.07256220281124115\n",
      "Training epoch: 8, Batch_Num: 182/300, N_Loss: 0.021581433713436127\n",
      "Training epoch: 8, Batch_Num: 183/300, N_Loss: 0.001724372385069728\n",
      "Training epoch: 8, Batch_Num: 184/300, N_Loss: 0.01150189433246851\n",
      "Training epoch: 8, Batch_Num: 185/300, N_Loss: 0.055714406073093414\n",
      "Training epoch: 8, Batch_Num: 186/300, N_Loss: 0.01678539253771305\n",
      "Training epoch: 8, Batch_Num: 187/300, N_Loss: 0.003589015221223235\n",
      "Training epoch: 8, Batch_Num: 188/300, N_Loss: 0.07008538395166397\n",
      "Training epoch: 8, Batch_Num: 189/300, N_Loss: 0.010285543277859688\n",
      "Training epoch: 8, Batch_Num: 190/300, N_Loss: 0.03813162073493004\n",
      "Training epoch: 8, Batch_Num: 191/300, N_Loss: 0.015945425257086754\n",
      "Training epoch: 8, Batch_Num: 192/300, N_Loss: 0.005600011441856623\n",
      "Training epoch: 8, Batch_Num: 193/300, N_Loss: 0.013344436883926392\n",
      "Training epoch: 8, Batch_Num: 194/300, N_Loss: 0.1943509429693222\n",
      "Training epoch: 8, Batch_Num: 195/300, N_Loss: 0.009697802364826202\n",
      "Training epoch: 8, Batch_Num: 196/300, N_Loss: 0.0030476523097604513\n",
      "Training epoch: 8, Batch_Num: 197/300, N_Loss: 0.005703729577362537\n",
      "Training epoch: 8, Batch_Num: 198/300, N_Loss: 0.027682945132255554\n",
      "Training epoch: 8, Batch_Num: 199/300, N_Loss: 0.02405993826687336\n",
      "Training epoch: 8, Batch_Num: 200/300, N_Loss: 0.0033134017139673233\n",
      "Training epoch: 8, Batch_Num: 201/300, N_Loss: 0.003523783991113305\n",
      "Training epoch: 8, Batch_Num: 202/300, N_Loss: 0.012461267411708832\n",
      "Training epoch: 8, Batch_Num: 203/300, N_Loss: 0.03661423549056053\n",
      "Training epoch: 8, Batch_Num: 204/300, N_Loss: 0.014114332385361195\n",
      "Training epoch: 8, Batch_Num: 205/300, N_Loss: 0.009190956130623817\n",
      "Training epoch: 8, Batch_Num: 206/300, N_Loss: 0.004767094738781452\n",
      "Training epoch: 8, Batch_Num: 207/300, N_Loss: 0.0022052067797631025\n",
      "Training epoch: 8, Batch_Num: 208/300, N_Loss: 0.002535730367526412\n",
      "Training epoch: 8, Batch_Num: 209/300, N_Loss: 0.003113087499514222\n",
      "Training epoch: 8, Batch_Num: 210/300, N_Loss: 0.002679781522601843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 8, Batch_Num: 211/300, N_Loss: 0.0020952566992491484\n",
      "Training epoch: 8, Batch_Num: 212/300, N_Loss: 0.06165285408496857\n",
      "Training epoch: 8, Batch_Num: 213/300, N_Loss: 0.005884463898837566\n",
      "Training epoch: 8, Batch_Num: 214/300, N_Loss: 0.0037166241090744734\n",
      "Training epoch: 8, Batch_Num: 215/300, N_Loss: 0.0033168199006468058\n",
      "Training epoch: 8, Batch_Num: 216/300, N_Loss: 0.08157705515623093\n",
      "Training epoch: 8, Batch_Num: 217/300, N_Loss: 0.018222913146018982\n",
      "Training epoch: 8, Batch_Num: 218/300, N_Loss: 0.008644814603030682\n",
      "Training epoch: 8, Batch_Num: 219/300, N_Loss: 0.0021487348712980747\n",
      "Training epoch: 8, Batch_Num: 220/300, N_Loss: 0.0024909828789532185\n",
      "Training epoch: 8, Batch_Num: 221/300, N_Loss: 0.0029782019555568695\n",
      "Training epoch: 8, Batch_Num: 222/300, N_Loss: 0.00921813864260912\n",
      "Training epoch: 8, Batch_Num: 223/300, N_Loss: 0.010090832598507404\n",
      "Training epoch: 8, Batch_Num: 224/300, N_Loss: 0.004799272865056992\n",
      "Training epoch: 8, Batch_Num: 225/300, N_Loss: 0.004435159731656313\n",
      "Training epoch: 8, Batch_Num: 226/300, N_Loss: 0.002843536203727126\n",
      "Training epoch: 8, Batch_Num: 227/300, N_Loss: 0.004148375242948532\n",
      "Training epoch: 8, Batch_Num: 228/300, N_Loss: 0.010615920647978783\n",
      "Training epoch: 8, Batch_Num: 229/300, N_Loss: 0.032712023705244064\n",
      "Training epoch: 8, Batch_Num: 230/300, N_Loss: 0.07906851172447205\n",
      "Training epoch: 8, Batch_Num: 231/300, N_Loss: 0.006060153245925903\n",
      "Training epoch: 8, Batch_Num: 232/300, N_Loss: 0.004365540575236082\n",
      "Training epoch: 8, Batch_Num: 233/300, N_Loss: 0.018863709643483162\n",
      "Training epoch: 8, Batch_Num: 234/300, N_Loss: 0.010165553539991379\n",
      "Training epoch: 8, Batch_Num: 235/300, N_Loss: 0.008288118988275528\n",
      "Training epoch: 8, Batch_Num: 236/300, N_Loss: 0.00639572087675333\n",
      "Training epoch: 8, Batch_Num: 237/300, N_Loss: 0.004208761733025312\n",
      "Training epoch: 8, Batch_Num: 238/300, N_Loss: 0.002374789444729686\n",
      "Training epoch: 8, Batch_Num: 239/300, N_Loss: 0.001992523204535246\n",
      "Training epoch: 8, Batch_Num: 240/300, N_Loss: 0.004350016824901104\n",
      "Training epoch: 8, Batch_Num: 241/300, N_Loss: 0.001488944748416543\n",
      "Training epoch: 8, Batch_Num: 242/300, N_Loss: 0.0024643673095852137\n",
      "Training epoch: 8, Batch_Num: 243/300, N_Loss: 0.01974431425333023\n",
      "Training epoch: 8, Batch_Num: 244/300, N_Loss: 0.031356051564216614\n",
      "Training epoch: 8, Batch_Num: 245/300, N_Loss: 0.0032962451223284006\n",
      "Training epoch: 8, Batch_Num: 246/300, N_Loss: 0.008832169696688652\n",
      "Training epoch: 8, Batch_Num: 247/300, N_Loss: 0.0032072418835014105\n",
      "Training epoch: 8, Batch_Num: 248/300, N_Loss: 0.0024526144843548536\n",
      "Training epoch: 8, Batch_Num: 249/300, N_Loss: 0.015290646813809872\n",
      "Training epoch: 8, Batch_Num: 250/300, N_Loss: 0.005746013019233942\n",
      "Training epoch: 8, Batch_Num: 251/300, N_Loss: 0.006639222614467144\n",
      "Training epoch: 8, Batch_Num: 252/300, N_Loss: 0.005652932450175285\n",
      "Training epoch: 8, Batch_Num: 253/300, N_Loss: 0.0032937333453446627\n",
      "Training epoch: 8, Batch_Num: 254/300, N_Loss: 0.026574501767754555\n",
      "Training epoch: 8, Batch_Num: 255/300, N_Loss: 0.0004977399366907775\n",
      "Training epoch: 8, Batch_Num: 256/300, N_Loss: 0.004624511580914259\n",
      "Training epoch: 8, Batch_Num: 257/300, N_Loss: 0.004556713160127401\n",
      "Training epoch: 8, Batch_Num: 258/300, N_Loss: 0.008475524373352528\n",
      "Training epoch: 8, Batch_Num: 259/300, N_Loss: 0.006417028605937958\n",
      "Training epoch: 8, Batch_Num: 260/300, N_Loss: 0.016898391768336296\n",
      "Training epoch: 8, Batch_Num: 261/300, N_Loss: 0.11185616254806519\n",
      "Training epoch: 8, Batch_Num: 262/300, N_Loss: 0.006962470710277557\n",
      "Training epoch: 8, Batch_Num: 263/300, N_Loss: 0.06463824957609177\n",
      "Training epoch: 8, Batch_Num: 264/300, N_Loss: 0.005333418492227793\n",
      "Training epoch: 8, Batch_Num: 265/300, N_Loss: 0.0048901475965976715\n",
      "Training epoch: 8, Batch_Num: 266/300, N_Loss: 0.020760459825396538\n",
      "Training epoch: 8, Batch_Num: 267/300, N_Loss: 0.0030807815492153168\n",
      "Training epoch: 8, Batch_Num: 268/300, N_Loss: 0.0024375023785978556\n",
      "Training epoch: 8, Batch_Num: 269/300, N_Loss: 0.001839960808865726\n",
      "Training epoch: 8, Batch_Num: 270/300, N_Loss: 0.0022592677269130945\n",
      "Training epoch: 8, Batch_Num: 271/300, N_Loss: 0.002892224583774805\n",
      "Training epoch: 8, Batch_Num: 272/300, N_Loss: 0.0073730964213609695\n",
      "Training epoch: 8, Batch_Num: 273/300, N_Loss: 0.003859815886244178\n",
      "Training epoch: 8, Batch_Num: 274/300, N_Loss: 0.0037282882258296013\n",
      "Training epoch: 8, Batch_Num: 275/300, N_Loss: 0.011894067749381065\n",
      "Training epoch: 8, Batch_Num: 276/300, N_Loss: 0.01906728744506836\n",
      "Training epoch: 8, Batch_Num: 277/300, N_Loss: 0.009399615228176117\n",
      "Training epoch: 8, Batch_Num: 278/300, N_Loss: 0.008019686676561832\n",
      "Training epoch: 8, Batch_Num: 279/300, N_Loss: 0.0024068052880465984\n",
      "Training epoch: 8, Batch_Num: 280/300, N_Loss: 0.0037701958790421486\n",
      "Training epoch: 8, Batch_Num: 281/300, N_Loss: 0.013403112068772316\n",
      "Training epoch: 8, Batch_Num: 282/300, N_Loss: 0.0014411271549761295\n",
      "Training epoch: 8, Batch_Num: 283/300, N_Loss: 0.002120805438607931\n",
      "Training epoch: 8, Batch_Num: 284/300, N_Loss: 0.01611335761845112\n",
      "Training epoch: 8, Batch_Num: 285/300, N_Loss: 0.007667991798371077\n",
      "Training epoch: 8, Batch_Num: 286/300, N_Loss: 0.02496146596968174\n",
      "Training epoch: 8, Batch_Num: 287/300, N_Loss: 0.00739850290119648\n",
      "Training epoch: 8, Batch_Num: 288/300, N_Loss: 0.04055587202310562\n",
      "Training epoch: 8, Batch_Num: 289/300, N_Loss: 0.0028775562532246113\n",
      "Training epoch: 8, Batch_Num: 290/300, N_Loss: 0.011418747715651989\n",
      "Training epoch: 8, Batch_Num: 291/300, N_Loss: 0.0023545713629573584\n",
      "Training epoch: 8, Batch_Num: 292/300, N_Loss: 0.011276166886091232\n",
      "Training epoch: 8, Batch_Num: 293/300, N_Loss: 0.008674602024257183\n",
      "Training epoch: 8, Batch_Num: 294/300, N_Loss: 0.00432633375748992\n",
      "Training epoch: 8, Batch_Num: 295/300, N_Loss: 0.0016764337196946144\n",
      "Training epoch: 8, Batch_Num: 296/300, N_Loss: 0.0016533584566786885\n",
      "Training epoch: 8, Batch_Num: 297/300, N_Loss: 0.025429287925362587\n",
      "Training epoch: 8, Batch_Num: 298/300, N_Loss: 0.012429023161530495\n",
      "Training epoch: 8, Batch_Num: 299/300, N_Loss: 0.0026874840259552\n",
      "Training epoch: 8, Batch_Num: 300/300, N_Loss: 0.00791455339640379\n",
      "Training epoch: 9, Batch_Num: 1/300, N_Loss: 0.005443371366709471\n",
      "Training epoch: 9, Batch_Num: 2/300, N_Loss: 0.002194085856899619\n",
      "Training epoch: 9, Batch_Num: 3/300, N_Loss: 0.005560529418289661\n",
      "Training epoch: 9, Batch_Num: 4/300, N_Loss: 0.007174822501838207\n",
      "Training epoch: 9, Batch_Num: 5/300, N_Loss: 0.0033463703002780676\n",
      "Training epoch: 9, Batch_Num: 6/300, N_Loss: 0.005871870089322329\n",
      "Training epoch: 9, Batch_Num: 7/300, N_Loss: 0.0010944630485028028\n",
      "Training epoch: 9, Batch_Num: 8/300, N_Loss: 0.009472387842833996\n",
      "Training epoch: 9, Batch_Num: 9/300, N_Loss: 0.0073729925788939\n",
      "Training epoch: 9, Batch_Num: 10/300, N_Loss: 0.0028667612932622433\n",
      "Training epoch: 9, Batch_Num: 11/300, N_Loss: 0.008502192795276642\n",
      "Training epoch: 9, Batch_Num: 12/300, N_Loss: 0.001848719664849341\n",
      "Training epoch: 9, Batch_Num: 13/300, N_Loss: 0.0052290321327745914\n",
      "Training epoch: 9, Batch_Num: 14/300, N_Loss: 0.0056583634577691555\n",
      "Training epoch: 9, Batch_Num: 15/300, N_Loss: 0.002617394318804145\n",
      "Training epoch: 9, Batch_Num: 16/300, N_Loss: 0.0019126880215480924\n",
      "Training epoch: 9, Batch_Num: 17/300, N_Loss: 0.004833886865526438\n",
      "Training epoch: 9, Batch_Num: 18/300, N_Loss: 0.01965179853141308\n",
      "Training epoch: 9, Batch_Num: 19/300, N_Loss: 0.00561014749109745\n",
      "Training epoch: 9, Batch_Num: 20/300, N_Loss: 0.006904733367264271\n",
      "Training epoch: 9, Batch_Num: 21/300, N_Loss: 0.00782619509845972\n",
      "Training epoch: 9, Batch_Num: 22/300, N_Loss: 0.012690187431871891\n",
      "Training epoch: 9, Batch_Num: 23/300, N_Loss: 0.005656098481267691\n",
      "Training epoch: 9, Batch_Num: 24/300, N_Loss: 0.010336051695048809\n",
      "Training epoch: 9, Batch_Num: 25/300, N_Loss: 0.013785896822810173\n",
      "Training epoch: 9, Batch_Num: 26/300, N_Loss: 0.008252846077084541\n",
      "Training epoch: 9, Batch_Num: 27/300, N_Loss: 0.007355518639087677\n",
      "Training epoch: 9, Batch_Num: 28/300, N_Loss: 0.0025365466717630625\n",
      "Training epoch: 9, Batch_Num: 29/300, N_Loss: 0.013995977118611336\n",
      "Training epoch: 9, Batch_Num: 30/300, N_Loss: 0.006196026224642992\n",
      "Training epoch: 9, Batch_Num: 31/300, N_Loss: 0.014506419189274311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 9, Batch_Num: 32/300, N_Loss: 0.021170413121581078\n",
      "Training epoch: 9, Batch_Num: 33/300, N_Loss: 0.15429003536701202\n",
      "Training epoch: 9, Batch_Num: 34/300, N_Loss: 0.004217858426272869\n",
      "Training epoch: 9, Batch_Num: 35/300, N_Loss: 0.004386755637824535\n",
      "Training epoch: 9, Batch_Num: 36/300, N_Loss: 0.013722556643188\n",
      "Training epoch: 9, Batch_Num: 37/300, N_Loss: 0.002168006496503949\n",
      "Training epoch: 9, Batch_Num: 38/300, N_Loss: 0.01665910892188549\n",
      "Training epoch: 9, Batch_Num: 39/300, N_Loss: 0.006705187261104584\n",
      "Training epoch: 9, Batch_Num: 40/300, N_Loss: 0.0031592261511832476\n",
      "Training epoch: 9, Batch_Num: 41/300, N_Loss: 0.02733103185892105\n",
      "Training epoch: 9, Batch_Num: 42/300, N_Loss: 0.0612206868827343\n",
      "Training epoch: 9, Batch_Num: 43/300, N_Loss: 0.031011156737804413\n",
      "Training epoch: 9, Batch_Num: 44/300, N_Loss: 0.030158789828419685\n",
      "Training epoch: 9, Batch_Num: 45/300, N_Loss: 0.0042776018381118774\n",
      "Training epoch: 9, Batch_Num: 46/300, N_Loss: 0.0020672641694545746\n",
      "Training epoch: 9, Batch_Num: 47/300, N_Loss: 0.004804431460797787\n",
      "Training epoch: 9, Batch_Num: 48/300, N_Loss: 0.0015467075863853097\n",
      "Training epoch: 9, Batch_Num: 49/300, N_Loss: 0.007614009082317352\n",
      "Training epoch: 9, Batch_Num: 50/300, N_Loss: 0.007555053569376469\n",
      "Training epoch: 9, Batch_Num: 51/300, N_Loss: 0.005362171214073896\n",
      "Training epoch: 9, Batch_Num: 52/300, N_Loss: 0.06661663204431534\n",
      "Training epoch: 9, Batch_Num: 53/300, N_Loss: 0.013410628773272038\n",
      "Training epoch: 9, Batch_Num: 54/300, N_Loss: 0.03245684877038002\n",
      "Training epoch: 9, Batch_Num: 55/300, N_Loss: 0.0036956847179681063\n",
      "Training epoch: 9, Batch_Num: 56/300, N_Loss: 0.01303100772202015\n",
      "Training epoch: 9, Batch_Num: 57/300, N_Loss: 0.034232258796691895\n",
      "Training epoch: 9, Batch_Num: 58/300, N_Loss: 0.001703008427284658\n",
      "Training epoch: 9, Batch_Num: 59/300, N_Loss: 0.0026465444825589657\n",
      "Training epoch: 9, Batch_Num: 60/300, N_Loss: 0.00833056215196848\n",
      "Training epoch: 9, Batch_Num: 61/300, N_Loss: 0.001009868341498077\n",
      "Training epoch: 9, Batch_Num: 62/300, N_Loss: 0.017398366704583168\n",
      "Training epoch: 9, Batch_Num: 63/300, N_Loss: 0.013348117470741272\n",
      "Training epoch: 9, Batch_Num: 64/300, N_Loss: 0.020500734448432922\n",
      "Training epoch: 9, Batch_Num: 65/300, N_Loss: 0.006465194281190634\n",
      "Training epoch: 9, Batch_Num: 66/300, N_Loss: 0.017696058377623558\n",
      "Training epoch: 9, Batch_Num: 67/300, N_Loss: 0.005506941583007574\n",
      "Training epoch: 9, Batch_Num: 68/300, N_Loss: 0.01542595587670803\n",
      "Training epoch: 9, Batch_Num: 69/300, N_Loss: 0.0040191709995269775\n",
      "Training epoch: 9, Batch_Num: 70/300, N_Loss: 0.00434857839718461\n",
      "Training epoch: 9, Batch_Num: 71/300, N_Loss: 0.012455488555133343\n",
      "Training epoch: 9, Batch_Num: 72/300, N_Loss: 0.07321202009916306\n",
      "Training epoch: 9, Batch_Num: 73/300, N_Loss: 0.0007386241341009736\n",
      "Training epoch: 9, Batch_Num: 74/300, N_Loss: 0.0028985256794840097\n",
      "Training epoch: 9, Batch_Num: 75/300, N_Loss: 0.0017287012888118625\n",
      "Training epoch: 9, Batch_Num: 76/300, N_Loss: 0.005538697354495525\n",
      "Training epoch: 9, Batch_Num: 77/300, N_Loss: 0.008264169096946716\n",
      "Training epoch: 9, Batch_Num: 78/300, N_Loss: 0.017106911167502403\n",
      "Training epoch: 9, Batch_Num: 79/300, N_Loss: 0.0042788987047970295\n",
      "Training epoch: 9, Batch_Num: 80/300, N_Loss: 0.004934095777571201\n",
      "Training epoch: 9, Batch_Num: 81/300, N_Loss: 0.01396625954657793\n",
      "Training epoch: 9, Batch_Num: 82/300, N_Loss: 0.0053007458336651325\n",
      "Training epoch: 9, Batch_Num: 83/300, N_Loss: 0.02100365050137043\n",
      "Training epoch: 9, Batch_Num: 84/300, N_Loss: 0.005500440951436758\n",
      "Training epoch: 9, Batch_Num: 85/300, N_Loss: 0.010214299894869328\n",
      "Training epoch: 9, Batch_Num: 86/300, N_Loss: 0.03409140929579735\n",
      "Training epoch: 9, Batch_Num: 87/300, N_Loss: 0.007906129583716393\n",
      "Training epoch: 9, Batch_Num: 88/300, N_Loss: 0.004491196945309639\n",
      "Training epoch: 9, Batch_Num: 89/300, N_Loss: 0.0116170234978199\n",
      "Training epoch: 9, Batch_Num: 90/300, N_Loss: 0.011943940073251724\n",
      "Training epoch: 9, Batch_Num: 91/300, N_Loss: 0.008801222778856754\n",
      "Training epoch: 9, Batch_Num: 92/300, N_Loss: 0.0026644309982657433\n",
      "Training epoch: 9, Batch_Num: 93/300, N_Loss: 0.03119076043367386\n",
      "Training epoch: 9, Batch_Num: 94/300, N_Loss: 0.0020311707630753517\n",
      "Training epoch: 9, Batch_Num: 95/300, N_Loss: 0.0011773428414016962\n",
      "Training epoch: 9, Batch_Num: 96/300, N_Loss: 0.011510374955832958\n",
      "Training epoch: 9, Batch_Num: 97/300, N_Loss: 0.008977486751973629\n",
      "Training epoch: 9, Batch_Num: 98/300, N_Loss: 0.013528574258089066\n",
      "Training epoch: 9, Batch_Num: 99/300, N_Loss: 0.04099913313984871\n",
      "Training epoch: 9, Batch_Num: 100/300, N_Loss: 0.011767801828682423\n",
      "Training epoch: 9, Batch_Num: 101/300, N_Loss: 0.004420329816639423\n",
      "Training epoch: 9, Batch_Num: 102/300, N_Loss: 0.005909077823162079\n",
      "Training epoch: 9, Batch_Num: 103/300, N_Loss: 0.004203164018690586\n",
      "Training epoch: 9, Batch_Num: 104/300, N_Loss: 0.006761706434190273\n",
      "Training epoch: 9, Batch_Num: 105/300, N_Loss: 0.05492693558335304\n",
      "Training epoch: 9, Batch_Num: 106/300, N_Loss: 0.00914827175438404\n",
      "Training epoch: 9, Batch_Num: 107/300, N_Loss: 0.003094820771366358\n",
      "Training epoch: 9, Batch_Num: 108/300, N_Loss: 0.0025791435036808252\n",
      "Training epoch: 9, Batch_Num: 109/300, N_Loss: 0.011258586309850216\n",
      "Training epoch: 9, Batch_Num: 110/300, N_Loss: 0.0051690055988729\n",
      "Training epoch: 9, Batch_Num: 111/300, N_Loss: 0.014953086152672768\n",
      "Training epoch: 9, Batch_Num: 112/300, N_Loss: 0.010939010418951511\n",
      "Training epoch: 9, Batch_Num: 113/300, N_Loss: 0.008680296130478382\n",
      "Training epoch: 9, Batch_Num: 114/300, N_Loss: 0.09181403368711472\n",
      "Training epoch: 9, Batch_Num: 115/300, N_Loss: 0.004240121226757765\n",
      "Training epoch: 9, Batch_Num: 116/300, N_Loss: 0.016708357259631157\n",
      "Training epoch: 9, Batch_Num: 117/300, N_Loss: 0.01977086067199707\n",
      "Training epoch: 9, Batch_Num: 118/300, N_Loss: 0.0024247420951724052\n",
      "Training epoch: 9, Batch_Num: 119/300, N_Loss: 0.0020529069006443024\n",
      "Training epoch: 9, Batch_Num: 120/300, N_Loss: 0.0007418945897370577\n",
      "Training epoch: 9, Batch_Num: 121/300, N_Loss: 0.002706830855458975\n",
      "Training epoch: 9, Batch_Num: 122/300, N_Loss: 0.0039031528867781162\n",
      "Training epoch: 9, Batch_Num: 123/300, N_Loss: 0.012288314290344715\n",
      "Training epoch: 9, Batch_Num: 124/300, N_Loss: 0.0039135124534368515\n",
      "Training epoch: 9, Batch_Num: 125/300, N_Loss: 0.011641551740467548\n",
      "Training epoch: 9, Batch_Num: 126/300, N_Loss: 0.034999068826436996\n",
      "Training epoch: 9, Batch_Num: 127/300, N_Loss: 0.0034547396935522556\n",
      "Training epoch: 9, Batch_Num: 128/300, N_Loss: 0.006864455528557301\n",
      "Training epoch: 9, Batch_Num: 129/300, N_Loss: 0.00536695122718811\n",
      "Training epoch: 9, Batch_Num: 130/300, N_Loss: 0.0027763289399445057\n",
      "Training epoch: 9, Batch_Num: 131/300, N_Loss: 0.0007353805121965706\n",
      "Training epoch: 9, Batch_Num: 132/300, N_Loss: 0.001372075523249805\n",
      "Training epoch: 9, Batch_Num: 133/300, N_Loss: 0.003982716705650091\n",
      "Training epoch: 9, Batch_Num: 134/300, N_Loss: 0.0038843145594000816\n",
      "Training epoch: 9, Batch_Num: 135/300, N_Loss: 0.0026616945397108793\n",
      "Training epoch: 9, Batch_Num: 136/300, N_Loss: 0.0088061997666955\n",
      "Training epoch: 9, Batch_Num: 137/300, N_Loss: 0.0019556405022740364\n",
      "Training epoch: 9, Batch_Num: 138/300, N_Loss: 0.001814311253838241\n",
      "Training epoch: 9, Batch_Num: 139/300, N_Loss: 0.002730283886194229\n",
      "Training epoch: 9, Batch_Num: 140/300, N_Loss: 0.004037715494632721\n",
      "Training epoch: 9, Batch_Num: 141/300, N_Loss: 0.002499909605830908\n",
      "Training epoch: 9, Batch_Num: 142/300, N_Loss: 0.0034033905249089003\n",
      "Training epoch: 9, Batch_Num: 143/300, N_Loss: 0.10579977184534073\n",
      "Training epoch: 9, Batch_Num: 144/300, N_Loss: 0.0029117183294147253\n",
      "Training epoch: 9, Batch_Num: 145/300, N_Loss: 0.0011762755457311869\n",
      "Training epoch: 9, Batch_Num: 146/300, N_Loss: 0.007319074589759111\n",
      "Training epoch: 9, Batch_Num: 147/300, N_Loss: 0.014703558757901192\n",
      "Training epoch: 9, Batch_Num: 148/300, N_Loss: 0.007084812968969345\n",
      "Training epoch: 9, Batch_Num: 149/300, N_Loss: 0.0014152934309095144\n",
      "Training epoch: 9, Batch_Num: 150/300, N_Loss: 0.0009065836784429848\n",
      "Training epoch: 9, Batch_Num: 151/300, N_Loss: 0.01725928857922554\n",
      "Training epoch: 9, Batch_Num: 152/300, N_Loss: 0.0039953794330358505\n",
      "Training epoch: 9, Batch_Num: 153/300, N_Loss: 0.04577811807394028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 9, Batch_Num: 154/300, N_Loss: 0.008761117234826088\n",
      "Training epoch: 9, Batch_Num: 155/300, N_Loss: 0.03328846022486687\n",
      "Training epoch: 9, Batch_Num: 156/300, N_Loss: 0.006598110776394606\n",
      "Training epoch: 9, Batch_Num: 157/300, N_Loss: 0.002000433625653386\n",
      "Training epoch: 9, Batch_Num: 158/300, N_Loss: 0.0017209042562171817\n",
      "Training epoch: 9, Batch_Num: 159/300, N_Loss: 0.005284206476062536\n",
      "Training epoch: 9, Batch_Num: 160/300, N_Loss: 0.0068579306825995445\n",
      "Training epoch: 9, Batch_Num: 161/300, N_Loss: 0.001954097533598542\n",
      "Training epoch: 9, Batch_Num: 162/300, N_Loss: 0.0061994255520403385\n",
      "Training epoch: 9, Batch_Num: 163/300, N_Loss: 0.007817323319613934\n",
      "Training epoch: 9, Batch_Num: 164/300, N_Loss: 0.15888959169387817\n",
      "Training epoch: 9, Batch_Num: 165/300, N_Loss: 0.005856848321855068\n",
      "Training epoch: 9, Batch_Num: 166/300, N_Loss: 0.014547928236424923\n",
      "Training epoch: 9, Batch_Num: 167/300, N_Loss: 0.007281554397195578\n",
      "Training epoch: 9, Batch_Num: 168/300, N_Loss: 0.003445341484621167\n",
      "Training epoch: 9, Batch_Num: 169/300, N_Loss: 0.02650274895131588\n",
      "Training epoch: 9, Batch_Num: 170/300, N_Loss: 0.17076747119426727\n",
      "Training epoch: 9, Batch_Num: 171/300, N_Loss: 0.011656943708658218\n",
      "Training epoch: 9, Batch_Num: 172/300, N_Loss: 0.0014767918037250638\n",
      "Training epoch: 9, Batch_Num: 173/300, N_Loss: 0.0042283800430595875\n",
      "Training epoch: 9, Batch_Num: 174/300, N_Loss: 0.007490719202905893\n",
      "Training epoch: 9, Batch_Num: 175/300, N_Loss: 0.010849270969629288\n",
      "Training epoch: 9, Batch_Num: 176/300, N_Loss: 0.01205724198371172\n",
      "Training epoch: 9, Batch_Num: 177/300, N_Loss: 0.0026479517109692097\n",
      "Training epoch: 9, Batch_Num: 178/300, N_Loss: 0.004732816480100155\n",
      "Training epoch: 9, Batch_Num: 179/300, N_Loss: 0.026368701830506325\n",
      "Training epoch: 9, Batch_Num: 180/300, N_Loss: 0.0022334216628223658\n",
      "Training epoch: 9, Batch_Num: 181/300, N_Loss: 0.010078744031488895\n",
      "Training epoch: 9, Batch_Num: 182/300, N_Loss: 0.012779486365616322\n",
      "Training epoch: 9, Batch_Num: 183/300, N_Loss: 0.0035100814420729876\n",
      "Training epoch: 9, Batch_Num: 184/300, N_Loss: 0.003644601907581091\n",
      "Training epoch: 9, Batch_Num: 185/300, N_Loss: 0.0034707647282630205\n",
      "Training epoch: 9, Batch_Num: 186/300, N_Loss: 0.008024509064853191\n",
      "Training epoch: 9, Batch_Num: 187/300, N_Loss: 0.03790184110403061\n",
      "Training epoch: 9, Batch_Num: 188/300, N_Loss: 0.0011587796034291387\n",
      "Training epoch: 9, Batch_Num: 189/300, N_Loss: 0.020687241107225418\n",
      "Training epoch: 9, Batch_Num: 190/300, N_Loss: 0.060252122581005096\n",
      "Training epoch: 9, Batch_Num: 191/300, N_Loss: 0.006367461755871773\n",
      "Training epoch: 9, Batch_Num: 192/300, N_Loss: 0.002071158494800329\n",
      "Training epoch: 9, Batch_Num: 193/300, N_Loss: 0.009841958992183208\n",
      "Training epoch: 9, Batch_Num: 194/300, N_Loss: 0.01594630628824234\n",
      "Training epoch: 9, Batch_Num: 195/300, N_Loss: 0.016087977215647697\n",
      "Training epoch: 9, Batch_Num: 196/300, N_Loss: 0.0014284283388406038\n",
      "Training epoch: 9, Batch_Num: 197/300, N_Loss: 0.005704245064407587\n",
      "Training epoch: 9, Batch_Num: 198/300, N_Loss: 0.003258358919993043\n",
      "Training epoch: 9, Batch_Num: 199/300, N_Loss: 0.010108920745551586\n",
      "Training epoch: 9, Batch_Num: 200/300, N_Loss: 0.0036455760709941387\n",
      "Training epoch: 9, Batch_Num: 201/300, N_Loss: 0.008618696592748165\n",
      "Training epoch: 9, Batch_Num: 202/300, N_Loss: 0.0020044762641191483\n",
      "Training epoch: 9, Batch_Num: 203/300, N_Loss: 0.03535068780183792\n",
      "Training epoch: 9, Batch_Num: 204/300, N_Loss: 0.027403926476836205\n",
      "Training epoch: 9, Batch_Num: 205/300, N_Loss: 0.005391027312725782\n",
      "Training epoch: 9, Batch_Num: 206/300, N_Loss: 0.001555896713398397\n",
      "Training epoch: 9, Batch_Num: 207/300, N_Loss: 0.002049001632258296\n",
      "Training epoch: 9, Batch_Num: 208/300, N_Loss: 0.07375649362802505\n",
      "Training epoch: 9, Batch_Num: 209/300, N_Loss: 0.008504700846970081\n",
      "Training epoch: 9, Batch_Num: 210/300, N_Loss: 0.00423932820558548\n",
      "Training epoch: 9, Batch_Num: 211/300, N_Loss: 0.0015498476568609476\n",
      "Training epoch: 9, Batch_Num: 212/300, N_Loss: 0.012284263037145138\n",
      "Training epoch: 9, Batch_Num: 213/300, N_Loss: 0.05189142003655434\n",
      "Training epoch: 9, Batch_Num: 214/300, N_Loss: 0.0058571272529661655\n",
      "Training epoch: 9, Batch_Num: 215/300, N_Loss: 0.006839965470135212\n",
      "Training epoch: 9, Batch_Num: 216/300, N_Loss: 0.0014371746219694614\n",
      "Training epoch: 9, Batch_Num: 217/300, N_Loss: 0.0016475204611197114\n",
      "Training epoch: 9, Batch_Num: 218/300, N_Loss: 0.02248668298125267\n",
      "Training epoch: 9, Batch_Num: 219/300, N_Loss: 0.002516106003895402\n",
      "Training epoch: 9, Batch_Num: 220/300, N_Loss: 0.0038012920413166285\n",
      "Training epoch: 9, Batch_Num: 221/300, N_Loss: 0.0032020150683820248\n",
      "Training epoch: 9, Batch_Num: 222/300, N_Loss: 0.06436534970998764\n",
      "Training epoch: 9, Batch_Num: 223/300, N_Loss: 0.018106045201420784\n",
      "Training epoch: 9, Batch_Num: 224/300, N_Loss: 0.0061929598450660706\n",
      "Training epoch: 9, Batch_Num: 225/300, N_Loss: 0.0032054316252470016\n",
      "Training epoch: 9, Batch_Num: 226/300, N_Loss: 0.0015175066655501723\n",
      "Training epoch: 9, Batch_Num: 227/300, N_Loss: 0.020820150151848793\n",
      "Training epoch: 9, Batch_Num: 228/300, N_Loss: 0.012421396560966969\n",
      "Training epoch: 9, Batch_Num: 229/300, N_Loss: 0.0020910589955747128\n",
      "Training epoch: 9, Batch_Num: 230/300, N_Loss: 0.019892103970050812\n",
      "Training epoch: 9, Batch_Num: 231/300, N_Loss: 0.009854797273874283\n",
      "Training epoch: 9, Batch_Num: 232/300, N_Loss: 0.10730262100696564\n",
      "Training epoch: 9, Batch_Num: 233/300, N_Loss: 0.013786839321255684\n",
      "Training epoch: 9, Batch_Num: 234/300, N_Loss: 0.017862016335129738\n",
      "Training epoch: 9, Batch_Num: 235/300, N_Loss: 0.009121116250753403\n",
      "Training epoch: 9, Batch_Num: 236/300, N_Loss: 0.00195541069842875\n",
      "Training epoch: 9, Batch_Num: 237/300, N_Loss: 0.039119377732276917\n",
      "Training epoch: 9, Batch_Num: 238/300, N_Loss: 0.0016497336328029633\n",
      "Training epoch: 9, Batch_Num: 239/300, N_Loss: 0.00144110934343189\n",
      "Training epoch: 9, Batch_Num: 240/300, N_Loss: 0.005280289798974991\n",
      "Training epoch: 9, Batch_Num: 241/300, N_Loss: 0.07782407850027084\n",
      "Training epoch: 9, Batch_Num: 242/300, N_Loss: 0.004655600991100073\n",
      "Training epoch: 9, Batch_Num: 243/300, N_Loss: 0.007757165003567934\n",
      "Training epoch: 9, Batch_Num: 244/300, N_Loss: 0.0013047069078311324\n",
      "Training epoch: 9, Batch_Num: 245/300, N_Loss: 0.014897284097969532\n",
      "Training epoch: 9, Batch_Num: 246/300, N_Loss: 0.0035013759043067694\n",
      "Training epoch: 9, Batch_Num: 247/300, N_Loss: 0.013007077388465405\n",
      "Training epoch: 9, Batch_Num: 248/300, N_Loss: 0.014122013933956623\n",
      "Training epoch: 9, Batch_Num: 249/300, N_Loss: 0.022850222885608673\n",
      "Training epoch: 9, Batch_Num: 250/300, N_Loss: 0.002604249631986022\n",
      "Training epoch: 9, Batch_Num: 251/300, N_Loss: 0.03626319020986557\n",
      "Training epoch: 9, Batch_Num: 252/300, N_Loss: 0.01382584497332573\n",
      "Training epoch: 9, Batch_Num: 253/300, N_Loss: 0.006356643512845039\n",
      "Training epoch: 9, Batch_Num: 254/300, N_Loss: 0.015235187485814095\n",
      "Training epoch: 9, Batch_Num: 255/300, N_Loss: 0.0023226826451718807\n",
      "Training epoch: 9, Batch_Num: 256/300, N_Loss: 0.003152180463075638\n",
      "Training epoch: 9, Batch_Num: 257/300, N_Loss: 0.007899301126599312\n",
      "Training epoch: 9, Batch_Num: 258/300, N_Loss: 0.08812365680932999\n",
      "Training epoch: 9, Batch_Num: 259/300, N_Loss: 0.006250680424273014\n",
      "Training epoch: 9, Batch_Num: 260/300, N_Loss: 0.008322942070662975\n",
      "Training epoch: 9, Batch_Num: 261/300, N_Loss: 0.002232883358374238\n",
      "Training epoch: 9, Batch_Num: 262/300, N_Loss: 0.0022474313154816628\n",
      "Training epoch: 9, Batch_Num: 263/300, N_Loss: 0.008066500537097454\n",
      "Training epoch: 9, Batch_Num: 264/300, N_Loss: 0.004803761839866638\n",
      "Training epoch: 9, Batch_Num: 265/300, N_Loss: 0.0056163240224123\n",
      "Training epoch: 9, Batch_Num: 266/300, N_Loss: 0.03998616337776184\n",
      "Training epoch: 9, Batch_Num: 267/300, N_Loss: 0.006204855628311634\n",
      "Training epoch: 9, Batch_Num: 268/300, N_Loss: 0.0029356747400015593\n",
      "Training epoch: 9, Batch_Num: 269/300, N_Loss: 0.06214931607246399\n",
      "Training epoch: 9, Batch_Num: 270/300, N_Loss: 0.005911619868129492\n",
      "Training epoch: 9, Batch_Num: 271/300, N_Loss: 0.013890903443098068\n",
      "Training epoch: 9, Batch_Num: 272/300, N_Loss: 0.0042288415133953094\n",
      "Training epoch: 9, Batch_Num: 273/300, N_Loss: 0.018566079437732697\n",
      "Training epoch: 9, Batch_Num: 274/300, N_Loss: 0.005227513611316681\n",
      "Training epoch: 9, Batch_Num: 275/300, N_Loss: 0.004469756502658129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 9, Batch_Num: 276/300, N_Loss: 0.001663168310187757\n",
      "Training epoch: 9, Batch_Num: 277/300, N_Loss: 0.011832408607006073\n",
      "Training epoch: 9, Batch_Num: 278/300, N_Loss: 0.03828295320272446\n",
      "Training epoch: 9, Batch_Num: 279/300, N_Loss: 0.002622415544465184\n",
      "Training epoch: 9, Batch_Num: 280/300, N_Loss: 0.004725074861198664\n",
      "Training epoch: 9, Batch_Num: 281/300, N_Loss: 0.02391239069402218\n",
      "Training epoch: 9, Batch_Num: 282/300, N_Loss: 0.0030705241952091455\n",
      "Training epoch: 9, Batch_Num: 283/300, N_Loss: 0.007833058945834637\n",
      "Training epoch: 9, Batch_Num: 284/300, N_Loss: 0.0013192167971283197\n",
      "Training epoch: 9, Batch_Num: 285/300, N_Loss: 0.017305172979831696\n",
      "Training epoch: 9, Batch_Num: 286/300, N_Loss: 0.014025083743035793\n",
      "Training epoch: 9, Batch_Num: 287/300, N_Loss: 0.008709823712706566\n",
      "Training epoch: 9, Batch_Num: 288/300, N_Loss: 0.011650084517896175\n",
      "Training epoch: 9, Batch_Num: 289/300, N_Loss: 0.002316212747246027\n",
      "Training epoch: 9, Batch_Num: 290/300, N_Loss: 0.02093384973704815\n",
      "Training epoch: 9, Batch_Num: 291/300, N_Loss: 0.014473210088908672\n",
      "Training epoch: 9, Batch_Num: 292/300, N_Loss: 0.017362212762236595\n",
      "Training epoch: 9, Batch_Num: 293/300, N_Loss: 0.029438167810440063\n",
      "Training epoch: 9, Batch_Num: 294/300, N_Loss: 0.004853131715208292\n",
      "Training epoch: 9, Batch_Num: 295/300, N_Loss: 0.02401847019791603\n",
      "Training epoch: 9, Batch_Num: 296/300, N_Loss: 0.0806136429309845\n",
      "Training epoch: 9, Batch_Num: 297/300, N_Loss: 0.002285800874233246\n",
      "Training epoch: 9, Batch_Num: 298/300, N_Loss: 0.0046058702282607555\n",
      "Training epoch: 9, Batch_Num: 299/300, N_Loss: 0.01707119308412075\n",
      "Training epoch: 9, Batch_Num: 300/300, N_Loss: 0.018115418031811714\n",
      "Training epoch: 10, Batch_Num: 1/300, N_Loss: 0.0074524772353470325\n",
      "Training epoch: 10, Batch_Num: 2/300, N_Loss: 0.007304493337869644\n",
      "Training epoch: 10, Batch_Num: 3/300, N_Loss: 0.007021766621619463\n",
      "Training epoch: 10, Batch_Num: 4/300, N_Loss: 0.0014429328730329871\n",
      "Training epoch: 10, Batch_Num: 5/300, N_Loss: 0.014724349603056908\n",
      "Training epoch: 10, Batch_Num: 6/300, N_Loss: 0.005282933358103037\n",
      "Training epoch: 10, Batch_Num: 7/300, N_Loss: 0.0034241974353790283\n",
      "Training epoch: 10, Batch_Num: 8/300, N_Loss: 0.001641711569391191\n",
      "Training epoch: 10, Batch_Num: 9/300, N_Loss: 0.0030455435626208782\n",
      "Training epoch: 10, Batch_Num: 10/300, N_Loss: 0.00879611261188984\n",
      "Training epoch: 10, Batch_Num: 11/300, N_Loss: 0.006318364758044481\n",
      "Training epoch: 10, Batch_Num: 12/300, N_Loss: 0.004694903735071421\n",
      "Training epoch: 10, Batch_Num: 13/300, N_Loss: 0.001563024939969182\n",
      "Training epoch: 10, Batch_Num: 14/300, N_Loss: 0.0013882867060601711\n",
      "Training epoch: 10, Batch_Num: 15/300, N_Loss: 0.016226036474108696\n",
      "Training epoch: 10, Batch_Num: 16/300, N_Loss: 0.003007979830726981\n",
      "Training epoch: 10, Batch_Num: 17/300, N_Loss: 0.011055303737521172\n",
      "Training epoch: 10, Batch_Num: 18/300, N_Loss: 0.002752381144091487\n",
      "Training epoch: 10, Batch_Num: 19/300, N_Loss: 0.005663026124238968\n",
      "Training epoch: 10, Batch_Num: 20/300, N_Loss: 0.11468791961669922\n",
      "Training epoch: 10, Batch_Num: 21/300, N_Loss: 0.0006932622054591775\n",
      "Training epoch: 10, Batch_Num: 22/300, N_Loss: 0.0041323499754071236\n",
      "Training epoch: 10, Batch_Num: 23/300, N_Loss: 0.012693455442786217\n",
      "Training epoch: 10, Batch_Num: 24/300, N_Loss: 0.04090334475040436\n",
      "Training epoch: 10, Batch_Num: 25/300, N_Loss: 0.03231767565011978\n",
      "Training epoch: 10, Batch_Num: 26/300, N_Loss: 0.008544765412807465\n",
      "Training epoch: 10, Batch_Num: 27/300, N_Loss: 0.0040806010365486145\n",
      "Training epoch: 10, Batch_Num: 28/300, N_Loss: 0.0029526185244321823\n",
      "Training epoch: 10, Batch_Num: 29/300, N_Loss: 0.010237658396363258\n",
      "Training epoch: 10, Batch_Num: 30/300, N_Loss: 0.0984557718038559\n",
      "Training epoch: 10, Batch_Num: 31/300, N_Loss: 0.0017632176168262959\n",
      "Training epoch: 10, Batch_Num: 32/300, N_Loss: 0.0017048815498128533\n",
      "Training epoch: 10, Batch_Num: 33/300, N_Loss: 0.0015871257055550814\n",
      "Training epoch: 10, Batch_Num: 34/300, N_Loss: 0.03634769096970558\n",
      "Training epoch: 10, Batch_Num: 35/300, N_Loss: 0.12993472814559937\n",
      "Training epoch: 10, Batch_Num: 36/300, N_Loss: 0.03344173729419708\n",
      "Training epoch: 10, Batch_Num: 37/300, N_Loss: 0.004519092850387096\n",
      "Training epoch: 10, Batch_Num: 38/300, N_Loss: 0.005942674353718758\n",
      "Training epoch: 10, Batch_Num: 39/300, N_Loss: 0.015939388424158096\n",
      "Training epoch: 10, Batch_Num: 40/300, N_Loss: 0.0017251349054276943\n",
      "Training epoch: 10, Batch_Num: 41/300, N_Loss: 0.004141570068895817\n",
      "Training epoch: 10, Batch_Num: 42/300, N_Loss: 0.02432381734251976\n",
      "Training epoch: 10, Batch_Num: 43/300, N_Loss: 0.0028569470159709454\n",
      "Training epoch: 10, Batch_Num: 44/300, N_Loss: 0.0029477246571332216\n",
      "Training epoch: 10, Batch_Num: 45/300, N_Loss: 0.0022020582109689713\n",
      "Training epoch: 10, Batch_Num: 46/300, N_Loss: 0.007619825191795826\n",
      "Training epoch: 10, Batch_Num: 47/300, N_Loss: 0.014177457429468632\n",
      "Training epoch: 10, Batch_Num: 48/300, N_Loss: 0.003680695313960314\n",
      "Training epoch: 10, Batch_Num: 49/300, N_Loss: 0.0008086820598691702\n",
      "Training epoch: 10, Batch_Num: 50/300, N_Loss: 0.0007585340645164251\n",
      "Training epoch: 10, Batch_Num: 51/300, N_Loss: 0.01389654353260994\n",
      "Training epoch: 10, Batch_Num: 52/300, N_Loss: 0.002541270572692156\n",
      "Training epoch: 10, Batch_Num: 53/300, N_Loss: 0.0010368257062509656\n",
      "Training epoch: 10, Batch_Num: 54/300, N_Loss: 0.006956947036087513\n",
      "Training epoch: 10, Batch_Num: 55/300, N_Loss: 0.030593590810894966\n",
      "Training epoch: 10, Batch_Num: 56/300, N_Loss: 0.003302587429061532\n",
      "Training epoch: 10, Batch_Num: 57/300, N_Loss: 0.008218055590987206\n",
      "Training epoch: 10, Batch_Num: 58/300, N_Loss: 0.003016863251104951\n",
      "Training epoch: 10, Batch_Num: 59/300, N_Loss: 0.0050940075889229774\n",
      "Training epoch: 10, Batch_Num: 60/300, N_Loss: 0.12184285372495651\n",
      "Training epoch: 10, Batch_Num: 61/300, N_Loss: 0.005429217126220465\n",
      "Training epoch: 10, Batch_Num: 62/300, N_Loss: 0.005417681764811277\n",
      "Training epoch: 10, Batch_Num: 63/300, N_Loss: 0.006616922561079264\n",
      "Training epoch: 10, Batch_Num: 64/300, N_Loss: 0.004625192377716303\n",
      "Training epoch: 10, Batch_Num: 65/300, N_Loss: 0.02420279197394848\n",
      "Training epoch: 10, Batch_Num: 66/300, N_Loss: 0.0058374106884002686\n",
      "Training epoch: 10, Batch_Num: 67/300, N_Loss: 0.06060703471302986\n",
      "Training epoch: 10, Batch_Num: 68/300, N_Loss: 0.005978866014629602\n",
      "Training epoch: 10, Batch_Num: 69/300, N_Loss: 0.0075639705173671246\n",
      "Training epoch: 10, Batch_Num: 70/300, N_Loss: 0.0004957034252583981\n",
      "Training epoch: 10, Batch_Num: 71/300, N_Loss: 0.016254793852567673\n",
      "Training epoch: 10, Batch_Num: 72/300, N_Loss: 0.018049662932753563\n",
      "Training epoch: 10, Batch_Num: 73/300, N_Loss: 0.03895833343267441\n",
      "Training epoch: 10, Batch_Num: 74/300, N_Loss: 0.005206126719713211\n",
      "Training epoch: 10, Batch_Num: 75/300, N_Loss: 0.008155864663422108\n",
      "Training epoch: 10, Batch_Num: 76/300, N_Loss: 0.021294383332133293\n",
      "Training epoch: 10, Batch_Num: 77/300, N_Loss: 0.025030195713043213\n",
      "Training epoch: 10, Batch_Num: 78/300, N_Loss: 0.005800734739750624\n",
      "Training epoch: 10, Batch_Num: 79/300, N_Loss: 0.004231415688991547\n",
      "Training epoch: 10, Batch_Num: 80/300, N_Loss: 0.005617950111627579\n",
      "Training epoch: 10, Batch_Num: 81/300, N_Loss: 0.01597006805241108\n",
      "Training epoch: 10, Batch_Num: 82/300, N_Loss: 0.0072450353763997555\n",
      "Training epoch: 10, Batch_Num: 83/300, N_Loss: 0.0035152474883943796\n",
      "Training epoch: 10, Batch_Num: 84/300, N_Loss: 0.003168553113937378\n",
      "Training epoch: 10, Batch_Num: 85/300, N_Loss: 0.019765865057706833\n",
      "Training epoch: 10, Batch_Num: 86/300, N_Loss: 0.015741460025310516\n",
      "Training epoch: 10, Batch_Num: 87/300, N_Loss: 0.02342480979859829\n",
      "Training epoch: 10, Batch_Num: 88/300, N_Loss: 0.004116231109946966\n",
      "Training epoch: 10, Batch_Num: 89/300, N_Loss: 0.004015786107629538\n",
      "Training epoch: 10, Batch_Num: 90/300, N_Loss: 0.005486675538122654\n",
      "Training epoch: 10, Batch_Num: 91/300, N_Loss: 0.008831270970404148\n",
      "Training epoch: 10, Batch_Num: 92/300, N_Loss: 0.003228855552151799\n",
      "Training epoch: 10, Batch_Num: 93/300, N_Loss: 0.011426658369600773\n",
      "Training epoch: 10, Batch_Num: 94/300, N_Loss: 0.023642512038350105\n",
      "Training epoch: 10, Batch_Num: 95/300, N_Loss: 0.009507443755865097\n",
      "Training epoch: 10, Batch_Num: 96/300, N_Loss: 0.010916800238192081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 10, Batch_Num: 97/300, N_Loss: 0.012839465402066708\n",
      "Training epoch: 10, Batch_Num: 98/300, N_Loss: 0.014457613229751587\n",
      "Training epoch: 10, Batch_Num: 99/300, N_Loss: 0.024677036330103874\n",
      "Training epoch: 10, Batch_Num: 100/300, N_Loss: 0.05791085958480835\n",
      "Training epoch: 10, Batch_Num: 101/300, N_Loss: 0.010448056273162365\n",
      "Training epoch: 10, Batch_Num: 102/300, N_Loss: 0.011962159536778927\n",
      "Training epoch: 10, Batch_Num: 103/300, N_Loss: 0.03652031719684601\n",
      "Training epoch: 10, Batch_Num: 104/300, N_Loss: 0.005754416808485985\n",
      "Training epoch: 10, Batch_Num: 105/300, N_Loss: 0.004741739481687546\n",
      "Training epoch: 10, Batch_Num: 106/300, N_Loss: 0.0018087899079546332\n",
      "Training epoch: 10, Batch_Num: 107/300, N_Loss: 0.009316042065620422\n",
      "Training epoch: 10, Batch_Num: 108/300, N_Loss: 0.005830989684909582\n",
      "Training epoch: 10, Batch_Num: 109/300, N_Loss: 0.02702775038778782\n",
      "Training epoch: 10, Batch_Num: 110/300, N_Loss: 0.0014961343258619308\n",
      "Training epoch: 10, Batch_Num: 111/300, N_Loss: 0.016813060268759727\n",
      "Training epoch: 10, Batch_Num: 112/300, N_Loss: 0.005637203808873892\n",
      "Training epoch: 10, Batch_Num: 113/300, N_Loss: 0.006439486052840948\n",
      "Training epoch: 10, Batch_Num: 114/300, N_Loss: 0.0017275736900046468\n",
      "Training epoch: 10, Batch_Num: 115/300, N_Loss: 0.017595775425434113\n",
      "Training epoch: 10, Batch_Num: 116/300, N_Loss: 0.005255866330116987\n",
      "Training epoch: 10, Batch_Num: 117/300, N_Loss: 0.0006221605581231415\n",
      "Training epoch: 10, Batch_Num: 118/300, N_Loss: 0.002103979466482997\n",
      "Training epoch: 10, Batch_Num: 119/300, N_Loss: 0.006343558430671692\n",
      "Training epoch: 10, Batch_Num: 120/300, N_Loss: 0.0029306099750101566\n",
      "Training epoch: 10, Batch_Num: 121/300, N_Loss: 0.009840758517384529\n",
      "Training epoch: 10, Batch_Num: 122/300, N_Loss: 0.0012281356612220407\n",
      "Training epoch: 10, Batch_Num: 123/300, N_Loss: 0.0048525091260671616\n",
      "Training epoch: 10, Batch_Num: 124/300, N_Loss: 0.0035689212381839752\n",
      "Training epoch: 10, Batch_Num: 125/300, N_Loss: 0.004406718537211418\n",
      "Training epoch: 10, Batch_Num: 126/300, N_Loss: 0.00478667626157403\n",
      "Training epoch: 10, Batch_Num: 127/300, N_Loss: 0.03299913927912712\n",
      "Training epoch: 10, Batch_Num: 128/300, N_Loss: 0.02227451093494892\n",
      "Training epoch: 10, Batch_Num: 129/300, N_Loss: 0.0017293884884566069\n",
      "Training epoch: 10, Batch_Num: 130/300, N_Loss: 0.02519032172858715\n",
      "Training epoch: 10, Batch_Num: 131/300, N_Loss: 0.0026560102123767138\n",
      "Training epoch: 10, Batch_Num: 132/300, N_Loss: 0.00459672324359417\n",
      "Training epoch: 10, Batch_Num: 133/300, N_Loss: 0.0027139121666550636\n",
      "Training epoch: 10, Batch_Num: 134/300, N_Loss: 0.001126545132137835\n",
      "Training epoch: 10, Batch_Num: 135/300, N_Loss: 0.002424420090392232\n",
      "Training epoch: 10, Batch_Num: 136/300, N_Loss: 0.0083736227825284\n",
      "Training epoch: 10, Batch_Num: 137/300, N_Loss: 0.014712316915392876\n",
      "Training epoch: 10, Batch_Num: 138/300, N_Loss: 0.050477806478738785\n",
      "Training epoch: 10, Batch_Num: 139/300, N_Loss: 0.01780892163515091\n",
      "Training epoch: 10, Batch_Num: 140/300, N_Loss: 0.09298989176750183\n",
      "Training epoch: 10, Batch_Num: 141/300, N_Loss: 0.03503461927175522\n",
      "Training epoch: 10, Batch_Num: 142/300, N_Loss: 0.00800945796072483\n",
      "Training epoch: 10, Batch_Num: 143/300, N_Loss: 0.0021920904982835054\n",
      "Training epoch: 10, Batch_Num: 144/300, N_Loss: 0.005513970740139484\n",
      "Training epoch: 10, Batch_Num: 145/300, N_Loss: 0.057943105697631836\n",
      "Training epoch: 10, Batch_Num: 146/300, N_Loss: 0.0176255963742733\n",
      "Training epoch: 10, Batch_Num: 147/300, N_Loss: 0.023255029693245888\n",
      "Training epoch: 10, Batch_Num: 148/300, N_Loss: 0.02549079991877079\n",
      "Training epoch: 10, Batch_Num: 149/300, N_Loss: 0.015305958688259125\n",
      "Training epoch: 10, Batch_Num: 150/300, N_Loss: 0.0033441619016230106\n",
      "Training epoch: 10, Batch_Num: 151/300, N_Loss: 0.002006598748266697\n",
      "Training epoch: 10, Batch_Num: 152/300, N_Loss: 0.009182372130453587\n",
      "Training epoch: 10, Batch_Num: 153/300, N_Loss: 0.011136341840028763\n",
      "Training epoch: 10, Batch_Num: 154/300, N_Loss: 0.0069153499789536\n",
      "Training epoch: 10, Batch_Num: 155/300, N_Loss: 0.0018847431056201458\n",
      "Training epoch: 10, Batch_Num: 156/300, N_Loss: 0.05539843067526817\n",
      "Training epoch: 10, Batch_Num: 157/300, N_Loss: 0.043950680643320084\n",
      "Training epoch: 10, Batch_Num: 158/300, N_Loss: 0.003221850609406829\n",
      "Training epoch: 10, Batch_Num: 159/300, N_Loss: 0.012193219736218452\n",
      "Training epoch: 10, Batch_Num: 160/300, N_Loss: 0.0069730463437736034\n",
      "Training epoch: 10, Batch_Num: 161/300, N_Loss: 0.0016623525880277157\n",
      "Training epoch: 10, Batch_Num: 162/300, N_Loss: 0.014882250688970089\n",
      "Training epoch: 10, Batch_Num: 163/300, N_Loss: 0.015689142048358917\n",
      "Training epoch: 10, Batch_Num: 164/300, N_Loss: 0.022019166499376297\n",
      "Training epoch: 10, Batch_Num: 165/300, N_Loss: 0.012653663754463196\n",
      "Training epoch: 10, Batch_Num: 166/300, N_Loss: 0.01228189468383789\n",
      "Training epoch: 10, Batch_Num: 167/300, N_Loss: 0.029961027204990387\n",
      "Training epoch: 10, Batch_Num: 168/300, N_Loss: 0.0031918021850287914\n",
      "Training epoch: 10, Batch_Num: 169/300, N_Loss: 0.026968112215399742\n",
      "Training epoch: 10, Batch_Num: 170/300, N_Loss: 0.0020773953292518854\n",
      "Training epoch: 10, Batch_Num: 171/300, N_Loss: 0.0017948736203834414\n",
      "Training epoch: 10, Batch_Num: 172/300, N_Loss: 0.006248108576983213\n",
      "Training epoch: 10, Batch_Num: 173/300, N_Loss: 0.016138503327965736\n",
      "Training epoch: 10, Batch_Num: 174/300, N_Loss: 0.0014517182717099786\n",
      "Training epoch: 10, Batch_Num: 175/300, N_Loss: 0.002998742740601301\n",
      "Training epoch: 10, Batch_Num: 176/300, N_Loss: 0.0008193922112695873\n",
      "Training epoch: 10, Batch_Num: 177/300, N_Loss: 0.005962260067462921\n",
      "Training epoch: 10, Batch_Num: 178/300, N_Loss: 0.010947700589895248\n",
      "Training epoch: 10, Batch_Num: 179/300, N_Loss: 0.00766238896176219\n",
      "Training epoch: 10, Batch_Num: 180/300, N_Loss: 0.00891848187893629\n",
      "Training epoch: 10, Batch_Num: 181/300, N_Loss: 0.008357461541891098\n",
      "Training epoch: 10, Batch_Num: 182/300, N_Loss: 0.03020634315907955\n",
      "Training epoch: 10, Batch_Num: 183/300, N_Loss: 0.002919811522588134\n",
      "Training epoch: 10, Batch_Num: 184/300, N_Loss: 0.0023799927439540625\n",
      "Training epoch: 10, Batch_Num: 185/300, N_Loss: 0.009807520546019077\n",
      "Training epoch: 10, Batch_Num: 186/300, N_Loss: 0.05714188516139984\n",
      "Training epoch: 10, Batch_Num: 187/300, N_Loss: 0.007422630675137043\n",
      "Training epoch: 10, Batch_Num: 188/300, N_Loss: 0.001285158097743988\n",
      "Training epoch: 10, Batch_Num: 189/300, N_Loss: 0.007629751227796078\n",
      "Training epoch: 10, Batch_Num: 190/300, N_Loss: 0.002612933050841093\n",
      "Training epoch: 10, Batch_Num: 191/300, N_Loss: 0.006464581470936537\n",
      "Training epoch: 10, Batch_Num: 192/300, N_Loss: 0.011608492583036423\n",
      "Training epoch: 10, Batch_Num: 193/300, N_Loss: 0.004896250553429127\n",
      "Training epoch: 10, Batch_Num: 194/300, N_Loss: 0.005152588244527578\n",
      "Training epoch: 10, Batch_Num: 195/300, N_Loss: 0.01982935704290867\n",
      "Training epoch: 10, Batch_Num: 196/300, N_Loss: 0.020112860947847366\n",
      "Training epoch: 10, Batch_Num: 197/300, N_Loss: 0.0049399384297430515\n",
      "Training epoch: 10, Batch_Num: 198/300, N_Loss: 0.010112493298947811\n",
      "Training epoch: 10, Batch_Num: 199/300, N_Loss: 0.016988668590784073\n",
      "Training epoch: 10, Batch_Num: 200/300, N_Loss: 0.015764039009809494\n",
      "Training epoch: 10, Batch_Num: 201/300, N_Loss: 0.006221709307283163\n",
      "Training epoch: 10, Batch_Num: 202/300, N_Loss: 0.010900797322392464\n",
      "Training epoch: 10, Batch_Num: 203/300, N_Loss: 0.006010439246892929\n",
      "Training epoch: 10, Batch_Num: 204/300, N_Loss: 0.0030525813344866037\n",
      "Training epoch: 10, Batch_Num: 205/300, N_Loss: 0.12813256680965424\n",
      "Training epoch: 10, Batch_Num: 206/300, N_Loss: 0.001020165509544313\n",
      "Training epoch: 10, Batch_Num: 207/300, N_Loss: 0.003207916161045432\n",
      "Training epoch: 10, Batch_Num: 208/300, N_Loss: 0.0373668372631073\n",
      "Training epoch: 10, Batch_Num: 209/300, N_Loss: 0.006794298067688942\n",
      "Training epoch: 10, Batch_Num: 210/300, N_Loss: 0.021384326741099358\n",
      "Training epoch: 10, Batch_Num: 211/300, N_Loss: 0.002651575719937682\n",
      "Training epoch: 10, Batch_Num: 212/300, N_Loss: 0.007411298807710409\n",
      "Training epoch: 10, Batch_Num: 213/300, N_Loss: 0.006495521403849125\n",
      "Training epoch: 10, Batch_Num: 214/300, N_Loss: 0.010349626652896404\n",
      "Training epoch: 10, Batch_Num: 215/300, N_Loss: 0.0043703718110919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 10, Batch_Num: 216/300, N_Loss: 0.011255210265517235\n",
      "Training epoch: 10, Batch_Num: 217/300, N_Loss: 0.0024418605025857687\n",
      "Training epoch: 10, Batch_Num: 218/300, N_Loss: 0.013278383761644363\n",
      "Training epoch: 10, Batch_Num: 219/300, N_Loss: 0.0020010103471577168\n",
      "Training epoch: 10, Batch_Num: 220/300, N_Loss: 0.00661398284137249\n",
      "Training epoch: 10, Batch_Num: 221/300, N_Loss: 0.026382341980934143\n",
      "Training epoch: 10, Batch_Num: 222/300, N_Loss: 0.0015756174689158797\n",
      "Training epoch: 10, Batch_Num: 223/300, N_Loss: 0.010943284258246422\n",
      "Training epoch: 10, Batch_Num: 224/300, N_Loss: 0.0014773744624108076\n",
      "Training epoch: 10, Batch_Num: 225/300, N_Loss: 0.0018126191571354866\n",
      "Training epoch: 10, Batch_Num: 226/300, N_Loss: 0.003275549039244652\n",
      "Training epoch: 10, Batch_Num: 227/300, N_Loss: 0.0018217972246930003\n",
      "Training epoch: 10, Batch_Num: 228/300, N_Loss: 0.003990738186985254\n",
      "Training epoch: 10, Batch_Num: 229/300, N_Loss: 0.019055599346756935\n",
      "Training epoch: 10, Batch_Num: 230/300, N_Loss: 0.011415400542318821\n",
      "Training epoch: 10, Batch_Num: 231/300, N_Loss: 0.015914754942059517\n",
      "Training epoch: 10, Batch_Num: 232/300, N_Loss: 0.06436750292778015\n",
      "Training epoch: 10, Batch_Num: 233/300, N_Loss: 0.026012180373072624\n",
      "Training epoch: 10, Batch_Num: 234/300, N_Loss: 0.006311285309493542\n",
      "Training epoch: 10, Batch_Num: 235/300, N_Loss: 0.024334074929356575\n",
      "Training epoch: 10, Batch_Num: 236/300, N_Loss: 0.00437286589294672\n",
      "Training epoch: 10, Batch_Num: 237/300, N_Loss: 0.010048300959169865\n",
      "Training epoch: 10, Batch_Num: 238/300, N_Loss: 0.005397931206971407\n",
      "Training epoch: 10, Batch_Num: 239/300, N_Loss: 0.0021728617139160633\n",
      "Training epoch: 10, Batch_Num: 240/300, N_Loss: 0.007367774378508329\n",
      "Training epoch: 10, Batch_Num: 241/300, N_Loss: 0.0023198621347546577\n",
      "Training epoch: 10, Batch_Num: 242/300, N_Loss: 0.0034604747779667377\n",
      "Training epoch: 10, Batch_Num: 243/300, N_Loss: 0.03909234702587128\n",
      "Training epoch: 10, Batch_Num: 244/300, N_Loss: 0.0014968396862968802\n",
      "Training epoch: 10, Batch_Num: 245/300, N_Loss: 0.011797347106039524\n",
      "Training epoch: 10, Batch_Num: 246/300, N_Loss: 0.007194038946181536\n",
      "Training epoch: 10, Batch_Num: 247/300, N_Loss: 0.0022378689609467983\n",
      "Training epoch: 10, Batch_Num: 248/300, N_Loss: 0.02502758987247944\n",
      "Training epoch: 10, Batch_Num: 249/300, N_Loss: 0.0036744633689522743\n",
      "Training epoch: 10, Batch_Num: 250/300, N_Loss: 0.06737913191318512\n",
      "Training epoch: 10, Batch_Num: 251/300, N_Loss: 0.0039086914621293545\n",
      "Training epoch: 10, Batch_Num: 252/300, N_Loss: 0.04192446917295456\n",
      "Training epoch: 10, Batch_Num: 253/300, N_Loss: 0.00917218066751957\n",
      "Training epoch: 10, Batch_Num: 254/300, N_Loss: 0.008216076530516148\n",
      "Training epoch: 10, Batch_Num: 255/300, N_Loss: 0.0038429833948612213\n",
      "Training epoch: 10, Batch_Num: 256/300, N_Loss: 0.011277375742793083\n",
      "Training epoch: 10, Batch_Num: 257/300, N_Loss: 0.0031923630740493536\n",
      "Training epoch: 10, Batch_Num: 258/300, N_Loss: 0.006703290157020092\n",
      "Training epoch: 10, Batch_Num: 259/300, N_Loss: 0.006385271903127432\n",
      "Training epoch: 10, Batch_Num: 260/300, N_Loss: 0.06021808087825775\n",
      "Training epoch: 10, Batch_Num: 261/300, N_Loss: 0.011120505630970001\n",
      "Training epoch: 10, Batch_Num: 262/300, N_Loss: 0.002064186381176114\n",
      "Training epoch: 10, Batch_Num: 263/300, N_Loss: 0.005061295349150896\n",
      "Training epoch: 10, Batch_Num: 264/300, N_Loss: 0.023879844695329666\n",
      "Training epoch: 10, Batch_Num: 265/300, N_Loss: 0.016713524237275124\n",
      "Training epoch: 10, Batch_Num: 266/300, N_Loss: 0.003937413450330496\n",
      "Training epoch: 10, Batch_Num: 267/300, N_Loss: 0.005043867975473404\n",
      "Training epoch: 10, Batch_Num: 268/300, N_Loss: 0.006331958342343569\n",
      "Training epoch: 10, Batch_Num: 269/300, N_Loss: 0.001712733181193471\n",
      "Training epoch: 10, Batch_Num: 270/300, N_Loss: 0.0026207321789115667\n",
      "Training epoch: 10, Batch_Num: 271/300, N_Loss: 0.002419463125988841\n",
      "Training epoch: 10, Batch_Num: 272/300, N_Loss: 0.008196473121643066\n",
      "Training epoch: 10, Batch_Num: 273/300, N_Loss: 0.0028568007983267307\n",
      "Training epoch: 10, Batch_Num: 274/300, N_Loss: 0.002505989745259285\n",
      "Training epoch: 10, Batch_Num: 275/300, N_Loss: 0.012011386454105377\n",
      "Training epoch: 10, Batch_Num: 276/300, N_Loss: 0.007871929556131363\n",
      "Training epoch: 10, Batch_Num: 277/300, N_Loss: 0.0018901813309639692\n",
      "Training epoch: 10, Batch_Num: 278/300, N_Loss: 0.01780049316585064\n",
      "Training epoch: 10, Batch_Num: 279/300, N_Loss: 0.006404140964150429\n",
      "Training epoch: 10, Batch_Num: 280/300, N_Loss: 0.09535343199968338\n",
      "Training epoch: 10, Batch_Num: 281/300, N_Loss: 0.0023379584308713675\n",
      "Training epoch: 10, Batch_Num: 282/300, N_Loss: 0.009741760790348053\n",
      "Training epoch: 10, Batch_Num: 283/300, N_Loss: 0.029637625440955162\n",
      "Training epoch: 10, Batch_Num: 284/300, N_Loss: 0.022330641746520996\n",
      "Training epoch: 10, Batch_Num: 285/300, N_Loss: 0.010781622491776943\n",
      "Training epoch: 10, Batch_Num: 286/300, N_Loss: 0.025191929191350937\n",
      "Training epoch: 10, Batch_Num: 287/300, N_Loss: 0.0070105367340147495\n",
      "Training epoch: 10, Batch_Num: 288/300, N_Loss: 0.028336431831121445\n",
      "Training epoch: 10, Batch_Num: 289/300, N_Loss: 0.009241413325071335\n",
      "Training epoch: 10, Batch_Num: 290/300, N_Loss: 0.013984888792037964\n",
      "Training epoch: 10, Batch_Num: 291/300, N_Loss: 0.019435536116361618\n",
      "Training epoch: 10, Batch_Num: 292/300, N_Loss: 0.009510239586234093\n",
      "Training epoch: 10, Batch_Num: 293/300, N_Loss: 0.011149410158395767\n",
      "Training epoch: 10, Batch_Num: 294/300, N_Loss: 0.0051925755105912685\n",
      "Training epoch: 10, Batch_Num: 295/300, N_Loss: 0.004389164503663778\n",
      "Training epoch: 10, Batch_Num: 296/300, N_Loss: 0.015889789909124374\n",
      "Training epoch: 10, Batch_Num: 297/300, N_Loss: 0.0017321095801889896\n",
      "Training epoch: 10, Batch_Num: 298/300, N_Loss: 0.003343032207340002\n",
      "Training epoch: 10, Batch_Num: 299/300, N_Loss: 0.0028416470158845186\n",
      "Training epoch: 10, Batch_Num: 300/300, N_Loss: 0.0016827904619276524\n",
      "EPOCH: 10, MAE_HR: 432.6, MSE_HR: 558.5\n",
      "BEST MAE_HR: 432.6, BEST MSE_HR: 558.5, BEST Epoch: 5.00\n",
      "Training epoch: 11, Batch_Num: 1/300, N_Loss: 0.004706909414380789\n",
      "Training epoch: 11, Batch_Num: 2/300, N_Loss: 0.0025117648765444756\n",
      "Training epoch: 11, Batch_Num: 3/300, N_Loss: 0.0016070109559223056\n",
      "Training epoch: 11, Batch_Num: 4/300, N_Loss: 0.015156700275838375\n",
      "Training epoch: 11, Batch_Num: 5/300, N_Loss: 0.011032124049961567\n",
      "Training epoch: 11, Batch_Num: 6/300, N_Loss: 0.0038285921327769756\n",
      "Training epoch: 11, Batch_Num: 7/300, N_Loss: 0.07409306615591049\n",
      "Training epoch: 11, Batch_Num: 8/300, N_Loss: 0.006357761099934578\n",
      "Training epoch: 11, Batch_Num: 9/300, N_Loss: 0.0020537269301712513\n",
      "Training epoch: 11, Batch_Num: 10/300, N_Loss: 0.0009194710291922092\n",
      "Training epoch: 11, Batch_Num: 11/300, N_Loss: 0.008885616436600685\n",
      "Training epoch: 11, Batch_Num: 12/300, N_Loss: 0.013470468111336231\n",
      "Training epoch: 11, Batch_Num: 13/300, N_Loss: 0.001662591239437461\n",
      "Training epoch: 11, Batch_Num: 14/300, N_Loss: 0.020422780886292458\n",
      "Training epoch: 11, Batch_Num: 15/300, N_Loss: 0.0017915221396833658\n",
      "Training epoch: 11, Batch_Num: 16/300, N_Loss: 0.01582740992307663\n",
      "Training epoch: 11, Batch_Num: 17/300, N_Loss: 0.00833252165466547\n",
      "Training epoch: 11, Batch_Num: 18/300, N_Loss: 0.0029351147823035717\n",
      "Training epoch: 11, Batch_Num: 19/300, N_Loss: 0.024565070867538452\n",
      "Training epoch: 11, Batch_Num: 20/300, N_Loss: 0.0044713132083415985\n",
      "Training epoch: 11, Batch_Num: 21/300, N_Loss: 0.019068051129579544\n",
      "Training epoch: 11, Batch_Num: 22/300, N_Loss: 0.030641768127679825\n",
      "Training epoch: 11, Batch_Num: 23/300, N_Loss: 0.0025287701282650232\n",
      "Training epoch: 11, Batch_Num: 24/300, N_Loss: 0.009652508422732353\n",
      "Training epoch: 11, Batch_Num: 25/300, N_Loss: 0.009150671772658825\n",
      "Training epoch: 11, Batch_Num: 26/300, N_Loss: 0.002061407780274749\n",
      "Training epoch: 11, Batch_Num: 27/300, N_Loss: 0.0019782837480306625\n",
      "Training epoch: 11, Batch_Num: 28/300, N_Loss: 0.0053461515344679356\n",
      "Training epoch: 11, Batch_Num: 29/300, N_Loss: 0.004869780503213406\n",
      "Training epoch: 11, Batch_Num: 30/300, N_Loss: 0.004194712731987238\n",
      "Training epoch: 11, Batch_Num: 31/300, N_Loss: 0.026793332770466805\n",
      "Training epoch: 11, Batch_Num: 32/300, N_Loss: 0.004836246371269226\n",
      "Training epoch: 11, Batch_Num: 33/300, N_Loss: 0.002500726841390133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 11, Batch_Num: 34/300, N_Loss: 0.010308513417840004\n",
      "Training epoch: 11, Batch_Num: 35/300, N_Loss: 0.01582232676446438\n",
      "Training epoch: 11, Batch_Num: 36/300, N_Loss: 0.003496772376820445\n",
      "Training epoch: 11, Batch_Num: 37/300, N_Loss: 0.00624769227579236\n",
      "Training epoch: 11, Batch_Num: 38/300, N_Loss: 0.03600194677710533\n",
      "Training epoch: 11, Batch_Num: 39/300, N_Loss: 0.002838348736986518\n",
      "Training epoch: 11, Batch_Num: 40/300, N_Loss: 0.0025606947019696236\n",
      "Training epoch: 11, Batch_Num: 41/300, N_Loss: 0.0114286495372653\n",
      "Training epoch: 11, Batch_Num: 42/300, N_Loss: 0.1402272880077362\n",
      "Training epoch: 11, Batch_Num: 43/300, N_Loss: 0.011304483748972416\n",
      "Training epoch: 11, Batch_Num: 44/300, N_Loss: 0.002988188061863184\n",
      "Training epoch: 11, Batch_Num: 45/300, N_Loss: 0.003172166645526886\n",
      "Training epoch: 11, Batch_Num: 46/300, N_Loss: 0.12925352156162262\n",
      "Training epoch: 11, Batch_Num: 47/300, N_Loss: 0.04169008508324623\n",
      "Training epoch: 11, Batch_Num: 48/300, N_Loss: 0.005767127498984337\n",
      "Training epoch: 11, Batch_Num: 49/300, N_Loss: 0.07851166278123856\n",
      "Training epoch: 11, Batch_Num: 50/300, N_Loss: 0.0066562918946146965\n",
      "Training epoch: 11, Batch_Num: 51/300, N_Loss: 0.010180479846894741\n",
      "Training epoch: 11, Batch_Num: 52/300, N_Loss: 0.007874898612499237\n",
      "Training epoch: 11, Batch_Num: 53/300, N_Loss: 0.009686256758868694\n",
      "Training epoch: 11, Batch_Num: 54/300, N_Loss: 0.018049735575914383\n",
      "Training epoch: 11, Batch_Num: 55/300, N_Loss: 0.027247905731201172\n",
      "Training epoch: 11, Batch_Num: 56/300, N_Loss: 0.002397581236436963\n",
      "Training epoch: 11, Batch_Num: 57/300, N_Loss: 0.10550963133573532\n",
      "Training epoch: 11, Batch_Num: 58/300, N_Loss: 0.003041347488760948\n",
      "Training epoch: 11, Batch_Num: 59/300, N_Loss: 0.01028798520565033\n",
      "Training epoch: 11, Batch_Num: 60/300, N_Loss: 0.002158363349735737\n",
      "Training epoch: 11, Batch_Num: 61/300, N_Loss: 0.0019858686719089746\n",
      "Training epoch: 11, Batch_Num: 62/300, N_Loss: 0.07994374632835388\n",
      "Training epoch: 11, Batch_Num: 63/300, N_Loss: 0.010443050414323807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9dd79a6f8f8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mDensity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput_HR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mLossHR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDensity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGT_Density\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mLossHR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, target, *args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, target,\n\u001b[0;32m---> 47\u001b[0;31m                                                   output, *ctx.additional_args)\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, MAX_EPOCH+1):\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    count = 0\n",
    "    n_loss = 0\n",
    "    for blob in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Input_HR, GT_Density = get_training_batch(blob)\n",
    "        \n",
    "        Input_HR = Variable(Input_HR)\n",
    "        GT_Density = Variable(GT_Density)\n",
    "        \n",
    "        Input_HR = Input_HR.cuda()\n",
    "        GT_Density = GT_Density.cuda()\n",
    "        \n",
    "        Density = net(Input_HR)\n",
    "        \n",
    "        LossHR = n_loss_fn(Density, GT_Density)\n",
    "        \n",
    "        loss = (1000 * LossHR)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        count += 1\n",
    "        \n",
    "        n_loss += loss.data[0]\n",
    "        print(\"Training epoch: {}, Batch_Num: {}/{}, N_Loss: {}\".format(\n",
    "            epoch, count, train_data_loader.get_num_samples(), loss.data[0]))\n",
    "        \n",
    "    N_Loss.append(n_loss/count)\n",
    "    \n",
    "    if (epoch % 5 == 0):\n",
    "        maeHR, mseHR = evaluate_model(net, val_data_loader)\n",
    "\n",
    "        if maeHR < best_maeHR:\n",
    "            best_maeHR = maeHR\n",
    "            best_mseHR = mseHR\n",
    "            best_epochHR = epoch\n",
    "\n",
    "            network.save_net(os.path.join(output_dir, 'best_icCNN.h5'), net)\n",
    "\n",
    "        print(\"EPOCH: %d, MAE_HR: %.1f, MSE_HR: %0.1f\" % (epoch, maeHR, mseHR))\n",
    "        print(\"BEST MAE_HR: %0.1f, BEST MSE_HR: %0.1f, BEST Epoch: %4.2f\" % (best_maeHR, best_mseHR, best_epochHR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save_net(os.path.join(output_dir, 'last_epoch_icCNN.h5'), net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(N_Loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
